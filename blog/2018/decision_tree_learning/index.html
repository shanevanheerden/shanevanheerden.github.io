<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>üå≤ Decision Tree Learning | Shane van Heerden</title> <meta name="author" content="Shane van Heerden"> <meta name="description" content="Part 3 - Classic Machine Learning Algorithms Series"> <meta name="keywords" content="machine-learning-engineer, data-scientist, phd, machine-learning, data-science, natural-language-processing, nlp, data-engineering, deep-learning, artificial-intelligence, ai"> <meta property="og:site_name" content="Shane van Heerden"> <meta property="og:type" content="website"> <meta property="og:title" content="Shane van Heerden | üå≤ Decision Tree Learning"> <meta property="og:url" content="https://shanevanheerden.github.io/blog/2018/decision_tree_learning/"> <meta property="og:description" content="Part 3 - Classic Machine Learning Algorithms Series"> <meta property="og:image" content="/assets/img/shane.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="üå≤ Decision Tree Learning"> <meta name="twitter:description" content="Part 3 - Classic Machine Learning Algorithms Series"> <meta name="twitter:image" content="/assets/img/shane.jpg"> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Shane  van Heerden"
        },
        "url": "https://shanevanheerden.github.io/blog/2018/decision_tree_learning/",
        "@type": "WebSite",
        "description": "Part 3 - Classic Machine Learning Algorithms Series",
        "headline": "üå≤ Decision Tree Learning",
        "sameAs": ["https://github.com/shanevanheerden", "https://www.linkedin.com/in/shaneandrewvanheerden"],
        "name": "Shane  van Heerden",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shanevanheerden.github.io/blog/2018/decision_tree_learning/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "üå≤ Decision Tree Learning",
      "description": "Part 3 - Classic Machine Learning Algorithms Series",
      "published": "November 11, 2018",
      "authors": [
        {
          "author": "Shane van Heerden",
          "authorURL": "",
          "affiliations": [
            {
              "name": "SUnORE, Stellenbosch University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shane¬†</span>van Heerden</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">üëã about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">üìù blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">üóûÔ∏è news</a> </li> <li class="nav-item "> <a class="nav-link" href="/skills/">üéì skills</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">üóÇ projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">üìö publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">üé® hobbies</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">üìÑ cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>üå≤ Decision Tree Learning</h1> <p>Part 3 - Classic Machine Learning Algorithms Series</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#1-introduction">1. Introduction</a></div> <div><a href="#2-classification-and-regression-trees">2. Classification and regression trees</a></div> <div><a href="#3-random-forests">3. Random forests</a></div> <div><a href="#4-the-c4-5-algorithm">4. The C4.5 algorithm</a></div> <div><a href="#5-wrapping-up">5. Wrapping up</a></div> </nav> </d-contents> <p><em>This blog post was adapted from Section 4.5 in my <a href="https://sunore.co.za/wp-content/uploads/2021/03/vanheerden_phd_2020.pdf" rel="external nofollow noopener" target="_blank">PhD dissertation</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog6.1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog6.1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog6.1-1400.webp"></source> <img src="/assets/img/blog/blog6.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This is the third post in a series of blog posts focusing on the some of the classic Machine Learning algorithms! In <a href="https://shanevanheerden.github.io/blog/2018/generalised_linear_models/">my last blog post</a>, I described the notion of Generalised Linear Models and showed how the well-known Logistic Regression model is meerly a special case of a much broader family of statistical models. In this post, we will talk about another much-loved class of machine learning models, namely Decision Trees. Let‚Äôs get started!</p> <h2 id="1-introduction">1. Introduction</h2> <p><em>Decision tree learning</em> is one of the most widely used supervised learning approaches ‚Äî a fact that is primarily attributed to their capability of performing both classification and regression tasks, their relatively simple mathematical formulation and their ability to represent their underlying decision making process visually and explicitly<d-cite key="Gupta2017"></d-cite>. A decision tree can, in essence, be explained in terms of two entities, namely decision <em>nodes</em> and <em>leaves</em>. Decision nodes describe the rule by which data are partitioned into smaller sub-parts, while leaves denote the final decisions or outcomes<d-cite key="Kulkarni2017"></d-cite>. There exists a wide variety of tree-based learning algorithms in the literature, the most widely used being <em>classification and regression trees</em> (CART), <em>random forests</em>, the <em>C4.5</em> algorithm and <em>gradient boosted decision trees</em>.</p> <hr> <h2 id="2-classification-and-regression-trees">2. Classification and regression trees</h2> <p>CART is a popular non-parametric algorithm for producing either classification or regression trees, depending on the defined target variable<d-cite key="Marsland2009"></d-cite>. The explanatory variables $\mathcal{X}={\mathbf{x}^{(1)},\ldots,\mathbf{x}^{(m)}}$ can be either numerical or categorical<d-cite key="Hand2001"></d-cite>. The stages in a CART analysis are two-fold and employ a binary recursive partitioning method. The first stage is the tree growth stage where so-called <em>parent</em> nodes are recursively partitioned into two more homogeneous <em>child</em> nodes. The second stage is concerned with limiting the overall tree growth by computing the optimal tree size.</p> <p>The process of producing a classification tree begins at an initial node, called the <em>root node</em>, in which the entire training set is contained. Tree growth is achieved by partitioning each node in such a way as to maximise a defined splitting criterion. If the $j$<sup><em>th</em></sup> attribute is defined as the parent node $d_p$, then there exists an optimal splitting value $x_j^{*}$ for this node. Once this value has been determined, an <em>if-then</em> splitting procedure may follow whereby all observations with a $j$<sup><em>th</em></sup> attribute value of $x_j^{*}$ or greater are partitioned into a right-hand child node $t_r$, as illustrated in Figure 1, while all observations with a $j$<em><sup>th</sup></em> attribute value less than $x_j^{*}$ are partitioned into a left-hand child node $t_l$<d-cite key="Rashidi2014"></d-cite>. The question, however, remains as to how to determine this ‚Äúoptimal‚Äù splitting value $x_j^{*}$ for the $j$<em><sup>th</sup></em> attribute.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog6.2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog6.2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog6.2-1400.webp"></source> <img src="/assets/img/blog/blog6.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 1:</em> Splitting a parent node into corresponding child nodes in a CART<d-cite key="Timofeev2004"></d-cite>. </div> <p>According to Lewis <d-cite key="Lewis2000"></d-cite>, the primary goal of splitting a parent node into two child constituents is to achieve the largest improvement in predictive accuracy by maximising the <em>homogeneity</em> or <em>purity</em> of the two subsequent child nodes. The <em>impurity</em> of a node $t$ is calculated by means of an <em>impurity function</em> $i(t)$<d-cite key="StatSoft2016"></d-cite>, which is defined in terms of the impurity of the parent node, denoted by $i(t_p)$, and the combined impurity of the associated child nodes, denoted by $i(t_c)$. Regardless of the child node split, the impurity of a parent node remains constant. The change in impurity, given a specific child split, may therefore be expressed as</p> <p>\begin{equation} \Delta i(t)=i(t_p)-E[i(t_c)],\label{4.eqn.tree1} \end{equation}</p> <p>where $E[i(t_c)]$ is the expected impurity of the two child nodes, which can also be expressed in terms of the probability of being split into the left- and right-hand nodes, denoted by $p(t_l)i(t_l)$ and $p(t_r)i(t_r)$, respectively. Consequently, in the case of a classification tree, all possible attribute values for all $n$ attributes in $\mathcal{X}$ must be considered as possible ‚Äúbest split,‚Äù resulting in the maximum value of (\ref{4.eqn.tree1}), expressed mathematically at a single parent node, as</p> <p>\begin{equation} \underset{\substack{x_j&lt;x_j^{*},j\in{1,\ldots,m}}}{\arg\max}\hspace{1.5mm}i(t_p)-p(t_l)i(t_l)-p(t_r)i(t_r).\label{4.eqn.tree2} \end{equation}</p> <p>Although this process of maximising the change in impurity described in (\ref{4.eqn.tree2}) is common to both classification and regression trees, the choice of impurity function $i(t)$ is defined in a different manner<d-cite key="Moisen2008"></d-cite>. Many different impurity functions have been proposed in the literature. In the case of classification trees, however, the most widely used is the <em>Gini splitting rule</em> and the <em>Twoing splitting rule</em><d-cite key="Timofeev2004"></d-cite>.</p> <p>A tree that has been grown to the point where it fits the training data almost perfectly typically results in an overly complex model that performs poorly in respect of the validation data, as illustrated in Figure 2. A tree that is too small, on the other hand, is not able to discover the underlying relationships present in the data. Consequently, the ideal tree size is one that achieves a suitable trade-off between overfitting and underfitting the training data. A CART is, therefore, grown to a near optimal size by employing one of two methods: The growth of the tree can be terminated according to (1) a <em>stopping criterion</em> or (2) employing a <em>pruning</em> procedure.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog6.3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog6.3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog6.3-1400.webp"></source> <img src="/assets/img/blog/blog6.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 2:</em> The change in misclassification rate according to a tree‚Äôs size<d-cite key="Steynberg2016"></d-cite>. </div> <p>As the name implies, the use of the stopping criterion approach forces the recursive tree growing procedure to be terminated once a specified criterion is met. Rashidi <em>et al.</em><d-cite key="Rashidi2014"></d-cite> described two such criteria. The first of these is the <em>maximum tree depth</em> criterion which terminates tree growth after a pre-specified number of branching levels have been created. The second is the <em>minimum node size to split</em> criterion which terminates tree growth at a specific node if the size of the node (<em>i.e.</em> the number of observations contained in the node) is less than a pre-specified value.</p> <p>Although the stopping criterion provides a simplified method for limiting the growth of a tree, the more popular method is that of pruning<d-cite key="Timofeev2004"></d-cite>. According to this procedure, a tree is first grown to its maximum size and then pruned backwards to a more suitable size<d-cite key="Moisen2008"></d-cite>. There exists a wide variety of methods for determining the optimal size to which a tree should be pruned backwards, including <em>reduced-error pruning</em>, <em>pessimistic pruning</em>, <em>rule post-pruning</em>, and <em>cost complexity pruning</em><d-cite key="James2013, Rokach2008"></d-cite>. Performance comparisons of the various pruning methods have been conducted in multiple studies, with the majority finding that no single pruning method is superior under all circumstances<d-cite key="Esposito1997, Mingers1989, Quinlan1987"></d-cite> ‚Äî a finding that echoes the <em>No Free Lunch</em><d-footnote>The No Free Lunch theorem states that <em>"any two optimization algorithms are equivalent when their performance is averaged across all possible problems."</em><d-cite key="Wolpert2005"></d-cite>.</d-footnote> theorem.</p> <p>Although CART is a powerful predictive model in its original form, there are three attractive methods for enhancing this standard learning model<d-cite key="Moisen2008"></d-cite>. The first of these relies on a procedure called <em>bagging</em> which produces an aggregation of multiple bootstrap samples (hence, bagging is also referred to as <em>bootstrap aggregating</em>). Although the notion of bagging may be applied to any learning model in an attempt to improve its stability and accuracy<d-cite key="James2013"></d-cite>, this procedure is described here exclusively in the context of the CART learning model so as to demonstrate its working. Given a training set $\mathcal{S}$, a total of $b$ separate data subsets $\mathcal{S}_1,\ldots,\mathcal{S}_b$ are randomly drawn from $\mathcal{S}$ and used to construct $b$ CART models. The predictive capability of CART model $i$ is then validated in respect of the data not used to construct the model (<em>i.e.</em> the set $\mathcal{S}\text{\textbackslash}\mathcal{S}_i$). In the case of a classification task, the final categorisation is given by the majority vote of the $b$ CART models (for this reason, $b$ should be chosen as odd in the case of a binary classification problem to avoid ties) while, in the case of a regression task, the results of the independent CART predictions are averaged<d-cite key="James2013"></d-cite>. The predictions achieved by this bagging approach are reported to exhibit less variance than that of a single CART model<d-cite key="Hastie2009"></d-cite>.</p> <p>The second CART enhancement method is that of <em>boosting</em>. Much like bagging, the notion of boosting may be applied to any learning model<d-cite key="James2013"></d-cite>, but, for demonstration purposes, is described here exclusively in the context of the CART learning model. Boosting involves the construction of $b$ CART models <em>sequentially</em> from an adaptive training data subset, where each new tree utilises the information discovered by its predecessors. This stands in contrast to bagging, which constructs multiple CART models <em>independently</em> from bootstrapped data samples. Initially, all observations are assigned an equal weighting and are randomly sampled according to this weighting to be included in the training sample set $\mathcal{S}$. During the construction and evaluation of each new CART model, the frequently misclassified observations (known as <em>problematic observations</em>) are given more weight, resulting in their more frequent inclusion in the new sample $\mathcal{S}^{\prime}$ for training future CART models. This effectively allows future models to correct the mistakes of their predecessors, thus improving the models‚Äô overall prediction ability<d-cite key="Moisen2008"></d-cite>. It should be noted, however, that boosting may suffer from overfitting if the value of $b$ is chosen too large<d-cite key="James2013"></d-cite>.</p> <p>The third and, by far, most popular CART enhancement is called <em>random forests</em> which is discussed, in its own right, in the next section.</p> <hr> <h2 id="3-random-forests">3. Random forests</h2> <p>Introduced by Breiman<d-cite key="Breiman2001"></d-cite> and often considered to be a panacea of all data science problems<d-cite key="AnalyticsVidhya2016"></d-cite>, random forests is a versatile and simplistic learning model that has proven effective in a wide range of classification and regression tasks<d-cite key="Cutler2011"></d-cite>. Other advantages of the method of random forests include its ability to handle categorical and numerical attributes without any need for scaling, its inclination to perform implicit feature selection, and its parallel algorithmic structure which allows for relatively fast training times<d-cite key="Deeb2015"></d-cite>.</p> <p>Similar to bagging, this method involves the construction of $b$ decision trees produced by CART and combines them together to produce a more accurate, stable prediction. In addition to the randomness induced by employing bootstrap sampling, random forests provide further randomness by imposing the following change to the standard CART model: Starting at the root node, instead of considering each of the $m$ attributes as possible candidates according to which to split the node, only a random subset of $m^{\prime}&lt;m$ (typically chosen as $m^{\prime}\approx\sqrt{m}$) attributes are considered. This process of only considering a random attribute subset to split a node is continued until the tree reaches the largest possible size. The process is repeated a total of $b$ times, and the final prediction is typically a majority vote (in the case of classification) or an average (in the case of regression) of all the predictions made by the $b$ trees constructed<d-cite key="Moisen2008"></d-cite>.</p> <p>The reason why this approach works so well can be described as follows: By only allowing the algorithm to consider a subset of the available attributes at a node split, the algorithm is charged to explain the target variables by not only utilising the ‚Äúbest‚Äù attributes present in the data. In other words, if there is an attribute which is an inherently very strong predictor of the target variables, this attribute will not be available to the algorithm $100(m-m^{\prime})/m\%$ of the time (on average), thereby forcing the algorithm to explore the use of other attributes which are moderately strong predictors<d-cite key="James2013"></d-cite>. In the case of bagging, the collection of trees are, to a large extent, similar to one another since they all typically rely on the same strong predictor when splitting the root node. Hence, the members of this collection of trees are highly correlated and an average of highly correlated results typically does not produce a large reduction in variance. For this reason, the method of random forests is typically described as a technique for <em>decorrelating</em> the trees produced by CART<d-cite key="James2013"></d-cite>.</p> <hr> <h2 id="4-the-c45-algorithm">4. The C4.5 algorithm</h2> <p>The C4.5 algorithm was developed by Quinlan<d-cite key="Quinlan1993"></d-cite> and is the successor of his earlier ID3 algorithm<d-cite key="Quinlan1986"></d-cite>. Similar to CART, this algorithm begins with the training set defined at a single root node of a tree which is subsequently partitioned into child nodes based on a specific splitting criterion. The algorithm is, however, only applicable in the case of classification tasks. In the case of the C4.5 algorithm, the <em>information gain ratio</em> is used as the default spitting criterion. Just like in CART, if a node is split based on a quantitative attribute, a threshold may be determined as the point at which the training data set is partitioned into two subsequent child nodes. In the case of a qualitative attribute, however, the C4.5 algorithm creates as many child nodes as there are associated categories. For this reason, the C4.5 algorithm is described as a <em>multi-way split</em> (or <em>non-binary split</em>) tree growing algorithm<d-cite key="Kim2001"></d-cite>. The tree growing procedure of the C4.5 algorithm also follows a greedy approach by attempting to find the best local node split which maximises the information gain ratio without employing any form of backtracking<d-cite key="Galathiya2012, Ruggieri2002"></d-cite>.</p> <p>The split criterion employed by the C4.5 algorithm is based on Shannon‚Äôs well-known concept of entropy from information theory<d-cite key="Shannon1949"></d-cite>. Entropy is a measure of the disorder in a data subset. Denote the set of $h$ categories for attribute $j$ by $\mathcal{C}_j=c^{(j)}_1,\ldots,c^{(j)}_h$, and the frequency of observations in a particular subset of data $\mathcal{S}\subset\mathcal{X}$ with a target variable of $q$ by freq$(q,\mathcal{S})$. Then the entropy of a category $c_k^{(j)}$ of attribute $j$ is given by</p> <p>\begin{equation} \mbox{Entropy}(\mathcal{S})=-\sum_{q=1}^{Q}\frac{\mbox{freq}(q,\mathcal{S})}{|\mathcal{S}|}\log_b\left(\frac{\mbox{freq}(q,\mathcal{S})}{|\mathcal{S}|}\right),\label{4.eqn.entropy} \end{equation}</p> <p>where $Q$ is the total number of target attribute categories. If a large proportion of observations in a data subset $\mathcal{S}$ have the value of $q$ (<em>i.e.</em> $\text{freq}(q,\mathcal{S})/|\mathcal{S}|\approx 1$), then the entropy in (\ref{4.eqn.entropy}) will be approximately zero (implying a high level of order). Conversely, if there is an equal representation of all attribute categories in the data subset $\mathcal{S}$ (<em>i.e.</em> $\text{freq}(q,\mathcal{S})/|\mathcal{S}|\approx 1/C_j$), then the entropy in (\ref{4.eqn.entropy}) will be close to one (implying a high level of disorder).</p> <p>With this notion in mind, consider the case in which a data subset $\mathcal{S}$, residing in a parent node $t_p$, is partitioned based on the categories of attribute $j$ producing $C_j$ child nodes $t_1,\ldots,t_{C_j}$. The <em>information gain</em> achieved by this node split can be defined as the total reduction of entropy in the data, expressed mathematically as</p> <p>\begin{equation} \mbox{Gain}(t_p,j)=\mbox{Entropy}(t_P)-\sum_{i=1}^{C_j}\frac{|t_i|}{|t_p|}\mbox{Entropy}(t_i).\label{4.eqn.gain} \end{equation}</p> <p>This criterion was first proposed for splitting nodes in the ID3 algorithm<d-cite key="Quinlan1986"></d-cite>. A notable drawback associated with this splitting criterion is that it is biased towards attributes with many categories<d-cite key="Thakur2010"></d-cite>. This led Quinlan<d-cite key="Quinlan1993"></d-cite> subsequently to build on this notion in order to create the information gain ratio, which is a more robust criterion than information gain and generally produces better classification results. This formulation utilises (\ref{4.eqn.entropy}) as a basis to define a so-called <em>split entropy</em> in terms of the categories of an attribute $j$ (as opposed to the target attribute categories), expressed mathematically as</p> <p>\begin{equation} \mbox{SplitEntropy}(t_p,j)=-\sum_{i=1}^{C_j}\frac{|t_i|}{|t_p|}\log_b\left(\frac{|t_i|}{|t_p|}\right).\label{4.eqn.splitentropy} \end{equation}</p> <p>If a large proportion of observations in a data subset $\mathcal{S}$ exhibit the value $c_k^{(j)}$ for attribute $j$ (<em>i.e.</em> $|t_i|/|t_p|\approx 1$), then the entropy in (\ref{4.eqn.entropy}) will be approximately zero, while, conversely, if there is an equal representation of all attribute categories in the data subset $\mathcal{S}$ (<em>i.e.</em> $|t_i|/|t_p|\approx 1/C_j$), then the entropy in (\ref{4.eqn.entropy}) will be close to one. Combining (\ref{4.eqn.gain}) and (\ref{4.eqn.splitentropy}), it is possible to define the information gain ratio as the proportion of information generated by partitioning a parent node $t_p$ according to attribute $j$, expressed mathematically as</p> <p>\begin{equation} \mbox{GainRatio}(t_p,j)=\frac{\mbox{Gain}(t_p,j)}{\mbox{SplitEntropy}(t_p,j)}.\label{4.eqn.gainratio} \end{equation}</p> <p>Unlike its predecessor, this formulation is more robust since it effectively penalises the choice of splitting attributes with many categories. A drawback of this approach, however, is that (\ref{4.eqn.gainratio}) tends to favour unbalanced splits in which one node partition is much larger than the rest<d-cite key="Harris2002"></d-cite>.</p> <hr> <h2 id="5-wrapping-up">5. Wrapping up</h2> <p>And that‚Äôs all I have. In this blog post, I walked yoiu through a host of different decision tree algorithms that are still widely used in practice today, namely CARTs, Random forests and the C4.5 algorithm. Stay tuned to this series where I‚Äôll dive into even more classic machine learning algorithms. Until next time!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/bibliography.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 950px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shanevanheerden/shanevanheerden.github.io","data-repo-id":"R_kgDOHll7zA","data-category":"Comments","data-category-id":"DIC_kwDOHll7zM4CWO-7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Shane van Heerden. Last updated: January 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T5QSVR2X6Z"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T5QSVR2X6Z");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>