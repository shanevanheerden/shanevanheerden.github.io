<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>🧮 Generalised Linear Models | Shane van Heerden</title> <meta name="author" content="Shane van Heerden"> <meta name="description" content="Part 2 - Classic Machine Learning Algorithms Series"> <meta name="keywords" content="machine-learning-engineer, data-scientist, phd, machine-learning, data-science, natural-language-processing, nlp, data-engineering, deep-learning, artificial-intelligence, ai"> <meta property="og:site_name" content="Shane van Heerden"> <meta property="og:type" content="website"> <meta property="og:title" content="Shane van Heerden | 🧮 Generalised Linear Models"> <meta property="og:url" content="https://shanevanheerden.github.io/blog/2018/generalised_linear_models/"> <meta property="og:description" content="Part 2 - Classic Machine Learning Algorithms Series"> <meta property="og:image" content="/assets/img/shane.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="🧮 Generalised Linear Models"> <meta name="twitter:description" content="Part 2 - Classic Machine Learning Algorithms Series"> <meta name="twitter:image" content="/assets/img/shane.jpg"> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Shane  van Heerden"
        },
        "url": "https://shanevanheerden.github.io/blog/2018/generalised_linear_models/",
        "@type": "WebSite",
        "description": "Part 2 - Classic Machine Learning Algorithms Series",
        "headline": "🧮 Generalised Linear Models",
        "sameAs": ["https://github.com/shanevanheerden", "https://www.linkedin.com/in/shaneandrewvanheerden"],
        "name": "Shane  van Heerden",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shanevanheerden.github.io/blog/2018/generalised_linear_models/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "🧮 Generalised Linear Models",
      "description": "Part 2 - Classic Machine Learning Algorithms Series",
      "published": "October 19, 2018",
      "authors": [
        {
          "author": "Shane van Heerden",
          "authorURL": "",
          "affiliations": [
            {
              "name": "SUnORE, Stellenbosch University",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shane </span>van Heerden</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">👋 about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">📝 blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">🗞️ news</a> </li> <li class="nav-item "> <a class="nav-link" href="/skills/">🎓 skills</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">🗂 projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">📚 publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">🎨 hobbies</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">📄 cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>🧮 Generalised Linear Models</h1> <p>Part 2 - Classic Machine Learning Algorithms Series</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#1-introduction">1. Introduction</a></div> <div><a href="#2-general-linear-models">2. General linear models</a></div> <div><a href="#3-the-exponential-family">3. The exponential family</a></div> <div><a href="#4-constructing-a-generalised-linear-model">4. Constructing a generalised linear model</a></div> <div><a href="#5-logistic-regression">5. Logistic regression</a></div> <div><a href="#6-warpping-up">6. Warpping up</a></div> </nav> </d-contents> <p><em>This blog post was adapted from Section 4.4 in my <a href="https://sunore.co.za/wp-content/uploads/2021/03/vanheerden_phd_2020.pdf" rel="external nofollow noopener" target="_blank">PhD dissertation</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog5.1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog5.1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog5.1-1400.webp"></source> <img src="/assets/img/blog/blog5.1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This is the second post in a series of blog posts focusing on the some of the classic Machine Learning algorithms! In <a href="https://shanevanheerden.github.io/blog/2018/an_overview_of_supervised_machine_learning/">my previous post</a>, I gave a brief introduction of Supervised Learning together with many considerations that you need to bear in mind when framing a problem as a supervised learning problem. In this post, we will begin exploring the classic suite of statistical learning models, namely <em>Generalized Linear Models</em> (GLMs). Let’s get started!</p> <h2 id="1-introduction">1. Introduction</h2> <p>The idea of generalising various statistical models into a single, overarching theoretical framework was first conceived in a 1972 paper by Nelder and Wedderburn<d-cite key="Nelder1972"></d-cite>. In that paper, the authors showed that many of the classical linear statistical models share a number of properties which may be exploited collectively, and that they admit a common method for computing parameter estimates. These commonalities facilitate the study of so-called GLMs as a single class of models, as opposed to considering the various models as an unrelated collection of special topics. In this section, it is shown that the well-known <em>Ordinary Least Squares Regression</em> (OLSR) and logistic regression parametric models are, in fact, special cases of the family of GLMs. In their most basic form, GLMs are an extension of the concepts proposed by its predecessor, <em>general linear models</em><d-footnote>The term <em>generalised linear model</em> and its abbreviation GLM are often confused with the term <em>general linear model</em>. The co-originator, John Nelder, has expressed regret over choosing this terminology<d-cite key="Senn2003"></d-cite>.</d-footnote>, which will serve as a starting point for this discussion.</p> <hr> <h2 id="2-general-linear-models">2. General linear models</h2> <p>In a general linear model, the fundamental assumption is that the set of target variables $\mathcal{Y}={y^{(1)},\ldots,y^{(m)}}$, now denoted by the vector $\mathbf{y}$, and the set of explanatory variables $\mathcal{X}={\mathbf{x}^{(i)},\ldots,\mathbf{x}^{(m)}}$, now denoted by the <em>design matrix</em> $\mathbf{X}$, are related linearly by an equation of the form</p> <p>\begin{equation} \mathbf{y}=X\mathbf{\beta}+\mathbf{\epsilon},\label{4.eqn.asmp} \end{equation}</p> <p>where $\mathbf{\beta}$ is a column vector of unknown parameters, while $\mathbf{\epsilon}$ is a column vector containing independently and identically distributed error terms that capture any unmodelled effects or random noise in the data. It is assumed that the expected values $\mathbf{\mu}=E(\mathbf{y}\mid\mathbf{X};\mathbf{\beta})$ of the target variables have an identity relationship with the linear predictor $\mathbf{X}\mathbf{\beta}$ in the sense that the hypothesis of the general linear model is given by $h(\mathbf{\tilde{x}})=E(y\mid \mathbf{x};\mathbf{\beta})=\mathbf{x}\mathbf{\beta}$. The strong assumption made by a general linear model is that the error terms are assumed to be sampled from a Gaussian (or normal) distribution with zero mean and a constant variance $\sigma^2$, written as $\epsilon^{(i)}\sim\mathcal{N}(0,\sigma^2)$. The density of $\epsilon^{(i)}$ is, therefore, given by</p> <p>\begin{equation} p(\epsilon^{(i)})=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(\epsilon^{(i)})^2}{2\sigma^2}\right), \end{equation}</p> <p>which implies that</p> <p>\begin{equation} p(y^{(i)}\mid\mathbf{x}^{(i)};\mathbf{\beta})=\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)}-\mathbf{x}^{(i)}\mathbf{\beta})^2}{2\sigma^2}\right). \end{equation}</p> <p>The distribution of the observed target variables $\mathbf{y}$ are consequently characterised by $\mathbf{y}\mid\mathbf{X};\mathbf{\beta}\sim\mathcal{N}(\mathbf{\beta},\sigma^2)$. Since each observation $(\mathbf{x}^{(i)}, y^{(i)})$ is assumed to be independent of all the other observations, the joint density or <em>likelihood</em> $L(\mathbf{\beta})=L(\mathbf{\beta};\mathbf{X},\mathbf{y})=p(\mathbf{y}\mid\mathbf{X};\mathbf{\beta})$ of the data is given by the product of the individual probabilities</p> \[\mkern-103pt L(\mathbf{\beta})=\prod_{i=1}^{m}p(y^{(i)}\mid\mathbf{x};\mathbf{\beta})\] <p>\begin{equation} =\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)}-\mathbf{x}^{(i)}\mathbf{\beta})^2}{2\sigma^2}\right). \end{equation}</p> <p>One would like to obtain an estimate of $\mathbf{\beta}$ which maximises the value of $L(\mathbf{\beta})$, called the <em>maximum likelihood estimation</em> (MLE). Instead of maximising $L(\mathbf{\beta})$, one may also maximise any strictly increasing function of the likelihood function. As a result, it is often more convenient to maximise a logarithmic form of the likelihood function (due to its relatively simple differentiability), appropriately called the <em>log likelihood</em> $\ell(\mathbf{\beta})$, where</p> \[\mkern-107pt\ell(\mathbf{\beta})=\log L(\mathbf{\beta})\] \[=\log\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)}-\mathbf{x}^{(i)}\mathbf{\beta})^2}{2\sigma^2}\right)\] \[\hspace{2pt}=\sum_{i=1}^{m}\log\frac{1}{\sqrt{2\pi}\sigma}\exp\left(-\frac{(y^{(i)}-\mathbf{x}^{(i)}\mathbf{\beta})^2}{2\sigma^2}\right)\] <p>\begin{equation} \hspace{3pt}=m\log\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^{m}(y^{(i)}-\mathbf{x}^{(i)}\mathbf{\beta})^2. \end{equation}</p> <p>From these results, one may conclude that the final choice of $\beta$-values does not depend on the variance $\sigma^2$ of the Gaussian distribution (this fact will prove useful later). Hence, for a general linear model, maximising $\ell(\mathbf{\beta})$ is equivalent to minimising the cost function</p> <p>\begin{equation} \sum_{i=1}^{m}(y^{(i)}-\mathbf{x}^{(i)}\mathbf{\beta})^2.\label{4.eqn.ols} \end{equation}</p> <p>The expression (\ref{4.eqn.ols}) may be recognised as that occurring in the well-known OLSR method for estimating the unknown parameters in a linear regression model. The optimal parameter values $\mathbf{\beta}^{*}$ are, therefore, those that minimise the sum of the squared errors between the actual and predicted target variable, for all observations in the training set. For most statistical models, these parameter values are typically computed using a technique called <em>gradient decent</em>, the details of which are described by Ng<d-cite key="Ng2012"></d-cite>. In the case of OLSR, however, these $\mathbf{\beta}$-values may be computed analytically by solving the well-known normal equations<d-cite key="Hastie2009"></d-cite>, to yield</p> <p>\begin{equation} \mathbf{\beta}{^*}=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}.\label{4.eqn.norm} \end{equation}</p> <hr> <h2 id="3-the-exponential-family">3. The exponential family</h2> <p>The formulation of a GLM is traditionally achieved within the framework of the <em>exponential family</em> of distributions. A class of distributions is said to form part of the exponential family if their probability mass/density functions may be written in the form</p> <p>\begin{equation} p(y;\mathbf{\eta})=b(y)\exp(\mathbf{\eta}^T\mathbf{T}(y)-a(\mathbf{\eta})),\label{4.eqn.expfam} \end{equation}</p> <p>where $\mathbf{\eta}$ is the so-called <em>canonical parameter</em> (or <em>natural parameter</em>) of the distribution, $\mathbf{T}(y)$ is the sufficient statistic (for the majority of the distributions, it is often the case that $\mathbf{T}(y)=y$), $b(y)$ is the <em>underlying measure</em>, and $a(\mathbf{\eta})$ is the <em>log partition function</em>, which ensures that the distribution sums/integrates to one. Hence,</p> <p>\begin{equation} a(\mathbf{\eta})=\log\displaystyle\int b(y)\exp(\mathbf{\eta}^T\mathbf{T}(y))\hspace{0.8mm}\mathrm{d}{x}. \end{equation}</p> <p>Therefore, for a fixed choice of the functions $b(y)$, $\mathbf{T}(y)$, and $a(\mathbf{\eta})$, one may define a <em>family</em> (or set) of distributions. Furthermore, by varying the canonical parameter $\mathbf{\eta}$, one may obtain different distributions within this family. It may be shown that many well-known distributions, such as the Gaussian, Bernoulli, multinomial, Poisson, gamma, beta, exponential and inverse Gaussian form part of the exponential family of distributions<d-cite key="McCullagh1989"></d-cite>.</p> <p>Using this formulation, one can show that the celebrated Gaussian (or normal) distribution is, in fact, a member of the exponential family of distributions. Recall that, during the derivation of a general linear model in Section 2, the value of $\sigma^2$ did not depend on the final choice of $\mathbf{\beta}$. Consequently, one may choose an arbitrary value for $\sigma^2$ without loss of generality. To simplify the subsequent derivation, $\sigma^2$ is set to unity (<em>i.e.</em> $\sigma^2=1$). In this way, the standard Gaussian distribution can be expanded to the form</p> \[\mkern-89pt p(y;\mu)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(y-\mu)^2\right)\] <p>\begin{equation} =\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}y^2\right)\cdot\exp\bigg(\mu y-\frac{1}{2}\mu^2\bigg). \end{equation}</p> <p>Notice that this form of the Gaussian probability density expression is in the exponential family form of (\ref{4.eqn.expfam}) with $\eta=\mu$, $T(y)=y$, $a(\eta)=\mu^2/2$ and $b(y)=(1/\sqrt{2\pi})\exp(-y^2/2)$.</p> <p>As in the case of the Gaussian distribution, the well-known Bernoulli distribution can also be shown to reside within the set of exponential family distributions. The Bernoulli distribution specifies the distribution of a variable $y\in{0,1}$ with mean $\phi$ as</p> \[\mkern-113pt p(y;\phi)=\phi^y(1-\phi)^{1-y}\] \[\mkern-16pt=\exp(y\log\phi+(1-y)\log(1-\phi))\] <p>\begin{equation} =\exp\bigg(\log\left(\frac{\phi}{1-\phi}\right)y+\log(1-\phi)\bigg). \end{equation}</p> <p>Again notice that this form of the Bernoulli probability mass expression is in the exponential family of the form (\ref{4.eqn.expfam}) with $\eta=\log(\phi/(1-\phi))$, $T(y)=y$, $a(\eta)=-\log(1-\phi)=\log(1+e^\eta)$ and $b(y)=1$. Interestingly, if the definition of $\eta$ is inverted by solving for $\phi$, the relation $\phi=1/(1+e^{-\eta})$ is obtained. One may notice this as the familiar sigmoid function — a fact that will prove useful during the derivation of logistic regression as a GLM.</p> <hr> <h2 id="4-constructing-a-generalised-linear-model">4. Constructing a generalised linear model</h2> <p>Consider a classification or regression problem in which the objective is to predict some target variable $y$ as a function of the explanatory variables $\mathbf{x}$. In order to derive a GLM for this problem, McCullagh and Nelder<d-cite key="McCullagh1989"></d-cite> formally proposed the establishment of three model assumptions (or components):</p> <ol> <li>A <em>random component</em>: Given a matrix of features $\mathbf{X}$ parameterised by $\mathbf{\beta}$, the target variables $\mathbf{y}$ are characterised by a probability distribution that belongs to the exponential family with canonical parameter $\mathbf{\eta}$, written as $\mathbf{y}\mid\mathbf{X};\mathbf{\beta}\sim$ ExpFamily($\mathbf{\eta}$).</li> <li>A <em>systematic component</em>: The linear combination of the input features $\mathbf{X}$ produces a linear predictor $\mathbf{\eta}=\mathbf{X}\mathbf{\beta}$.</li> <li>A <em>functional link</em>: A known monotonic differentiable <em>link function</em> $g(\cdot)$ relates the expected values $\mathbf{\mu}=E[\mathbf{T}(y)\mid\mathbf{X};\mathbf{\beta}]$ of the target variable (random component) to the linear predictor $\mathbf{\eta}=\mathbf{X}\mathbf{\beta}$ of features (systematic component), such that the hypothesis of the GLM is given by $h(\mathbf{\tilde{x}})=E[\mathbf{T}(y)\mid\mathbf{\tilde{x}};\mathbf{\beta}]=g(\mathbf{\tilde{x}}\mathbf{\beta})^{-1}$.</li> </ol> <p>In the case of OLSR, Assumption 1 was merely a Gaussian distribution and Assumption 3 was and identity link function, implying that $\mathbf{\mu}=\mathbf{\eta}$. GLMs, therefore, extend the assumption in (\ref{4.eqn.asmp}) of general linear models to the relation</p> <p>\begin{equation} \mathbf{y}=g(\mathbf{X}\mathbf{\beta})^{-1}+\mathbf{\epsilon}. \end{equation}</p> <p>The distribution of a set of independent target variables $\mathbf{y}$ may now be characterised by any distribution in the exponential family while the homogeneity of variance is not satisfied (it rather varies as a function of the covariate mean)<d-cite key="Ng2012"></d-cite>. Furthermore, the target variable and the explanatory variables no longer assume a linear relationship but are rather related <em>via</em> a link function.</p> <hr> <h2 id="5-logistic-regression">5. Logistic regression</h2> <p>As in the case of OLSR, one may also show that logistic regression is simply a special case of the family of GLMs. Consider the case in which the set of target variables $\mathcal{Y}={y^{(1)},\ldots,y^{(m)}}$ are binary in nature (<em>i.e.</em> $y\in{0,1}$). In this case, it seems natural to model the conditional distribution of $\mathcal{Y}$ given the set of observations $\mathcal{X}$ as a Bernoulli distribution. Consequently, in the case of logistic regression, the hypothesis $h(\mathbf{x})$ is given by</p> <p>\begin{equation} \mkern-28pt h(\mathbf{x})=E[y\mid \mathbf{x};\mathbf{\beta}]\label{4.lr1} \end{equation} \begin{equation} \mkern-41pt =\phi\label{4.lr2} \end{equation} \begin{equation} \mkern-12pt =\frac{1}{1+e^{-\eta}}\label{4.lr3} \end{equation} \begin{equation} =\frac{1}{1+e^{-\mathbf{\beta}^T\mathbf{x}}}.\label{4.lr4} \end{equation}</p> <p>Again, (\ref{4.lr1}) is the functional link of Assumption 3, (\ref{4.lr2}) follows from the fact that $y\mid\mathbf{x};\mathbf{b}\sim \text{Bern}(\phi)$, (\ref{4.lr3}) follows from Assumption 1 and the earlier derivation in Section 3 which showed that $\phi=1/(1+e^{-\eta})$ in the formulation of a Bernoulli distribution as an exponential family distribution. Finally, (\ref{4.lr4}) follows from Assumption 3.</p> <p>Notice that $h(\mathbf{x})$ is always bounded between zero and one since $h(\mathbf{x})\rightarrow 1$ as $\mathbf{\beta}^T\mathbf{x}\rightarrow \infty$ and $h(\mathbf{x})\rightarrow 0$ as $\mathbf{\beta}^T\mathbf{x}\rightarrow -\infty$. By convention, $p(y=1\mid\mathbf{x};\mathbf{\beta})$ can be chosen as</p> <p>\begin{equation} p(y=1\mid\mathbf{x};\mathbf{\beta})=h(\mathbf{x})\label{4.eqn.lr1} \end{equation}</p> <p>which means that</p> <p>\begin{equation} p(y=0\mid\mathbf{x};\mathbf{\beta})=1-h(\mathbf{x}).\label{4.eqn.lr2} \end{equation}</p> <p>Given that $p(y=0\mid\mathbf{x};\mathbf{\beta})+p(y=1\mid\mathbf{x};\mathbf{\beta})=1$, (\ref{4.eqn.lr1}) and (\ref{4.eqn.lr2}) may be combined into the more compact Bernoulli representation</p> <p>\begin{equation} p(y\mid\mathbf{x};\mathbf{\beta})=h(\mathbf{x})^y(1-h(\mathbf{x}))^{1-y}.\label{4.eqn.lr3} \end{equation}</p> <p>Since the $m$ observations in $\mathcal{X}$ are assumed to be generated independently, the likelihood of the parameters $\mathbf{\beta}$ may, therefore, be expressed as</p> \[\mkern-101pt \mathcal{L}(\mathbf{\beta})=p(\mathbf{y}\mid\mathbf{X};\mathbf{\beta})\] \[\mkern-71pt =\prod_{i=1}^mp(y^{(i)}\mid\mathbf{x}^{(i)};\mathbf{\beta})\] <p>\begin{equation} =\prod_{i=1}^m\left(h(\mathbf{x}^{(i)})\right)^{y^{(i)}}\left(1-h(\mathbf{x}^{(i)})\right)^{1-y^{(i)}},\label{4.eqn.lr4} \end{equation}</p> <p>where, again, it is typically easier to maximise (\ref{4.eqn.lr4}) in terms of its log likelihood</p> \[\mkern-120pt \ell(\mathbf{\beta})=\log\mathcal{L}(\beta)\] <p>\begin{equation} =\sum_{i=1}^my^{(i)}\log h(\mathbf{x}^{(i)}+(1-y^{(i)}\log(1-h(\mathbf{x}^{(i)}))\label{4.eqn.lr5}. \end{equation}</p> <p>As previously mentioned, optimal values of $\mathbf{\beta}^{*}$ are typically realised by maximising (\ref{4.eqn.lr5}) using the method of gradient decent<d-cite key="Ng2012"></d-cite>.</p> <p>Often, an additional <em>regularisation</em> term is added to (\ref{4.eqn.lr5}) so as to penalise (\ref{4.eqn.lr5}) for large choices of the value of $\beta$ in an attempt to avoid overfitting<d-cite key="Buhlmannk2011"></d-cite>. Although the notion of regularisation may be applied in the case of many different learning models, this concept is described exclusively in the context of the logistic regression model derivation. The two most popular regularisation types are <em>L1 regularisation</em> (often termed <em>lasso regression</em>) and <em>L2 regularisation</em> (often termed <em>ridge regression</em>). In the case of the former, the logistic regression log likelihood in (\ref{4.eqn.lr5}) can be redefined as</p> <p>\begin{equation} \ell(\mathbf{\beta})=\sum_{i=1}^my^{(i)}\log h(\mathbf{x}^{(i)})+(1-y^{(i)})\log(1-h(\mathbf{x}^{(i)}))-\lambda\sum_{j=1}^n|\beta_j| \end{equation}</p> <p>and, in the case of the latter as</p> <p>\begin{equation} \ell(\mathbf{\beta})=\sum_{i=1}^my^{(i)}\log h(\mathbf{x}^{(i)})+(1-y^{(i)})\log(1-h(\mathbf{x}^{(i)}))-\lambda\sum_{j=1}^n\beta_j^2, \end{equation}</p> <p>where $\lambda$ is a parameter used to inflate the regularisation penalty. If $\lambda=0$, no regularisation is applied while, if a large value of $\lambda$ is chosen, the $\beta$-values will approach zero.</p> <h2 id="6-wrapping-up">6. Wrapping up</h2> <p>And that’s all folks — congratulations for making it to the end! In this blog post, we went deep into GLMs and their relation to the exponential family of distributions. I also showed how the well-known OLSR and Logistic Regression model is meerly a special case of a much broader family of statistical models. Join me in my <a href="https://shanevanheerden.github.io/blog/2018/decision_tree_learning/">next blog post</a> where I’ll describe a much-loved (and less math-heavy) class of machine learning models, namely Decision Trees. See you there!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/bibliography.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 950px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shanevanheerden/shanevanheerden.github.io","data-repo-id":"R_kgDOHll7zA","data-category":"Comments","data-category-id":"DIC_kwDOHll7zM4CWO-7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shane van Heerden. Last updated: January 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T5QSVR2X6Z"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T5QSVR2X6Z");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>