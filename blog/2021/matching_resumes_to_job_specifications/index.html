<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>üìÑ Matching Resumes to Job Specifications | Shane van Heerden</title> <meta name="author" content="Shane van Heerden"> <meta name="description" content="How I used NLP to help efficiently place prospective candidates"> <meta name="keywords" content="machine-learning-engineer, data-scientist, phd, machine-learning, data-science, natural-language-processing, nlp, data-engineering, deep-learning, artificial-intelligence, ai"> <meta property="og:site_name" content="Shane van Heerden"> <meta property="og:type" content="website"> <meta property="og:title" content="Shane van Heerden | üìÑ Matching Resumes to Job Specifications"> <meta property="og:url" content="https://shanevanheerden.github.io/blog/2021/matching_resumes_to_job_specifications/"> <meta property="og:description" content="How I used NLP to help efficiently place prospective candidates"> <meta property="og:image" content="/assets/img/shane.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="üìÑ Matching Resumes to Job Specifications"> <meta name="twitter:description" content="How I used NLP to help efficiently place prospective candidates"> <meta name="twitter:image" content="/assets/img/shane.jpg"> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Shane  van Heerden"
        },
        "url": "https://shanevanheerden.github.io/blog/2021/matching_resumes_to_job_specifications/",
        "@type": "WebSite",
        "description": "How I used NLP to help efficiently place prospective candidates",
        "headline": "üìÑ Matching Resumes to Job Specifications",
        "sameAs": ["https://github.com/shanevanheerden", "https://www.linkedin.com/in/shaneandrewvanheerden"],
        "name": "Shane  van Heerden",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shanevanheerden.github.io/blog/2021/matching_resumes_to_job_specifications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "üìÑ Matching Resumes to Job Specifications",
      "description": "How I used NLP to help efficiently place prospective candidates",
      "published": "August 26, 2021",
      "authors": [
        {
          "author": "Shane van Heerden",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Cape AI",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shane¬†</span>van Heerden</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">üëã about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">üìù blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">üóûÔ∏è news</a> </li> <li class="nav-item "> <a class="nav-link" href="/skills/">üéì skills</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">üóÇ projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">üìö publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">üé® hobbies</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">üìÑ cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>üìÑ Matching Resumes to Job Specifications</h1> <p>How I used NLP to help efficiently place prospective candidates</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#1-the-problem">1. The problem</a></div> <div><a href="#2-understanding-the-clients-needs">2. Understanding the clients needs</a></div> <div><a href="#3-the-solution-architecture">3. The solution architecture</a></div> <div><a href="#4-wrapping-up">4. Wrapping up</a></div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog10.1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog10.1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog10.1-1400.webp"></source> <img src="/assets/img/blog/blog10.1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>I always jump at the opportunity to tackle new and interesting challenges using Machine Learning and Advanced Analytics. In the constantly changing field of AI, this is how I discover and push the bounds of what is truly possible.</p> <h2 id="1-the-problem">1. The problem</h2> <p>In the fast-paced world of job recruitment, staying ahead of the curve requires innovative solutions. Recently, I was tasked with probably my toughest challenges by a client who operates a job recruitment firm within South Africa. I undertook the challenge of revolutionizing the client‚Äôs current recruitment process by harnessing the power of advanced Natural Language Processing (NLP) techniques.</p> <p>In this blog post, I‚Äôll take you through the intricacies of the project, detailing the architecture, preprocessing pipelines, data storage, APIs, trained models, user interface, and the intricate AI logic that powers the system.</p> <hr> <h2 id="2-understanding-the-clients-needs">2. Understanding the clients needs</h2> <p>Understanding the client‚Äôs needs was the crucial first step in the journey. The client‚Äôs existing recruitment process was time-consuming and inefficient, relying heavily on its recruiters to manually review each resume and match it to relevant job openings. This resulted in slower placement times and potential missed opportunities for both job seekers and employers.</p> <p>To address these challenges, the client sought a solution that could not only streamline their current processes by automating the matching of resumes to job specifications but also bring innovation to the forefront of their operations. They wanted a system that could accurately identify relevant resumes for each open position (and vice versa), eliminating the need for manual screening.</p> <p>Before developing the solution, I spent time understanding the specific needs of the client. I met with the company‚Äôs recruiters to understand their key pain points and how they currently matched resumes to job openings. I also analyzed the company‚Äôs data to identify any patterns or trends that could be used to improve the matching process. Through this collaborative process, I gained crucial insights into the intricacies of their current recruitment workflow, enabling myself to tailor my solution to their specific needs. As a result, I developed a set of requirements for the proposed solution:</p> <ul> <li> <strong>Automated</strong>: The solution should be able to automatically match resumes to job openings (and vice versa) without any manual intervention.</li> <li> <strong>Accurate</strong>: The solution should be able to accurately match resumes to job openings (and vice versa) based on the skills, experience, and qualifications of the candidates.</li> <li> <strong>Scalable</strong>: The solution should be able to handle a large volume of resumes and job openings.</li> <li> <strong>Easy to use</strong>: The solution should be easy to use for JobCrystal‚Äôs recruiters.</li> </ul> <hr> <h2 id="3-the-solution-architecture">3. The solution architecture</h2> <p>The success of the project relied heavily on creating a well-designed solution architecture. The system‚Äôs core comprises multiple components collaborating to provide a strong and intelligent recruitment solution. Each stage, from data ingestion to storing results in ElasticSearch, was crafted with emphasis on scalability, efficiency, and smooth integration into the client‚Äôs current infrastructure. Figure 1 visually depicts the interconnected components, showcasing a rough architecture of the solution.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog10.2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog10.2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog10.2-1400.webp"></source> <img src="/assets/img/blog/blog10.2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 1: The architecture of the proposed solution.</em> </div> <h3 id="31-data-initialisation">3.1. Data initialisation</h3> <p>To kickstart the process, the system ingests data from two primary sources: The client‚Äôs FTP server and Dropbox account. The FTP server contains approximately 35 000 unlabelled documents, while Dropbox contributes around 3 000 labeled resumes associated with job specifications. This data undergoes an initialization step, whereby raw text is extracted from the documents, cleaned (using the <a href="https://pypi.org/project/clean-text/" rel="external nofollow noopener" target="_blank">clean-text package</a>) and stored as separate sentences in an Elasticsearch database. Additionally, five metadata extraction steps are executed at this stage which encompass updating information related to file locations, industry and job title labels, and constructing sets of named entities.</p> <h3 id="32-training-the-models">3.2. Training the models</h3> <p>The system utilized various trained models to perform specific tasks associated with document preprocessing and unsupervised ranking. Two <em>Bidirectional Encoder Representations from Transformers</em> (BERT) models are finetuned in order to predict, given the raw resume text, a candidate‚Äôs industry and associated job title, giving rise to the <em>IndustryBERT</em> and <em>JobTitleBERT</em> models, respectively. The fine-tuned models ensure accurate categorization, forming the foundation for subsequent stages in the process.</p> <p>A transformer-based spaCy <em>Named Entity Recognition</em> (NER) model is also trained at this stage to extract key information like key skills, location, degrees and college names which may be used for downstream filtering. Additionally, <em>Uniform Manifold Approximation and Projection</em> (UMAP) and <em>Hierarchical Density-Based Spatial Clustering of Applications with Noise</em> (HDBSCAN) models are fitted to facilitate document vector representations which is an essential component in the subsequent unsupervised ranking step.</p> <h3 id="33-preprocessing-the-data">3.3. Preprocessing the data</h3> <p>All sentences comprising the raw resume text are then fed through various preprocessing steps, the outputs of which are each stored in separate indices in Elasticsearch. During this phase, the finetuned industry and job title BERT models as well as the custom NER model are leveraged, together with two pre-trained BERT-based models. More specifically:</p> <ul> <li>Each resume sentence is classified according to the most likely industry and associated job title using the IndustryBERT and JobTitleBERT models.</li> <li>Resume-specific named entities are extracted from the resume using the custom spaCy NER model.</li> <li>Each resume sentence is converted to a vector representation using the well-known <a href="https://github.com/UKPLab/sentence-transformers" rel="external nofollow noopener" target="_blank">SentenceTransformers</a> package.</li> <li>Each resume sentence is classified according to what part of a resume the sentence likely describes (e.g. experience, education, skills, certifications, awards, hobbies, references, etc.) using <a href="https://huggingface.co/manishiitg/distilbert-resume-parts-classify" rel="external nofollow noopener" target="_blank">a pretrained model</a>.</li> </ul> <h3 id="34-data-storage">3.4. Data storage</h3> <p>Elasticsearch served as the central repository for storing preprocessed document data. Furthermore, the ability to execute database queries according to the <a href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="external nofollow noopener" target="_blank">cosine similarity measure</a> makes it an extremely efficient database for enabling efficient document retrieval. A dedicated AWS EC2 instance is used to host the Elasticsearch instance.</p> <h3 id="35-the-ai-logic">3.5. The ‚ÄúAI logic‚Äù</h3> <p>The AI logic is the heart of the system, orchestrating the matchmaking process. By considering predicted industry and job titles, user-specified keyword entities, and leveraging unsupervised ranking techniques, our system intelligently filters and ranks documents, providing personalized recommendations to users. This is accomplished through 5 sequential steps facilitated by a <a href="https://streamlit.io/" rel="external nofollow noopener" target="_blank">Streamlit</a> user interface displayed in Figure 2.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog10.3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog10.3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog10.3-1400.webp"></source> <img src="/assets/img/blog/blog10.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 2: The graphic user interface for the resume-job matching solution.</em> </div> <ul> <li> <strong>Step 1.</strong> The first step requires the user to select whether they are wanting to match a resume to a job specification.</li> <li> <strong>Step 2.</strong> The user may then upload the resume to the system in their PDF or DOCX format. Uploading a resume triggers the initialisation and prepocessing procedures described in ¬ß3.1 and ¬ß3.3, the results of which are also stored in the Elasticsearch database. During this step, the industry and corresponding job title of the candidate are predicted using the IndustryBERT and JobTitleBERT models desribed in ¬ß3.2, respectively.</li> <li> <strong>Step 3.</strong> The predicted industry and corresponding job title of the candidate are then surfaced to the user as a confirmation. If an errorous prediction was made by the model, the user has the opportunity to correct the classification at this stage. This step is important since the candidate‚Äôs suitability will only be assessed according to those job specifications matching the candidate‚Äôs industry and job title.</li> <li> <strong>Step 4</strong> In some cases, the user may wish to further filter out the suitability of the candidate to a job specification unless specific entities are not present in both documents. More specifically, the user may specify in this step that specific skills, degrees, colleges and/or histroic company names must be present in both documents. This is accomplished by utilising our custom spaCy NER model described in ¬ß3.2.</li> <li> <strong>Step 5</strong>: The combination of all information obtained in Steps 1-4 culminate in this final step. All job specifications which assume the same industry and job titles as the resume confirmed by the user in Step 3, as well as any entities specified by the user in Step 4, are queried from the Elasticsearch database as potential candidates. All candidate job specifications then undergo an unsupervised ranking procedure according to the similarity to the uploaded resume. Two unsupervised ranking algorithms where proposed and evaluated according to preference-based assessments by a job recruitment expert: <ul> <li>The first algorithm relies on ordering all candidate job specifications by utilising Elasticsearch‚Äôs <a href="https://www.elastic.co/search-labs/blog/articles/text-similarity-search-with-vectors-in-elasticsearch" rel="external nofollow noopener" target="_blank">text similarity functionality</a>. More specifically, job specifications are ordered according to the <a href="https://en.wikipedia.org/wiki/Cosine_similarity" rel="external nofollow noopener" target="_blank">cosign similarity</a> between the text vector representations of the job specifications according to the resume‚Äôs vector representation.</li> <li>The second algorithm takes inspiration from the recently proposed <a href="https://arxiv.org/pdf/2008.09470.pdf" rel="external nofollow noopener" target="_blank">Top2Vec approach</a> and utilises the pre-fitted UMAP and HDBSCAN models described in ¬ß3.2. First, the spaCy document vector representation of the resume is reduced from 300 to 80 dimensions using the UMAP model. This is done since UMAP has been shown to be an effective preprocessing step to boost the performance of density based clustering methods. This reduced vector representation then acts as input to the HDBSCAN model which produces a cluster prediction. The most appropriate job specifications to recommend are those which share the same cluster as the resume.</li> </ul> </li> </ul> <p>The effectiveness of the resume matching system was extensively evaluated by the client using the Streamlit user interface.</p> <blockquote> <p>After thoughrough evaluation of the system, the client was impressed with the overall quality of the recommendations and I proceeded to productionising the solution.</p> </blockquote> <hr> <h2 id="4-wrapping-up">4. Wrapping up</h2> <p>By leveraging advanced NLP techniques, I was able to create a system that significantly improved the client‚Äôs efficiency and effectiveness in finding the right candidates for their open positions, as confirmed by the positive client feedback in Figure 3. The client was so impressed with the solution that they decided to reposition their business offering around this new AI-based recruitment matching system, as shown in the accompanying video below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog10.6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog10.6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog10.6-1400.webp"></source> <img src="/assets/img/blog/blog10.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 3: Client feedback.</em> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/jobcrystal.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""></video> </figure> </div> <p>As NLP continues to evolve, I can only expect even more innovative applications in the field of recruitment, further transforming the way companies find and hire top talent. It‚Äôs projects like this that futher cement my belief that Data Science and Machine Learning has the power to revolutionise almost every industry.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/bibliography.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 950px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shanevanheerden/shanevanheerden.github.io","data-repo-id":"R_kgDOHll7zA","data-category":"Comments","data-category-id":"DIC_kwDOHll7zM4CWO-7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Shane van Heerden. Last updated: January 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T5QSVR2X6Z"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T5QSVR2X6Z");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>