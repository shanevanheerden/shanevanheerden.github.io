<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>üß† Neural Language Models | Shane van Heerden</title> <meta name="author" content="Shane van Heerden"> <meta name="description" content="Part 3 - Natural Language Processing Series"> <meta name="keywords" content="machine-learning-engineer, data-scientist, phd, machine-learning, data-science, natural-language-processing, nlp, data-engineering, deep-learning, artificial-intelligence, ai"> <meta property="og:site_name" content="Shane van Heerden"> <meta property="og:type" content="website"> <meta property="og:title" content="Shane van Heerden | üß† Neural Language Models"> <meta property="og:url" content="https://shanevanheerden.github.io/blog/2020/neural_language_models/"> <meta property="og:description" content="Part 3 - Natural Language Processing Series"> <meta property="og:image" content="/assets/img/shane.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="üß† Neural Language Models"> <meta name="twitter:description" content="Part 3 - Natural Language Processing Series"> <meta name="twitter:image" content="/assets/img/shane.jpg"> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Shane  van Heerden"
        },
        "url": "https://shanevanheerden.github.io/blog/2020/neural_language_models/",
        "@type": "WebSite",
        "description": "Part 3 - Natural Language Processing Series",
        "headline": "üß† Neural Language Models",
        "sameAs": ["https://github.com/shanevanheerden", "https://www.linkedin.com/in/shaneandrewvanheerden"],
        "name": "Shane  van Heerden",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shanevanheerden.github.io/blog/2020/neural_language_models/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "üß† Neural Language Models",
      "description": "Part 3 - Natural Language Processing Series",
      "published": "November 12, 2020",
      "authors": [
        {
          "author": "Shane van Heerden",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Cape AI",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shane¬†</span>van Heerden</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">üëã about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">üìù blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">üóûÔ∏è news</a> </li> <li class="nav-item "> <a class="nav-link" href="/skills/">üéì skills</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">üóÇ projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">üìö publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">üé® hobbies</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">üìÑ cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>üß† Neural Language Models</h1> <p>Part 3 - Natural Language Processing Series</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#1-what-exactly-is-a-language-model">1. What exactly is a language model?</a></div> <div><a href="#2-building-our-own-neural-language-model">2. Building our own Neural Language Model</a></div> <div><a href="#3-wrapping-up">3. Wrapping up</a></div> </nav> </d-contents> <p><em>This blog post was originally featured on the <a href="https://medium.com/cape-ai-stories/natural-language-processing-series-3d2857bc67ab" rel="external nofollow noopener" target="_blank">Cape AI Medium page</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog3.1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog3.1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog3.1-1400.webp"></source> <img src="/assets/img/blog/blog3.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This is the third blog post in our series of blog posts focusing on the exciting field of Natural Language Processing! In <a href="https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/">our first post</a>, we saw that the application of neural networks for building language models was a major turning point in the NLP timeline, and in <a href="https://shanevanheerden.github.io/blog/2020/word_embeddings/">our second post</a> we explored the significance of Word Embeddings in advancing the field. With this, we‚Äôre now ready to build our own language model!</p> <h2 id="1-what-exactly-is-a-language-model">1. What exactly is a language model?</h2> <p>In its most simple form:</p> <blockquote> <p>The task of a language model is simply to predict the probability of the next word appearing in a sequence of text given the previous words that have occurred.</p> </blockquote> <p>Traditionally, this problem was tackled with Statistical Language Models which primarily consisted of using so-called n-gram models in combination with some sort of smoothing technique<d-cite key="Kneser1995"></d-cite>. The big pivot in the way researchers thought about this problem occurred when Bengio <em>et al.</em><d-cite key="Bengio2000"></d-cite> proposed using a feed-forward neural network together with a word ‚Äúlookup-table‚Äù for representing the n previous words (often referred to as a <em>token</em>) in a sequence, as shown in Figure 1. Today, this ‚Äúlookup-table‚Äù is known as a <em>word embedding</em> which you may already be familiar with if you read our second blog post. And thus the <em>Neural Language Model</em> was born!</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog3.2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog3.2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog3.2-1400.webp"></source> <img src="/assets/img/blog/blog3.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 1:</em> The first feed-forward neural network used for language modelling<d-cite key="Kneser1995"></d-cite>. </div> <hr> <h2 id="2-building-our-own-neural-language-model">2. Building our own Neural Language Model</h2> <p>We‚Äôre going to keep things very practical in this post by jumping straight into a coding example! In this example, we are going to walk you through:</p> <ol> <li>How you can prepare a document of input text for developing a word-based language model,</li> <li>How you can design and implement your very own neural language model, and</li> <li>How you can use this model to generate some new text with a similar tone to the input text.</li> </ol> <p>Let‚Äôs get started!</p> <h3 id="21-package-installations">2.1. Package installations</h3> <p>We only need to install two packages for this tutorial: good-ol‚Äô numpy and keras (which will do most of the deep learning heavy-lifting for us). Go ahead and run the following in your terminal:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>pip <span class="nb">install </span>numpy
<span class="nv">$ </span>pip <span class="nb">install </span>keras</code></pre></figure> <h3 id="22-creating-a-training-document">2.2. Creating a training document</h3> <p>Now we need some high-quality text. And what better place to look than the much-loved <em>Cat in the Hat</em> story by Dr Seuss that we all probably all read as kids. Thankfully, <a href="https://github.com/robertsdionne" rel="external nofollow noopener" target="_blank">Robert Dionne</a> has already compiled a <a href="https://raw.githubusercontent.com/robertsdionne/rwet/master/hw2/catinthehat.txt" rel="external nofollow noopener" target="_blank">text file</a> containing the full story which we can read in using the following code:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">requests</span>
 
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/robertsdionne/rwet/master/hw2/catinthehat.txt</span><span class="sh">'</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">text</span>
<span class="nf">print</span><span class="p">(</span><span class="n">doc</span><span class="p">[:</span><span class="mi">300</span><span class="p">])</span></code></pre></figure> <p>In the script above, we use the very useful <a href="https://pypi.org/project/requests/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">requests</code></a> module and get the text file directly from GitHub. Let‚Äôs print out the first 300 characters of our document just to be sure we‚Äôve grabbed the right file:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;</span> <span class="n">The</span> <span class="n">Cat</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">Hat</span>
<span class="n">By</span> <span class="n">Dr</span><span class="p">.</span> <span class="n">Seuss</span>
<span class="n">The</span> <span class="n">sun</span> <span class="n">did</span> <span class="ow">not</span> <span class="n">shine</span><span class="p">.</span>
<span class="n">It</span> <span class="n">was</span> <span class="n">too</span> <span class="n">wet</span> <span class="n">to</span> <span class="n">play</span><span class="p">.</span>
<span class="n">So</span> <span class="n">we</span> <span class="n">sat</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">house</span>
<span class="n">All</span> <span class="n">that</span> <span class="n">cold</span><span class="p">,</span> <span class="n">cold</span><span class="p">,</span> <span class="n">wet</span> <span class="n">day</span><span class="p">.</span>
<span class="n">I</span> <span class="n">sat</span> <span class="n">there</span> <span class="k">with</span> <span class="n">Sally</span><span class="p">.</span>
<span class="n">We</span> <span class="n">sat</span> <span class="n">there</span><span class="p">,</span> <span class="n">we</span> <span class="n">two</span><span class="p">.</span>
<span class="n">And</span> <span class="n">I</span> <span class="n">said</span><span class="p">,</span> <span class="sh">"</span><span class="s">How I wish
We had something to do!</span><span class="sh">"</span>
<span class="n">Too</span> <span class="n">wet</span> <span class="n">to</span> <span class="n">go</span> <span class="n">out</span>
<span class="n">And</span> <span class="n">too</span> <span class="n">cold</span> <span class="n">to</span> <span class="n">play</span> <span class="n">ball</span><span class="p">.</span>
<span class="n">So</span> <span class="n">we</span> <span class="n">sat</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">house</span><span class="p">.</span></code></pre></figure> <p>Seems right!</p> <h3 id="23-text-pre-processing">2.3. Text pre-processing</h3> <p>Next, we need to do some text pre-processing in which we will transform our document into a sequence of tokens which we can use to construct a training data set for our model. Based on the short snippet of the story we saw, we clean the text in the following way:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">string</span>
 
<span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">).</span><span class="nf">lower</span><span class="p">()</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="nf">translate</span><span class="p">(</span><span class="nb">str</span><span class="p">.</span><span class="nf">maketrans</span><span class="p">(</span><span class="sh">''</span><span class="p">,</span> <span class="sh">''</span><span class="p">,</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">))</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total tokens: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Vocabulary size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <p>We begin by replacing all new line characters with spaces and converting all characters to their lower case form. Next, we remove any punctuation from our document. We can then split our document into individual tokens (or words) based on the space characters by making use of the sring‚Äôs <code class="language-plaintext highlighter-rouge">.split()</code> method. Let‚Äôs count the number of tokens to see what we have to work with:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Tokens</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">hat</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">by</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">dr</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">seuss</span><span class="sh">'</span><span class="p">,</span> <span class="bp">...</span>
<span class="n">Total</span> <span class="n">tokens</span><span class="p">:</span> <span class="mi">6290</span>
<span class="n">Vocabulary</span> <span class="n">size</span><span class="p">:</span> <span class="mi">855</span></code></pre></figure> <p>Seems like our story contains 6290 tokens in total but has a vocabulary size of only 855 unique words.</p> <p>Now, the next step is to transform our tokens into a set of sequences that can act as our training dataset. For this, let‚Äôs organise our list of tokens into sequences of 64 input words and one target word (giving us a total sequence length of 65). We can do this by ‚Äúsliding‚Äù a window (of size 65) sliding across our list of tokens and joining them together to create a sequence sample.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">length</span> <span class="o">=</span> <span class="mi">64</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)):</span>
  <span class="n">line</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">length</span><span class="p">:</span><span class="n">i</span><span class="p">])</span>
  <span class="n">sequences</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of sequences: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <p>Let‚Äôs also print out how many training sequences we obtained:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Number</span> <span class="n">of</span> <span class="n">sequences</span><span class="p">:</span> <span class="mi">6225</span></code></pre></figure> <p>This should be more than enough training sequences for demonstration purposes.</p> <h3 id="24-prepare-the-dataset">2.4. Prepare the dataset</h3> <p>Although the string representation of our words is nice for humans to look at, it won‚Äôt mean much for a neural network which only deals with numbers. We therefore need to map each of the words in our vocabulary to a corresponding integer value.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
 
<span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
<span class="n">vocab_size</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure> <p>For this, we can use keras‚Äôs <code class="language-plaintext highlighter-rouge">Tokenizer</code> class. We can first define a tokenizer object and fit it on all our set of sequences, which, in effect, finds all unique words in our data and maps each of them to a unique integer ID. More specifically, words are assigned values from 1 to the total number of words. We then use the tokenizer to re-define our sequences to be a set of integers and store the result as a numpy array. At this stage, since the word at the end of the vocabulary will be 855 but Python indexing of arrays starts at zero, we increment the vocabulary size to correct for this.</p> <p>Now that our sequences are properly encoded, we can split them into the set of features X and target variables y.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span></code></pre></figure> <p>For indexing reasons, we utilise numpy‚Äôs handy splicing operation to perform this splitting. After this, we one-hot encode the target word using keras‚Äôs <code class="language-plaintext highlighter-rouge">to_categorical()</code> method which, in effect, transforms our output to a vector of length <code class="language-plaintext highlighter-rouge">vocab_size</code> with a value of 1 in the place of the word‚Äôs position and a value of 0 everywhere else. It will then be the job of the model to learn a probability distribution over all words in our vocabulary.</p> <h3 id="25-define-and-train-the-model">2.5. Define and train the model</h3> <p>Hooray! We‚Äôve arrived at the fun part of choosing the structure of our neural language model and training it.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Embedding</span>
 
<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span></code></pre></figure> <p>We‚Äôll keep things fairly standard by defining a keras sequential model and adding some layers to it. More specifically, we will use an Embedding Layer to learn the representation of words, as well as a <em>Long Short-Term Memory</em> (LSTM) recurrent neural network to learn to predict words based on their context. The inputs to the Embedding layer is the vocabulary size, the length of the embedding vector (which we choose as 100) and the length of the sequence. We also specify a layer size of 128 for both LSTM layers (since powers of two are computationally more efficient). Finally, we will define two fully connected layers which will be used to interpret the features extracted from the sequence, the first having again 128 layers and a <em>Rectified Linear Unit</em> (ReLU) activation function and the last having 856 layers and a softmax activation function (to ensure the outputs are scaled between zero and one). We can print out a summary of our model as a sort of sanity check:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Model</span><span class="p">:</span> <span class="sh">"</span><span class="s">sequential</span><span class="sh">"</span>
<span class="n">_________________________________________________________________</span>
<span class="nc">Layer </span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>                 <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   
</span><span class="o">=================================================================</span>
<span class="nf">embedding_1 </span><span class="p">(</span><span class="n">Embedding</span><span class="p">)</span>      <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>            <span class="mi">54784</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">lstm_1 </span><span class="p">(</span><span class="n">LSTM</span><span class="p">)</span>                <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>           <span class="mi">98816</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">lstm_2 </span><span class="p">(</span><span class="n">LSTM</span><span class="p">)</span>                <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>               <span class="mi">131584</span>    
<span class="n">_________________________________________________________________</span>
<span class="nf">dense_1 </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>               <span class="mi">16512</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">dense_2 </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">856</span><span class="p">)</span>               <span class="mi">110424</span>    
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">412</span><span class="p">,</span><span class="mi">120</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">412</span><span class="p">,</span><span class="mi">120</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">_________________________________________________________________</span></code></pre></figure> <p>Everything seems in order. Now we can compile and train our model as follows:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span></code></pre></figure> <p>We define a cross entropy loss function which is sensible since we are technically dealing with a multi-class classification problem. We will also specify that keras must use the efficient Adam optimizer for updating the model weights evaluated on accuracy. Finally, we fit the model on our data for 100 training epochs with a fairly modest batch size of 128. Now all we have to do is go grab a coffee and let our model train.</p> <h3 id="26-training-the-model">2.6. Training the model</h3> <p>Congratulations! You have just trained your very own neural language model! Let‚Äôs explore your new model‚Äôs capabilities by generating some random text with it. For this, we are going to create a function that takes as input the model and associated tokenizer we have just created, together with the sequence length, number of words to generate, and some input text which will act as a starting point in the generation process.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
 
<span class="k">def</span> <span class="nf">generate_sequence</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_words</span><span class="p">,</span> <span class="n">input_text</span><span class="p">):</span>
  <span class="n">word_lookup</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">((</span><span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_words</span><span class="p">):</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">([</span><span class="n">input_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">([</span><span class="n">encoded</span><span class="p">],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict_classes</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_word</span> <span class="o">=</span> <span class="n">word_lookup</span><span class="p">[</span><span class="n">yhat</span><span class="p">]</span>
    <span class="n">input_text</span> <span class="o">+=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span> <span class="o">+</span> <span class="n">new_word</span>
 <span class="k">return</span> <span class="n">input_text</span></code></pre></figure> <p>We begin by defining a dictionary that can act as a sort of lookup that provides us with the string representation of a word given a word‚Äôs ID. Next, we encode the input text according to the mapping defined by our tokenizer. To ensure that the input text doesn‚Äôt grow too long, we truncate the text to the sequence length required by our model using Keras‚Äôs <code class="language-plaintext highlighter-rouge">pad_sequences()</code> method. We can now pass this encoded sequence to our model as input and the output we receive is the ID of the most likely word in the sequence. We can then use our lookup dictionary to get the string representation of our word and append it to our input text with a space.</p> <p>All that‚Äôs left to do now is try our function out.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">input_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">then the cat turned around and laughed</span><span class="sh">"</span>
<span class="nf">generate_sequence</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">input_text</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span></code></pre></figure> <p>We pass the model, tokenizer and sequence length, as well as some input text and specify that 10 additional tokens must be generated. And the moment of truth‚Ä¶</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sh">'</span><span class="s">then the cat turned around and laughed you should a yink i have a lot of one my teeth are gold</span><span class="sh">'</span></code></pre></figure> <p>Interesting choice of words, but I don‚Äôt quite think our model is going to be the next Dr Seuss. Nonetheless, this should give you a good enough idea about how neural language models are working under the hood. Advanced neural language models produce significantly better results since they utilise much more sophisticated architectures, are trained on much more text and have significantly more parameters compared to our toy example. But I encourage you to take this example and make it your own. Try playing around with the model architecture and training hyperparameters or even change the input text to something completely different and see what kind of results you can generate!</p> <hr> <h2 id="3-wrapping-up">3. Wrapping up</h2> <p>And that‚Äôs all! In this post, I walked you through how to create your very own neural language model in Python with some help from Keras and how you can use this model to generate your own text sequences.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/bibliography.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 950px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shanevanheerden/shanevanheerden.github.io","data-repo-id":"R_kgDOHll7zA","data-category":"Comments","data-category-id":"DIC_kwDOHll7zM4CWO-7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Shane van Heerden. Last updated: January 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T5QSVR2X6Z"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T5QSVR2X6Z");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>