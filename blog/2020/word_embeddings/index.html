<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>📊 Word Embeddings | Shane van Heerden</title> <meta name="author" content="Shane van Heerden"> <meta name="description" content="Part 2 - Natural Language Processing Series"> <meta name="keywords" content="machine-learning-engineer, data-scientist, phd, machine-learning, data-science, natural-language-processing, nlp, data-engineering, deep-learning, artificial-intelligence, ai"> <meta property="og:site_name" content="Shane van Heerden"> <meta property="og:type" content="website"> <meta property="og:title" content="Shane van Heerden | 📊 Word Embeddings"> <meta property="og:url" content="https://shanevanheerden.github.io/blog/2020/word_embeddings/"> <meta property="og:description" content="Part 2 - Natural Language Processing Series"> <meta property="og:image" content="/assets/img/shane.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="📊 Word Embeddings"> <meta name="twitter:description" content="Part 2 - Natural Language Processing Series"> <meta name="twitter:image" content="/assets/img/shane.jpg"> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Shane  van Heerden"
        },
        "url": "https://shanevanheerden.github.io/blog/2020/word_embeddings/",
        "@type": "WebSite",
        "description": "Part 2 - Natural Language Processing Series",
        "headline": "📊 Word Embeddings",
        "sameAs": ["https://github.com/shanevanheerden", "https://www.linkedin.com/in/shaneandrewvanheerden"],
        "name": "Shane  van Heerden",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shanevanheerden.github.io/blog/2020/word_embeddings/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "📊 Word Embeddings",
      "description": "Part 2 - Natural Language Processing Series",
      "published": "October 12, 2020",
      "authors": [
        {
          "author": "Shane van Heerden",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Cape AI",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shane </span>van Heerden</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">👋 about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">📝 blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">🗞️ news</a> </li> <li class="nav-item "> <a class="nav-link" href="/skills/">🎓 skills</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">🗂 projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">📚 publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">🎨 hobbies</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">📄 cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>📊 Word Embeddings</h1> <p>Part 2 - Natural Language Processing Series</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#1-what-are-word-embeddings">1. What are Word Embeddings?</a></div> <div><a href="#2-word2vec-methods">2. Word2Vec Methods</a></div> <div><a href="#3-training-a-word2vec-model">3. Training a Word2Vec Model</a></div> <div><a href="#4-word2vec-tutorial">4. Word2Vec Tutorial</a></div> <div><a href="#5-wrapping-up">5. Wrapping up</a></div> </nav> </d-contents> <p><em>This blog post was originally featured on the <a href="https://medium.com/cape-ai-stories/natural-language-processing-series-3f412d3ab933" rel="external nofollow noopener" target="_blank">Cape AI Medium page</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.1-1400.webp"></source> <img src="/assets/img/blog/blog2.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This is the second post in a series of blog posts focusing on the exciting field of <em>Natural Language Processing</em> (NLP)! In <a href="https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/">our previous blog post</a>, we saw that word embeddings were an important milestone in the history of NLP. Although these were used as far back as 2001, in 2013 Mikolov <em>et al.</em><d-cite key="Mikolov2013"></d-cite> proposed a simple but novel method for efficiently training word embeddings (or <em>word2vec</em> models) on very large unlabelled corpora which ultimately led to their wide-scale adoption. So, what actually are word embeddings and how do they fit into an NLP practitioner’s toolkit? Let’s find out!</p> <h2 id="1-what-are-word-embeddings">1. What are Word Embeddings?</h2> <p>A word embedding is simply an alternative representation for text where words are represented as real-valued vectors as opposed to a sequence of string characters. This representation is <em>learnt</em> in such a way that words that share the same or similar meaning also have similar vector representations.</p> <p>To get a feel for this concept, let’s look at Figure 1 which is a visual representation of a word’s vector, where red indicates a value close to 2 and blue a value close to -2. As we can see, the vectors for “man” and “woman” appear to be more similar in colour to each other than when compared to the word “king”. A mystical property of word embeddings is that they often capture semantic meaning between words. This means that we can add and subtract word vectors and arrive at intuitive results. Probably the most famous of these is the vector resulting from taking the word “king”, subtracting the word “man” and adding the word “woman”. The resultant vector is visually shown in Figure 1 and, if we compare this against all 400 000 words that make up the embedding vocabulary, we find that the vector for the word “queen” is in fact the closest to this resultant vector!</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.2-1400.webp"></source> <img src="/assets/img/blog/blog2.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 1:</em> A visual representation of the word vectors corresponding to gender-specific words<d-cite key="Alammar2019"></d-cite>. </div> <hr> <h2 id="2-word2vec-methods">2. Word2Vec Methods</h2> <p>Now that we have a bit of an intuitive feel of word embeddings, let’s get a better grasp on the theory surrounding word embeddings. In their paper, Mikolov et al. proposed two methods for creating word embeddings, namely the <em>Continuous Bag-of-Words</em> (CBOW) and <em>Skip-gram</em> method, as shown in Figure 2. In the CBOW method, the current word <em>w(t)</em> is predicted based on the context of the surrounding words. The Skip-gram method does the exact opposite by attempting to predict the surrounding words given the current word <em>w(t)</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.3-1400.webp"></source> <img src="/assets/img/blog/blog2.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 2:</em> The CBOW and Skip-gram method<d-cite key="Mikolov2013"></d-cite>. </div> <p>Let’s expand on this a bit more by diving into how one would go about constructing a dataset from an unlabelled corpus of text to leverage each of these two methods. For a more in-depth explanation of these concepts, we highly recommend reading <a href="http://jalammar.github.io/illustrated-word2vec/" rel="external nofollow noopener" target="_blank">Jay Alammar’s blog post</a> on this subject which inspired this section.</p> <h3 id="21-continuous-bag-of-words">2.1. Continuous Bag-of-Words</h3> <p>As a way of example, suppose we had a large text corpus which contained the sentence: “<em>Thou shalt not make a machine in the likeness of a human mind</em>”. In order to create a dataset from this, the CBOW method, in essence, slides a context window of a fixed word size (let’s say three in this case) over the sentence, as illustrated in Figure 3. In this case, we take the first two words in the window as input features and the last word to be the output label. We repeat this process of constructing the dataset by moving the window one word on until the end of the corpus has been reached.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.4-1400.webp"></source> <img src="/assets/img/blog/blog2.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 3:</em> The CBOW method for constructing a dataset<d-cite key="Alammar2019"></d-cite>. </div> <h3 id="22-skip-gram">2.2. Skip-gram</h3> <p>Continuing with the previous example, the Skip-gram model constructs a dataset in a slightly different way. Instead of just considering the n previous words when trying to predict a target word, we now consider predicting all the words within a specific context window surrounding a specific word. A context window could be, say, two words before and after the word being considered. This is illustrated in Figure 4. In this case, the centre word in the window is the input word and a target word is created for each surrounding word. Again, we repeat this window-sliding process of constructing the dataset until the end of the corpus has been reached.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.5-1400.webp"></source> <img src="/assets/img/blog/blog2.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 4:</em> The skip-gram method for constructing a dataset<d-cite key="Alammar2019"></d-cite>. </div> <p>In reality, training a model on this form of the dataset is computationally expensive. This is due to the fact that we have to compute the error of the neural language model against the whole output vocabulary vector for every training sample! In practice, a technique called <em>negative sampling</em> is employed. This involves transforming the dataset shown in Figure 4 to a new form shown in Figure 5. In this procedure, the input and output words are now both features and a new target column is added, where a value of <code class="language-plaintext highlighter-rouge">1</code> indicates that the two words are neighbours.</p> <p>In other words, the task has now changed from predicting the neighbour of a word to predicting if two words are neighbours.</p> <p>In order to ensure that not all samples have a target variable of one, we introduce a few negative samples (hence the method’s name) to the dataset by sampling a set of random words from our vocabulary and setting the corresponding target label to zero.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.6-1400.webp"></source> <img src="/assets/img/blog/blog2.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 5:</em> The notion of negative sampling in the skip-gram method<d-cite key="Alammar2019"></d-cite>. </div> <hr> <h2 id="3-training-a-word2vec-model">3. Training a Word2Vec Model</h2> <p>Alright, quick checkpoint: CBOW and Skip-gram with negative sampling are methods used to create/train word embeddings and not for training a word2vec model. Let us now dive into the process of training a word2vec model.</p> <p>At the beginning, we initialise each word in our dataset as a vector (with some specified length) of random values. In each training step, we look at just a single positive sample with its associated negative samples, as shown in Figure 6. If we compute the dot product between the input and output word vectors for each sample and squash this value through a sigmoid function (so that all values are between zero and one), we obtain a specific score. Now, we know that in the case of the positive sample the resultant sigmoid score should be close to one (because the two words are indeed neighbours) and the others should be close to zero. We can therefore compute an error by subtracting this sigmoid score from the target labels.</p> <p>Here comes the <em>learning</em> part: we can now use this error to adjust the values of each word vector in such a way so that we compute a lower error score the next time. We continue this process for all word vectors in our dataset, incrementally adjusting their values to achieve slightly better embeddings until the learning algorithm (which is usually a neural network) converges.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.7-1400.webp"></source> <img src="/assets/img/blog/blog2.7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 6:</em> Training a Skip-gram language model with negative sampling<d-cite key="Alammar2019"></d-cite>. </div> <hr> <h2 id="4-word2vec-tutorial">4. Word2Vec Tutorial</h2> <blockquote> <p>Time to code!</p> </blockquote> <p>Okay, enough with the theory. Let’s get a bit more practical by showing you how to implement this in the real world using Python. In this tutorial, we are going to walk you through how to construct your own text corpus and how to train your very own word2vec model.</p> <h3 id="41-package-installations-and-imports">4.1. Package Installations and Imports</h3> <p>First things first, we need to install and import a few packages. We will begin by installing the <a href="https://pypi.org/project/beautifulsoup4/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">beautifulsoup4</code></a> package which will be used for cleaning up scraped data from Wikipedia, together with the <a href="https://lxml.de/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">lxml</code></a> package to parse HTML content from Wikipedia pages. Additionally, we will also need the <a href="https://www.nltk.org/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">nltk</code></a> and <a href="https://pypi.org/project/gensim/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">gensim</code></a> packages to do most of the NLP heavy-lifting for us.</p> <p>Go ahead and run:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>pip <span class="nb">install </span>beautifulsoup4
<span class="nv">$ </span>pip <span class="nb">install </span>lxml
<span class="nv">$ </span>pip <span class="nb">install </span>nltk
<span class="nv">$ </span>pip <span class="nb">install </span>gensim</code></pre></figure> <h3 id="42-creating-a-corpus">4.2. Creating a Corpus</h3> <p>In practice, word2vec models are commonly trained on millions of words. For illustration purposes, however, in this tutorial, we are just going to scrape a small corpus of text from the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Machine_learning" rel="external nofollow noopener" target="_blank">Machine Learning</a> to train our model.</p> <p>The following Python script does this for us:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">bs4</span> <span class="k">as</span> <span class="n">bs</span>
<span class="kn">import</span> <span class="n">urllib.request</span>
<span class="kn">import</span> <span class="n">re</span>

<span class="c1"># Scrape text from Wikipedia
</span><span class="n">scrapped_html</span> <span class="o">=</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="nf">urlopen</span><span class="p">(</span><span class="sh">'</span><span class="s">https://en.wikipedia.org/wiki/Machine_learning</span><span class="sh">'</span><span class="p">)</span>
<span class="n">raw_html</span> <span class="o">=</span> <span class="n">scrapped_html</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>  <span class="c1"># Parse all paragraphs
</span><span class="n">parsed_html</span> <span class="o">=</span> <span class="n">bs</span><span class="p">.</span><span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">raw_html</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>
<span class="n">paragraphs</span> <span class="o">=</span> <span class="n">parsed_html</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">p</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">paragraphs</span><span class="p">])</span></code></pre></figure> <p>In the script above, we first scrape and read all html data from the Wikipedia page using the <code class="language-plaintext highlighter-rouge">urlopen</code> method from the <a href="https://pypi.org/project/requests/" rel="external nofollow noopener" target="_blank"><code class="language-plaintext highlighter-rouge">requests</code></a> package. We then parse the raw html into a <code class="language-plaintext highlighter-rouge">BeautifulSoup</code> object and extract all text contained within <code class="language-plaintext highlighter-rouge">p</code> (paragraph) tags using the <code class="language-plaintext highlighter-rouge">find_all</code> method. Finally, we combine all of the paragraphs into a single text string.</p> <h3 id="43-text-pre-processing">4.3. Text Pre-Processing</h3> <p>The next step involves pre-processing the text so that it is in the correct format for training the Gensim Word2Vec model:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">nltk</span>
<span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>

<span class="c1"># Clean the text
</span><span class="n">clean_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>
<span class="n">clean_text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">[^a-zA-Z]</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">clean_text</span><span class="p">)</span>
<span class="n">clean_text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\s+</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">clean_text</span><span class="p">)</span>

<span class="c1"># Prepare the dataset
</span><span class="n">sentences</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="nf">sent_tokenize</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">nltk</span><span class="p">.</span><span class="nf">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>

<span class="c1"># Remove any stopwords
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
    <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)]</span></code></pre></figure> <p>In the script above, we clean the text by first converting all characters to their lowercase form and then removing any digits, special characters and trailing spaces that may exist in the text. Next, we prepare the dataset into a collection of words so that it can be used to train Gensim’s Word2Vec model. We first split our <code class="language-plaintext highlighter-rouge">clean_text</code> into individual sentences using nltk’s <code class="language-plaintext highlighter-rouge">sent_tokenize</code> method and then further into individual words using the <code class="language-plaintext highlighter-rouge">word_tokenize</code> method. Finally, we cycle through our collection of words and remove any which are in nltk’s predefined list of English stopwords (or common words like “the”, “and”, “to”, etc).</p> <h3 id="44-creating-a-word2vec-model">4.4. Creating a Word2Vec Model</h3> <p>With this, we can now easily train our own Word2Vec model as follows:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="c1"># Create the model
</span><span class="n">word2vec</span> <span class="o">=</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Print vocabulary of model
</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">vocabprint</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span></code></pre></figure> <p>Here, we simply instantiate a Gensim Word2Vec object using our collection of words. In this instantiation, we specify a value of <code class="language-plaintext highlighter-rouge">1</code> for the <code class="language-plaintext highlighter-rouge">sg</code> parameter and a value of <code class="language-plaintext highlighter-rouge">5</code> for the <code class="language-plaintext highlighter-rouge">negative</code> parameter — telling Gensim to utilise the skip-gram model and employ 5 negative samples respectively. We also specify a value of <code class="language-plaintext highlighter-rouge">2</code> for the <code class="language-plaintext highlighter-rouge">min_count</code> parameter so that the model only includes words that have appeared at least twice in our corpus. We can view a list of all of the unique words of the model by printing out the keys of the <code class="language-plaintext highlighter-rouge">wv.vocab</code> attribute.</p> <h3 id="45-exploring-the-model">4.5. Exploring the Model</h3> <p>Congratulations! You now have your very own word2vec model trained on your own corpus of text. Let’s explore your creation a bit further.</p> <p>We know that the word2vec model is used to map all words to a vector representation. We can view the vector corresponding to any word in our model’s vocabulary as follows:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Print vector of word
</span><span class="nf">print</span><span class="p">(</span><span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">'</span><span class="s">learning</span><span class="sh">'</span><span class="p">])</span>

<span class="o">&gt;&gt;</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.0321747</span>  <span class="o">-</span><span class="mf">0.09254234</span>  <span class="mf">0.15719296</span> <span class="o">-</span><span class="mf">0.18106991</span> <span class="o">-</span><span class="mf">0.05903807</span>  <span class="mf">0.06826855</span> <span class="o">-</span><span class="mf">0.0546078</span>  <span class="o">-</span><span class="mf">0.14852802</span>  <span class="mf">0.00465734</span>  <span class="mf">0.00357654</span> <span class="o">-</span><span class="mf">0.11191269</span>  <span class="mf">0.00123405</span> <span class="o">-</span><span class="mf">0.00260608</span> <span class="o">-</span><span class="mf">0.01252941</span>  <span class="mf">0.03632444</span>  <span class="mf">0.05868229</span> <span class="o">-</span><span class="mf">0.01147301</span>  <span class="mf">0.01271282</span> <span class="mf">0.20245183</span> <span class="o">-</span><span class="mf">0.08651368</span> <span class="o">-</span><span class="mf">0.02063143</span>  <span class="mf">0.10166611</span>  <span class="mf">0.05482733</span> <span class="o">-</span><span class="mf">0.06414133</span> <span class="o">-</span><span class="mf">0.09078009</span>  <span class="mf">0.03067572</span> <span class="o">-</span><span class="mf">0.06758043</span>  <span class="mf">0.00037985</span>  <span class="mf">0.10958873</span>  <span class="mf">0.00133575</span> <span class="o">-</span><span class="mf">0.17905715</span> <span class="o">-</span><span class="mf">0.20932317</span> <span class="o">-</span><span class="mf">0.08205332</span> <span class="o">-</span><span class="mf">0.02681177</span> <span class="o">-</span><span class="mf">0.04361923</span>  <span class="mf">0.06187554</span> <span class="o">-</span><span class="mf">0.03218165</span> <span class="o">-</span><span class="mf">0.12924905</span>  <span class="mf">0.02013807</span>  <span class="mf">0.06686062</span>  <span class="mf">0.0272868</span>  <span class="o">-</span><span class="mf">0.02437109</span> <span class="mf">0.00260349</span> <span class="o">-</span><span class="mf">0.02714068</span>  <span class="mf">0.02746866</span>  <span class="mf">0.05804974</span>  <span class="mf">0.08129382</span> <span class="o">-</span><span class="mf">0.03174321</span> <span class="mf">0.08987447</span> <span class="o">-</span><span class="mf">0.05901277</span>  <span class="mf">0.07368492</span>  <span class="mf">0.04791974</span>  <span class="mf">0.08164269</span> <span class="o">-</span><span class="mf">0.05956567</span> <span class="o">-</span><span class="mf">0.12904255</span> <span class="o">-</span><span class="mf">0.07182135</span> <span class="o">-</span><span class="mf">0.0183635</span>   <span class="mf">0.0681318</span>   <span class="mf">0.09037064</span>  <span class="mf">0.03774516</span> <span class="mf">0.12213644</span>  <span class="mf">0.1648079</span>  <span class="o">-</span><span class="mf">0.18914327</span>  <span class="mf">0.02890724</span> <span class="o">-</span><span class="mf">0.02372251</span> <span class="o">-</span><span class="mf">0.11012954</span> <span class="o">-</span><span class="mf">0.1020454</span>  <span class="o">-</span><span class="mf">0.01853278</span> <span class="o">-</span><span class="mf">0.05396225</span> <span class="o">-</span><span class="mf">0.00242959</span> <span class="o">-</span><span class="mf">0.10753106</span> <span class="o">-</span><span class="mf">0.07141761</span> <span class="mf">0.10347419</span> <span class="o">-</span><span class="mf">0.10766012</span> <span class="o">-</span><span class="mf">0.16925317</span> <span class="o">-</span><span class="mf">0.10747337</span> <span class="o">-</span><span class="mf">0.08550954</span>  <span class="mf">0.01928806</span> <span class="mf">0.2854533</span>  <span class="o">-</span><span class="mf">0.12928957</span>  <span class="mf">0.05894407</span>  <span class="mf">0.05380363</span>  <span class="mf">0.06413457</span> <span class="o">-</span><span class="mf">0.18124926</span> <span class="mf">0.06807419</span> <span class="o">-</span><span class="mf">0.12306273</span> <span class="o">-</span><span class="mf">0.16328035</span>  <span class="mf">0.14293176</span> <span class="o">-</span><span class="mf">0.23447195</span> <span class="o">-</span><span class="mf">0.07432053</span> <span class="o">-</span><span class="mf">0.1858378</span>   <span class="mf">0.07563769</span>  <span class="mf">0.06148988</span>  <span class="mf">0.10675731</span> <span class="o">-</span><span class="mf">0.03910929</span> <span class="o">-</span><span class="mf">0.00869188</span> <span class="o">-</span><span class="mf">0.05734098</span>  <span class="mf">0.07828433</span>  <span class="mf">0.04085582</span>  <span class="mf">0.0544676</span><span class="p">]</span></code></pre></figure> <p>From these outputs, we can see that Gensim, by default, maps all words to a one hundred dimensional numpy vector. As we have seen, one of the benefits of using word2vec is that the model is able to capture the semantic meaning of words in relation to one another. Let’s try and confirm this by printing out the words most similar (in vector space) to the word “learning”:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Print most similar words
</span><span class="nf">print</span><span class="p">(</span><span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="sh">'</span><span class="s">learning</span><span class="sh">'</span><span class="p">))</span>

<span class="o">&gt;&gt;</span>  <span class="p">[(</span><span class="sh">'</span><span class="s">machine</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9990278482437134</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9989383220672607</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9988012313842773</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">training</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9987066984176636</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">biases</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9984654784202576</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">set</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9983999729156494</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">classification</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9983198046684265</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">used</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9982497692108154</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">use</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9981957674026489</span><span class="p">),(</span><span class="sh">'</span><span class="s">systems</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9980695247650146</span><span class="p">)]</span></code></pre></figure> <p>The list above shows us the words most similar to the word “learning” and displays their corresponding cosine similarities. No surprise, the word “machine” appears at the top of our list. We can also see words such as “data”, “model” and “training” also appearing near the top which means they often occur with the word “learning” — this makes sense. We should therefore be quite confident that our model has indeed successfully managed to capture some sort of semantic relationship between words using just a single Wikipedia page!</p> <hr> <h2 id="5-wrapping-up">5. Wrapping up</h2> <p>And there you have it! In this blog post, we described the two flavours of word2vec models, namely CBOW and Skip-gram as well as the model training procedure. Armed with this information, we got a bit more practical by walking you through how to create your very own word2vec model in Python using the Gensim library.</p> <p>Stay tuned to this series to learn more about the awesome world of NLP as we share more on the latest developments, code implementations and thought-provoking perspectives on NLP’s impact on the way we interact with the world. It’s an extremely exciting time for anyone to get into the world of NLP!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/bibliography.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 950px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shanevanheerden/shanevanheerden.github.io","data-repo-id":"R_kgDOHll7zA","data-category":"Comments","data-category-id":"DIC_kwDOHll7zM4CWO-7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Shane van Heerden. Last updated: January 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T5QSVR2X6Z"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T5QSVR2X6Z");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>