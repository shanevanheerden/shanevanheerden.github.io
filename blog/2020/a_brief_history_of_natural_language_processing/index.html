<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>üï∞Ô∏è A Brief History of Natural Language Processing | Shane van Heerden</title> <meta name="author" content="Shane van Heerden"> <meta name="description" content="Part 1 - Natural Language Processing Series"> <meta name="keywords" content="machine-learning-engineer, data-scientist, phd, machine-learning, data-science, natural-language-processing, nlp, data-engineering, deep-learning, artificial-intelligence, ai"> <meta property="og:site_name" content="Shane van Heerden"> <meta property="og:type" content="website"> <meta property="og:title" content="Shane van Heerden | üï∞Ô∏è A Brief History of Natural Language Processing"> <meta property="og:url" content="https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/"> <meta property="og:description" content="Part 1 - Natural Language Processing Series"> <meta property="og:image" content="/assets/img/shane.jpg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="üï∞Ô∏è A Brief History of Natural Language Processing"> <meta name="twitter:description" content="Part 1 - Natural Language Processing Series"> <meta name="twitter:image" content="/assets/img/shane.jpg"> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Shane  van Heerden"
        },
        "url": "https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/",
        "@type": "WebSite",
        "description": "Part 1 - Natural Language Processing Series",
        "headline": "üï∞Ô∏è A Brief History of Natural Language Processing",
        "sameAs": ["https://github.com/shanevanheerden", "https://www.linkedin.com/in/shaneandrewvanheerden"],
        "name": "Shane  van Heerden",
        "@context": "https://schema.org"
      }
    </script> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/icon.png"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "üï∞Ô∏è A Brief History of Natural Language Processing",
      "description": "Part 1 - Natural Language Processing Series",
      "published": "October 7, 2020",
      "authors": [
        {
          "author": "Shane van Heerden",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Cape AI",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Shane¬†</span>van Heerden</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">üëã about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">üìù blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">üóûÔ∏è news</a> </li> <li class="nav-item "> <a class="nav-link" href="/skills/">üéì skills</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">üóÇ projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">üìö publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/hobbies/">üé® hobbies</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">üìÑ cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>üï∞Ô∏è A Brief History of Natural Language Processing</h1> <p>Part 1 - Natural Language Processing Series</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#1-neural-language-models-2001">1. Neural Language Models (2001)</a></div> <div><a href="#2-multi-task-learning-2008">2. Multi-task Learning (2008)</a></div> <div><a href="#3-word-embeddings-2013">3. Word Embeddings (2013)</a></div> <div><a href="#4-neural-networks-for-nlp-2013">4. Neural Networks for NLP (2013)</a></div> <div><a href="#5-sequence-to-sequence-models-2014">5. Sequence-to-Sequence Models (2014)</a></div> <div><a href="#6-attention-mechanisms-2015">6. Attention Mechanisms (2015)</a></div> <div><a href="#7-pre-trained-language-models-2018">7. Pre-trained Language Models (2018)</a></div> <div><a href="#8-where-we-are-today-and-looking-forward">8. Where we are today and looking forward‚Ä¶</a></div> </nav> </d-contents> <p><em>This blog post was originally featured on the <a href="https://medium.com/cape-ai-stories/natural-language-processing-series-8d51b87b6004" rel="external nofollow noopener" target="_blank">Cape AI Medium page</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.1-1400.webp"></source> <img src="/assets/img/blog/blog1.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This is the first blog post in a series focusing on the wonderful world of Natural Language Processing (NLP)! In this post I‚Äôll present you with the lay of the land ‚Äî describing seven important milestones which have been achieved in the NLP field over the last 20 years. This is largely inspired by <a href="https://www.youtube.com/watch?v=sGVi4gb90zk" rel="external nofollow noopener" target="_blank">Sebastian Ruder‚Äôs talk</a> at the 2018 Deep Learning Indaba which <a href="https://shanevanheerden.github.io/news/news11/">I attended</a> in Stellenbosch.</p> <p>Short disclaimer before I begin: This post is heavily skewed towards neural network-based advancements. Many of these milestones, however, were built on many influential ideas presented by non-neural network-based work during the same era, which, for brevity purposes, have been omitted from this post.</p> <h2 id="1-neural-language-models-2001">1. Neural Language Models (2001)</h2> <p>It‚Äôs 2001 and the field of NLP is quite nascent. Academics all around the world are beginning to think more about how language could be modelled. After a lot of research, Neural Language models are born. Language modelling is simply the task of determining the probability of the next word (often referred to as a <em>token</em>) occurring in a piece of text given all the previous words. Traditional approaches for tackling this problem were based on n-gram models in combination with some sort of smoothing technique<d-cite key="Kneser1995"></d-cite>. Bengio <em>et al.</em><d-cite key="Bengio2000"></d-cite> were the first to propose using a feed-forward neural network, a so-called word ‚Äúlookup-table‚Äù, for representing the <em>n</em> previous words in a sequence as illustrated in Figure 1. Today, this is known as <em>word embeddings</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.2-1400.webp"></source> <img src="/assets/img/blog/blog1.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 1:</em> The first feed-forward neural network used for language modelling<d-cite key="Kneser1995"></d-cite>. </div> <hr> <h2 id="2-multi-task-learning-2008">2. Multi-task Learning (2008)</h2> <p>Excitement and interest grows steadily in the years following Neural Language models. Advances in computer hardware allow researchers to push the boundaries on language modelling, giving rise to new NLP methods. One such method is multi-task learning. The notion of multi-task learning involves training models to solve more than one learning task, while also using a set of shared parameters. As a result, models are forced to learn a representation that exploits the commonalities and differences across all tasks.</p> <p>Collobert and Weston<d-cite key="Collobert2008"></d-cite> were the first to apply a form of multi-task learning in the NLP domain back in 2008. They trained two convolutional models with max pooling to perform both part-of-speech and named entity recognition tagging, while also sharing a common word lookup table (or word embedding), as shown in Figure 2. Years later, their paper was highlighted by many experts as a fundamental milestone in deep learning for NLP and received the Test-of-time Award at the 2018 <em>International Conference on Machine Learning</em> (ICML).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.3-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.3-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.3-1400.webp"></source> <img src="/assets/img/blog/blog1.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 2:</em> The first multi-task model sharing a common word lookup table<d-cite key="Collobert2008"></d-cite>. </div> <hr> <h2 id="3-word-embeddings-2013">3. Word Embeddings (2013)</h2> <p>If you‚Äôve had any exposure to NLP, the first thing you have probably come across is the idea of word embeddings (or more commonly known as <em>word2vec</em>). Although we have seen that word embeddings have been used as far back as 2001, in 2013 Mikolov <em>et al.</em><d-cite key="Mikolov2013"></d-cite> proposed a simple but novel method for efficiently training these word embeddings on very large unlabeled corpora which ultimately led to their wide-scale adoption.</p> <p>Word embeddings attempt to create a dense vector representation of text, and addresses many challenges faced with using traditional sparse bag-of-words representation. Word embeddings were shown to capture every intuitive relationship between words such as gender, verb tense and country capital, as illustrated in Figure 3.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.4-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.4-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.4-1400.webp"></source> <img src="/assets/img/blog/blog1.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 3:</em> The intuitive relationships captured by word embeddings<d-cite key="Mikolov2013"></d-cite>. </div> <hr> <h2 id="4-neural-networks-for-nlp-2013">4. Neural Networks for NLP (2013)</h2> <p>Looking back, 2013 appeared to be an inflection point in the NLP field, as research and development grew exponentially thereon. The advancements in word embeddings ultimately sparked the wider application of neural networks in NLP. The key challenge that needed to be addressed was architecturally allowing sequences of variable lengths to be inputted into the neural net which ultimately lead to three architectures emerging, namely: <em>recurrent neural networks</em> (RNNs) (which were soon replaced by <em>long-short term memory</em> (LSTM) networks), <em>convolutional neural networks</em> (CNNs), and recursive neural networks. Today, these neural network architectures have produced exceptional results and are widely used for many NLP applications.</p> <hr> <h2 id="5-sequence-to-sequence-models-2014">5. Sequence-to-Sequence Models (2014)</h2> <p>Soon after the emergence of RNNs and CNNs for language modelling, Sutskever <em>et al.</em><d-cite key="Sutskever2014"></d-cite> were the first to propose a general framework for mapping one sequence to another, which is now known as <em>sequence-to-sequence</em> models. In this framework, an encoder network processes an input sequence token by token and compresses it into a vector representation, represented by the blue layers in Figure 4. A decoder network (represented by the red layers) is then used to predict a new sequence of output tokens based on the encoder state, which takes every previously predicted token as input.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.5-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.5-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.5-1400.webp"></source> <img src="/assets/img/blog/blog1.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 4:</em> A visual representation of a sequence-to-sequence model used for translation<d-cite key="See2016"></d-cite>. </div> <p>This architecture is particularly useful in tasks such as <em>machine translation</em> (MT) and <em>natural language generation</em> (NLG). It‚Äôs no surprise that, in 2016, Google announced that it is in the process of replacing all of its statistical-based MT systems with neural MT models<d-cite key="Wu2016"></d-cite>. Additionally, since the decoder model can be conditioned on any arbitrary representation, it can also be used for tasks like generating captions for images<d-cite key="Vinyals2015"></d-cite>.</p> <hr> <h2 id="6-attention-mechanisms-2015">6. Attention Mechanisms (2015)</h2> <p>Although useful in a wide range of tasks, sequence-to-sequence models were struggling with being able to capture long-range dependencies between words in text. In 2015, the concept of attention was introduced by Bahdanau <em>et al.</em><d-cite key="Bahdanau2015"></d-cite> as a way of addressing this bottleneck. In essence, attention in a neural network is a mechanism for deciding which parts of the input sequence to attend to when routing information. Various attention mechanisms have also been applied in the computer vision space for image captioning<d-cite key="Xu2015"></d-cite>, which also provides a glimpse into the inner workings of the model, as is seen in Figure 5.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.6-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.6-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.6-1400.webp"></source> <img src="/assets/img/blog/blog1.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 5:</em> A visual representation of the attention mechanism in an image captioning model<d-cite key="Xu2015"></d-cite>. </div> <p>Attention is not only restricted to the input sequence and can also be used to focus on surrounding words in a body of text ‚Äî commonly referred to as <em>self attention</em> ‚Äî to obtain more contextual meaning. This is at the heart of the current state-of-the-art <em>transformer</em> architecture, proposed by Vaswani <em>et al.</em><d-cite key="Vaswani2017"></d-cite> in 2017, which is composed of multiple self-attention layers. The transformer sparked an explosion of new language model architectures (and an inside joke among AI practitioners regarding Sesame Street Muppets), the most notable being <em>Bidirectional Encoder Representations from Transformers</em> (BERT) and <em>Generative Pre-trained Transformers</em> (GPT).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.7-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.7-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.7-1400.webp"></source> <img src="/assets/img/blog/blog1.7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 6:</em> The various language model architectures based on the transformer model<d-cite key="Wang2019"></d-cite>. </div> <h2 id="7-pre-trained-language-models-2018">7. Pre-trained Language Models (2018)</h2> <p>Dai &amp; Le<d-cite key="Dai2015"></d-cite>. were the first to propose using pre-trained language models in 2015 but this notion was only recently shown to be beneficial across a broad range of NLP-related tasks. More specifically, it was shown that pre-trained language models could be <em>fine-tuned</em> on other data related to a specific target task<d-cite key="Ramachandran2017, Howard2018"></d-cite>. Additionally, language model embeddings could also be used as features in a target model leading to significant improvements over the then state-of-the-art models<d-cite key="Peters2018"></d-cite>, as shown in Figure 7.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.8-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.8-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.8-1400.webp"></source> <img src="/assets/img/blog/blog1.8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <em>Figure 7:</em> Improvements over state-of-the-art models when employing language model embeddings<d-cite key="Peters2018"></d-cite>. </div> <hr> <h2 id="8-where-we-are-today-and-looking-forward">8. Where we are today and looking forward‚Ä¶</h2> <p>Nowadays, there exists an array of initiatives aimed at open-sourcing many large state-of-the-art pre-trained models. These models can be fine-tuned to perform various NLP-related tasks like <em>sequence classification</em>, <em>extractive question answering</em>, <em>named entity recognition</em> and <em>text summarization</em> (to name a few). NLP is advancing at an incredible pace and is giving rise to global communities dedicated to solving the world‚Äôs most important problems through language understanding.</p> <p>Stay tuned to this series to learn more about the awesome world of NLP as I share more on the latest developments, code implementations and thought-provoking perspectives on NLP‚Äôs impact on the way we interact with the world. It‚Äôs an extremely exciting time for anyone to get into the world of NLP!</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/bibliography.bib"></d-bibliography> <div id="giscus_thread" style="max-width: 950px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"shanevanheerden/shanevanheerden.github.io","data-repo-id":"R_kgDOHll7zA","data-category":"Comments","data-category-id":"DIC_kwDOHll7zM4CWO-7","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"top","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> ¬© Copyright 2025 Shane van Heerden. Last updated: January 18, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-T5QSVR2X6Z"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-T5QSVR2X6Z");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>