<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://shanevanheerden.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://shanevanheerden.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-18T08:20:51+00:00</updated><id>https://shanevanheerden.github.io/feed.xml</id><title type="html">blank</title><subtitle>Shane van Heerden&apos;s personal website. </subtitle><entry><title type="html">🚀 Jetpack — A Declarative Framework for Scaling Feature Engineering in Databricks</title><link href="https://shanevanheerden.github.io/blog/2024/jetpack_a_declarative_framework_for_scaling_feature_engineering_in_databricks/" rel="alternate" type="text/html" title="🚀 Jetpack — A Declarative Framework for Scaling Feature Engineering in Databricks"/><published>2024-09-06T00:00:00+00:00</published><updated>2024-09-06T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2024/jetpack_a_declarative_framework_for_scaling_feature_engineering_in_databricks</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2024/jetpack_a_declarative_framework_for_scaling_feature_engineering_in_databricks/"><![CDATA[<h1 id="-work-in-progress-">🚧 <b>WORK IN PROGRESS</b> 🚧</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog14.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog14.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog14.1-1400.webp"/> <img src="/assets/img/blog/blog14.1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="1-introduction">1. Introduction</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog14.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog14.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog14.2-1400.webp"/> <img src="/assets/img/blog/blog14.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1: The components needed to put an ML system into production.</em> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/jetpack.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <hr/> <h2 id="2-the-need-for-a-feature-store">2. The need for a feature store</h2> <p>Feature engineering is a critical step in machine learning pipelines, where raw data is transformed into meaningful features that drive model performance. With ML practitioners typically spending 60-70% of their time on feature and data engineering, streamlining this process has the ability to significantly accelerate a company’s AI maturity. However, traditional feature engineering approaches often result in:</p> <ul> <li><strong>Complexity:</strong> Features are scattered across multiple scripts, notebooks, and databases.</li> <li><strong>Fragility:</strong> Changes to feature definitions or dependencies can break downstream models.</li> <li><strong>Scalability:</strong> Feature computation and management become cumbersome as feature stores grow.</li> </ul> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog14.3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog14.3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog14.3-1400.webp"/> <img src="/assets/img/blog/blog14.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(a)</center> <p><br/></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog14.4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog14.4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog14.4-1400.webp"/> <img src="/assets/img/blog/blog14.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(b)</center> <div class="caption"> <em>Figure 2: Data flow of ML systems (a) without and (b) with a feature store.</em> </div> <p>To address these challenges, we’ve introduced a declarative framework for facilitating feature engineering at Luno, which we’ve named <strong>Jetpack</strong>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog14.5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog14.5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog14.5-1400.webp"/> <img src="/assets/img/blog/blog14.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 3: The custom feature engineering infrastructure developed at major tech companies.</em> </div> <hr/> <h2 id="3-goals-of-jetpack">3. Goals of Jetpack</h2> <p>The primary goal of Jetpack is to:</p> <blockquote> <p>Accelerate feature engineering by enabling Data Scientists to <em>declaratively</em> define efficient <em>end-to-end</em> feature pipelines and easily manage the <em>operational lifecycle</em> of features.</p> </blockquote> <p>By defining features using high-level configurations, we decouple feature definitions from their implementation details. This approach brings numerous benefits, namely:</p> <ul> <li><strong>Simplification:</strong> Feature definitions are concise, readable, and maintainable.</li> <li><strong>Reusability:</strong> Features can be easily shared across models and projects and better organised according to standard entities.</li> <li><strong>Flexibility:</strong> Changes to feature definitions are properly managed and propagated to downstream models.</li> <li><strong>Scalability:</strong> Feature computation and management can be optimised for performance.</li> </ul> <p>This approach is largely inspired by <a href="https://www.infoq.com/presentations/fabricator-ml/">Doordash’s approach</a> to scaling feature engineering albeit reimagined within a Databricks context. In about a year of adopting this approach, Doordash was able to scale their number of unique features by 5x, their daily feature values by 10x, and their feature jobs by 8.33x.</p> <hr/> <h2 id="4-how-does-jetpack-work">4. How does Jetpack work?</h2> <p>On a high level and with reference to Figure 4, all features are constructed from upstream silver and/or gold tables, or even existing feature store tables. The definition of the feature together with the orchestration details are specified as a simplified YAML config files. The actual orchestration of features is facilitated by the Jetpack system which periodically writes feature values to downstream feature store tables.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog14.6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog14.6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog14.6-1400.webp"/> <img src="/assets/img/blog/blog14.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 4: The high-level jetpack architecture.</em> </div> <p>Three main categories of information are required when defining a feature, namely:</p> <ol> <li><strong>Upstream table dependency information</strong> which is needed to facilitate the feature computation,</li> <li><strong>Feature information</strong> which describes the logic needed to compute a feature as well as additional metadata, and</li> <li><strong>Orchestration information</strong> which describe the details of how the feature should be computed.</li> </ol> <p>An example of a feature configuration file is shown below. In this example, we want to create a series of features that count the number of broker buys and sells for a customer over multiple time windows. Take some time to read through the comments in the example config file to get a better idea of the information needed to define this feature.</p> <pre><code class="language-YAML"># Upstream
dependencies:  # Add all upstream table dependencies used in the 'sql' feature definition below must be defined here. You can add other table dependency by extending this list.
  - table: base.bitx.vw_users  # This is the table in catalog.schema.table format.
    alias: users  # This is an alias name you can give the table to use in the 'sql' feature definition below.
  - table: base.broker.quotes
    alias: quotes

# Feature
name: count_broker_[quote_type.key]s_[day_window]d  # This is the name of the feature. Parameter injection notation, defined in 'params', can be used here.
description: The number of broker [quote_type.key]s exercised by the customer in the last [day_window] days.  # This is the description of the feature. Parameter injection notation, defined in 'params', can be used here.
tags:  # These are the tags associated with the feature. Tags are useful when grouping features by domain.
  - fraud
  - churn
is_pii: false  # Set this to 'true' if your feature is PII.
data_type: numerical  # This is the data type of the feature. Parameter injection notation, defined in 'params', can be used here. Supported data types include: 'numerical', 'binary', 'categorical' and 'ordinal'.
version: 1  # If this is the first version of the feature, this should have a value of 1. If you are proposinging updated logic to an existing feature that is being used by downstream models, a new feature yaml file should be created with an incremented version number. Depricated features should be retired in the corresponding '_archive' folder only when all upstream consumers have been migrated off the feature.
params:  # These are parameters that can be injected into the 'name', 'description', 'sql' and 'orchestration.depends_on' fields. This is useful for cases when you want to generate many different features from the same underlying logic but with different parameter values. You can use parameter injection by specifying parameter names using square bracket notation (i.e. [])). There are two parameter types that you may choose:
  day_window: [7, 14, 30, 90, 180, 365]  # List parameters are defined like this. This means that anywhere we specify [day_window], the values 7, 14, 30, 90, 180 and 365 from this list would be sequentially injected.
  quote_type: {"buy": 1, "sell": 2}  # Dictionary parameters are defined like this. This means that anywhere we specify [quote_type.keys], the values 'buy' and 'sell' would be sequentially injected. In the same way, anywhere we specify [quote_type.values], the values '1' and '2' would be sequentially injected.
# In this example, since there are 6 'day_window' parameter values and 2 'quote_type' parameter values, this example.yaml file will create 6*2=12 feature columns in our final feature store.
sql: &gt;-  # This is where you define the logic for your feature. Your SQL must contain a column called "id" (and optionally "reference_date"), together with the single feature value column. If you are defining features on a users level, a final join on users is recommended like in this example. If a "reference_date" column is not specified, it will be set automatically based on the 'cadence' specified under 'orchestration' below. The '&gt;-' syntax at the beginning of this SQL statement allows us to write the SQL statement over multiple lines.
      WITH user_broker_tx AS (
          SELECT user_id, COUNT(DISTINCT id) as count_broker_tx
          FROM {quotes}  # This is the 'alias' table name defined in 'dependencies' above.
          WHERE status = 4 -- Complete
          AND quote_type = [quote_type.value] -- Buy/Sell
          AND exercised_at IS NOT NULL
          AND base_currency in [CRYPTO_CURRENCIES]  # Specific static lists (not defined under 'params') can also be injected into the SQL query. This is useful when you do not want to hardcode specific values which may change over time (e.g. the crypto currency codes we support). For a list of supported lists, see here: https://gitlab.com/lunomoney/product-engineering/py/-/blob/main/notebooks/ml/features/jetpack.py#L26-31.
          AND DATEDIFF(CAST(GETDATE() AS Date),CAST(exercised_at AS Date)) &lt;= [day_window] # The [day_window] values are being injected here.
          GROUP BY user_id
      )
      SELECT id, coalesce(count_broker_tx, 0) AS [NAME] # The NAME parameter should be used to ensure the feature name is identical to the value specifed under the 'name' attribute
      FROM {users} a  # This is the 'alias' table name defined in 'dependencies' above.
      LEFT JOIN user_broker_tx b
      ON a.id = b.user_id

# Downstream
orchestration:  # All parameters concering the orchestration of the feature are defined here.
  enabled: false  # Set this to 'true' if you want to enable computation of this feature.
  cadence: daily  # This is how often we write out the feature. Supported cadences include: 'daily', 'weekly' and 'monthly'.
  cluster: small  # This is the cluster size to be used to compute the feature. Supported types include: 'small'.
  engine: standard  # This is the runtime engine to be used to compute the feature. Supported types include: 'standard' and 'photon'.
  depends_on:
    - feature_v1  # If this feature depends on a lower level feature, the dependency should be defined here.
</code></pre> <p>Assuming a set of feature configurations have been defined, Jetpack is now employed to facilitate the feature orchestration and computation process. A detailed visual description of the feature computation process is shown in Figure 5.</p> <p>The process is initiated by a job which assumes a set of <code class="language-plaintext highlighter-rouge">cadence</code>, <code class="language-plaintext highlighter-rouge">cluster</code> and <code class="language-plaintext highlighter-rouge">engine</code> specifications. When the job kicks off, any features with the same <code class="language-plaintext highlighter-rouge">cadence</code>, <code class="language-plaintext highlighter-rouge">cluster</code> and <code class="language-plaintext highlighter-rouge">engine</code> specifications as the job are identified as the subset of features to be computed in this job run.</p> <p>Since one may specify dependencies between features (using the <code class="language-plaintext highlighter-rouge">depends_on</code> feature attribute), this results in a directed acyclic graph (DAG) of feature orderings. Hence, features must therefore be organised into batches and computed sequentially. In other words and with reference to Figure 2, since <code class="language-plaintext highlighter-rouge">feature2.yaml</code> depends on <code class="language-plaintext highlighter-rouge">feature1.yaml</code> (i.e. the <code class="language-plaintext highlighter-rouge">sql</code> logic in <code class="language-plaintext highlighter-rouge">feature2.yaml</code> needs the results of <code class="language-plaintext highlighter-rouge">feature1.yaml</code>), we first compute <code class="language-plaintext highlighter-rouge">feature1.yaml</code> in <code class="language-plaintext highlighter-rouge">Batch 1</code> and merge the result into the feature store before proceeding with <code class="language-plaintext highlighter-rouge">Batch 2</code> which contains <code class="language-plaintext highlighter-rouge">feature2.yaml</code>.</p> <p>In a computational batch, the <code class="language-plaintext highlighter-rouge">sql</code> logic defining the feature’s value is computed using the <code class="language-plaintext highlighter-rouge">spark.sql()</code> method, where all upstream <code class="language-plaintext highlighter-rouge">dependiencies</code> are passed as keyword arguments to reduce the number of read operations if there are shared table dependencies between features in the batch. The individual feature results are then combined using an outer join and subsequently merged into the appropriate feature store.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog14.7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog14.7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog14.7-1400.webp"/> <img src="/assets/img/blog/blog14.7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 5: A visual depiction of the feature orchestration and computation process facilitated by the Jetpack framework.</em> </div> <hr/> <h2 id="5-wrapping-up">5. Wrapping up</h2>]]></content><author><name>Shane van Heerden</name></author><category term="CaseStudy"/><summary type="html"><![CDATA[How I reduced feature creation and operationalisation time by 10x]]></summary></entry><entry><title type="html">👨🏼‍💻 An Interview with Shane van Heerden</title><link href="https://shanevanheerden.github.io/blog/2023/an_interview_with_shane_van_heerden/" rel="alternate" type="text/html" title="👨🏼‍💻 An Interview with Shane van Heerden"/><published>2023-08-28T00:00:00+00:00</published><updated>2023-08-28T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2023/an_interview_with_shane_van_heerden</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2023/an_interview_with_shane_van_heerden/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog9.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog9.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog9.1-1400.webp"/> <img src="/assets/img/blog/blog9.1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="1-what-is-your-name-and-role-and-luno">1. What is your name and role and Luno?</h2> <p>Hi, I’m Shane and I’m a Senior Data Scientist at Luno.</p> <hr/> <h2 id="2-what-has-been-your-career-journey">2. What has been your career journey?</h2> <p>After finishing school, I, like many others, was a bit stumped about what career I wanted to pursue. At the time, the best I could do was lean into the subjects I did well in and was most passionate about at the time, namely: Physics, Business Studies and Maths. I believe this is what ultimately convinced me to pursue an undergraduate in Industrial Engineering at Stellenbosch University in 2013, a degree that ultimately taught me to think like an engineer with a business hat on.</p> <p>Throughout my undergraduate years, I discovered my deepest passions ignited when a module incorporated elements of programming and mathematics. A pivotal point in my career journey occurred right at the end of my undergraduate in 2016 from an unexpected source: YouTube. A recommended Stanford lecture introduced me to the realm of <em>“Machine Learning,”</em> a paradigm enabling computers to make intelligent decisions without explicit, programmed instructions. I was hooked! So much so that I decided to pursue a PhD in Data Science and Machine Learning which I successfully defended at the beginning of 2020.</p> <p>Transitioning from academia, I was eager to discover how my newfound, mostly theoretical Data Science and Machine Learning knowledge could actually solve real-world problems. My first job at a Data Science consulting startup gave me the chance to do just that. I worked on a bunch of different Data Science projects in various industries, which was a great learning experience. I think this experience really prepared me well for the unique problems I would encounter at Luno.</p> <p>I have been working at Luno for about 2 years now and I’m amazed at the personal and professional growth journey I have been on. During this time, I have transitioned from being more in the fun-loving Customer Service space to now grappling with the complex world of anti-money laundering and fraud prevention. Each day, I find myself even more amazed at the extraordinary lengths Luno goes to make sure customers are safe.</p> <hr/> <h2 id="3-what-do-you-enjoy-about-your-role">3. What do you enjoy about your role?</h2> <p>The best thing about being a Data Scientist is the variety and depth of the problems that I get to work on everyday. Coming from a strong research background, I enjoy the intellectual challenge of trying to crack a really tough problem. Even more than that, the satisfaction I get from seeing that solution driving real value is what gets me out of bed every morning.</p> <hr/> <h2 id="4-what-are-the-top-three-tasks-you-would-complete-day-to-day">4. What are the top three tasks you would complete day-to-day?</h2> <p>One of the greatest privileges about being a Data Scientist is that your skill set gives you the ability to solve many different types of problems in many interesting ways. In that same breath, no two days ever look the same for me. Some themes of tasks do remain the same though:</p> <ol> <li>Most mornings start off with connecting with my fellow Data Science colleagues in our morning standup. This is usually followed by an informal collaboration session where we can exchange ideas or help each other out if we are struggling with a specific problem. I’ve found having this space to connect is so important especially in this more digital work world we now find ourselves in.</li> <li>If I’m at the very beginnings of understanding a problem and what possible solutions I am considering, most of my time will be spent engaging in meetings with stakeholders or engineers, reading blog posts on how other people have approached similar problems, searching for data to see if it even exists, or formulating my thoughts on a blank canvas with sticky notes and diagrams so that I can easily bounce some of my ideas off my other colleagues.</li> <li>If I’m in the later stages of a project where I’ve already converged on a solution, most of the time I’ll be building out a data pipeline to get the data I need into a usable form, tinkering around with specific model configurations in a Jupyter notebook, thinking about the best way to convey a piece of data or preparing for an upcoming presentation to stakeholders.</li> </ol> <hr/> <h2 id="5-what-are-the-top-skills-you-need-to-master-to-succeed-in-your-role">5. What are the top skills you need to master to succeed in your role?</h2> <p>This is a tough one. Naturally, as a Data Scientist, one may gravitate towards those hard skills like Python, Statistics or even Deep Learning. Although important, I have found that what really distinguishes someone in this field is their ability to pair their hard skills with some very important soft skills. Here are the top 3 skills I think every Data Scientist should master:</p> <ol> <li><strong>Have a curious mind.</strong> The field of Data Science is constantly changing. People around the world are discovering new and interesting ways to solve problems using Data Science techniques. An approach that was state-of-the-art a month ago will soon become obsolete. Although I would definitely not advise going down every new rabbit hole, I believe remaining deeply curious about the Data Science field is incredibly important for fueling new creative ideas to problem solving. Moreover, let this curiosity extend into the way in which you understand and approach people and problems!</li> <li><strong>Learn how to listen.</strong> One of the common pitfalls Data Scientists very often stumble into is rushing into a solution without truly understanding the problem they are trying to solve. It takes a great deal of humility to put all the fancy models and algorithms aside to truly listen and understand the problem a stakeholder is facing. Encourage open dialogue by using prompts such as <em>“Could you share more about your process?”</em> and <em>“Can you walk me through your plan once I provide you a solution?”</em>. Listening ultimately nurtures a stronger connection with the problem itself rather than being fixated on a preconceived solution.</li> <li><strong>Master the art of storytelling.</strong> One of the early lessons I learnt in my Data Science journey was that no matter how ingenious your solution is for someone’s problem, it won’t count for much if you can’t explain it in a way that the decision-makers can easily grasp. Slapping a bunch of graphs onto a PowerPoint without considering the journey you’re guiding stakeholders through is a recipe for disappointment. Our brains are just not good at retaining raw facts for very long, and they often don’t drive the meaningful actions we’re aiming for. Rather, we are wired to understand, retain and deeply connect with people through stories. Take your stakeholders on the journey of how you arrived at your solution. Mastering storytelling, of course, takes practice, and I’m definitely still learning. But one thing you can do is seize every opportunity you get to present, even though it may seem scary at times!</li> </ol> <hr/> <h2 id="6-what-advice-would-you-give-to-someone-looking-to-develop-into-the-position-you-are-in">6. What advice would you give to someone looking to develop into the position you are in?</h2> <p>Leaning very much on the the skills I previously mentioned, there are a few nuggets of advice I would have loved to give myself earlier on my journey as a Data Scientist:</p> <ol> <li><strong>Don’t focus too much on the method.</strong> Too often, as a young Data Scientist, one can get caught up in debates like <em>“Should I use Python or R, Plotly or Seaborn, PyTorch or Tensorflow?”</em>. Just pick one! What’s more important is that you are able to extract value from data using one of these tools.</li> <li><strong>Plan your problem approach.</strong> When embarking on tackling a Data Science problem, there is often no shortage of possible solution approaches you could begin to explore. If only you also had copious amounts of time to go down each rabbit hole! Through experience, you’ll get better at predicting which solution avenues are likely to yield the best results and knowing when a path is likely heading towards a dead end. Taking some time ahead to plan out what you are thinking of exploring and validating or even improving on some of these ideas by getting feedback from other colleagues and/or stakeholders can be super crucial to your success.</li> <li><strong>Understand your solution deeply.</strong> I know it may sound obvious, but once you have proposed a solution to a problem, make sure you understand your solution deeply. If, for example, you have proposed a new model that is going to predict fraud, make sure it’s trustworthy by knowing how that model is coming to its conclusions. Don’t only look at metrics like AUC or R² but rather interrogate your model by looking at how it misclassified specific examples and understanding its shortfalls. In this process, it’s also important to document how you arrived at your solution and all the assumptions you may have had to make along the way. Ultimately, this will build trust in your solution among your stakeholders which will hopefully lead to the desired impact you are wanting to achieve.</li> </ol> <hr/> <h2 id="7-what-has-been-your-highlight-at-luno">7. What has been your highlight at Luno?</h2> <p>The crypto landscape is constantly changing and filled with new challenges. Being in a company with the enthusiasm and willingness for innovation makes it so easy to get excited for work every morning. It may sound cliche, but this is why the people at Luno are an everyday highlight! I am truly grateful that I have the opportunity to learn from such incredibly talented colleagues that push me to become the best version of myself. Moreover, I’ve been fortunate enough to have an exceptional manager who not only fosters an environment where I can develop the skills I want to grow in but also trusts my opinions and views me as a partner in Data Science discovery.</p>]]></content><author><name>Shane van Heerden</name></author><category term="Personal"/><summary type="html"><![CDATA[Life in the shoes of a Senior Data Scientist at Luno]]></summary></entry><entry><title type="html">📄 Matching Resumes to Job Specifications</title><link href="https://shanevanheerden.github.io/blog/2021/matching_resumes_to_job_specifications/" rel="alternate" type="text/html" title="📄 Matching Resumes to Job Specifications"/><published>2021-08-26T00:00:00+00:00</published><updated>2021-08-26T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2021/matching_resumes_to_job_specifications</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2021/matching_resumes_to_job_specifications/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog10.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog10.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog10.1-1400.webp"/> <img src="/assets/img/blog/blog10.1.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I always jump at the opportunity to tackle new and interesting challenges using Machine Learning and Advanced Analytics. In the constantly changing field of AI, this is how I discover and push the bounds of what is truly possible.</p> <h2 id="1-the-problem">1. The problem</h2> <p>In the fast-paced world of job recruitment, staying ahead of the curve requires innovative solutions. Recently, I was tasked with probably my toughest challenges by a client who operates a job recruitment firm within South Africa. I undertook the challenge of revolutionizing the client’s current recruitment process by harnessing the power of advanced Natural Language Processing (NLP) techniques.</p> <p>In this blog post, I’ll take you through the intricacies of the project, detailing the architecture, preprocessing pipelines, data storage, APIs, trained models, user interface, and the intricate AI logic that powers the system.</p> <hr/> <h2 id="2-understanding-the-clients-needs">2. Understanding the clients needs</h2> <p>Understanding the client’s needs was the crucial first step in the journey. The client’s existing recruitment process was time-consuming and inefficient, relying heavily on its recruiters to manually review each resume and match it to relevant job openings. This resulted in slower placement times and potential missed opportunities for both job seekers and employers.</p> <p>To address these challenges, the client sought a solution that could not only streamline their current processes by automating the matching of resumes to job specifications but also bring innovation to the forefront of their operations. They wanted a system that could accurately identify relevant resumes for each open position (and vice versa), eliminating the need for manual screening.</p> <p>Before developing the solution, I spent time understanding the specific needs of the client. I met with the company’s recruiters to understand their key pain points and how they currently matched resumes to job openings. I also analyzed the company’s data to identify any patterns or trends that could be used to improve the matching process. Through this collaborative process, I gained crucial insights into the intricacies of their current recruitment workflow, enabling myself to tailor my solution to their specific needs. As a result, I developed a set of requirements for the proposed solution:</p> <ul> <li><strong>Automated</strong>: The solution should be able to automatically match resumes to job openings (and vice versa) without any manual intervention.</li> <li><strong>Accurate</strong>: The solution should be able to accurately match resumes to job openings (and vice versa) based on the skills, experience, and qualifications of the candidates.</li> <li><strong>Scalable</strong>: The solution should be able to handle a large volume of resumes and job openings.</li> <li><strong>Easy to use</strong>: The solution should be easy to use for JobCrystal’s recruiters.</li> </ul> <hr/> <h2 id="3-the-solution-architecture">3. The solution architecture</h2> <p>The success of the project relied heavily on creating a well-designed solution architecture. The system’s core comprises multiple components collaborating to provide a strong and intelligent recruitment solution. Each stage, from data ingestion to storing results in ElasticSearch, was crafted with emphasis on scalability, efficiency, and smooth integration into the client’s current infrastructure. Figure 1 visually depicts the interconnected components, showcasing a rough architecture of the solution.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog10.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog10.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog10.2-1400.webp"/> <img src="/assets/img/blog/blog10.2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1: The architecture of the proposed solution.</em> </div> <h3 id="31-data-initialisation">3.1. Data initialisation</h3> <p>To kickstart the process, the system ingests data from two primary sources: The client’s FTP server and Dropbox account. The FTP server contains approximately 35 000 unlabelled documents, while Dropbox contributes around 3 000 labeled resumes associated with job specifications. This data undergoes an initialization step, whereby raw text is extracted from the documents, cleaned (using the <a href="https://pypi.org/project/clean-text/">clean-text package</a>) and stored as separate sentences in an Elasticsearch database. Additionally, five metadata extraction steps are executed at this stage which encompass updating information related to file locations, industry and job title labels, and constructing sets of named entities.</p> <h3 id="32-training-the-models">3.2. Training the models</h3> <p>The system utilized various trained models to perform specific tasks associated with document preprocessing and unsupervised ranking. Two <em>Bidirectional Encoder Representations from Transformers</em> (BERT) models are finetuned in order to predict, given the raw resume text, a candidate’s industry and associated job title, giving rise to the <em>IndustryBERT</em> and <em>JobTitleBERT</em> models, respectively. The fine-tuned models ensure accurate categorization, forming the foundation for subsequent stages in the process.</p> <p>A transformer-based spaCy <em>Named Entity Recognition</em> (NER) model is also trained at this stage to extract key information like key skills, location, degrees and college names which may be used for downstream filtering. Additionally, <em>Uniform Manifold Approximation and Projection</em> (UMAP) and <em>Hierarchical Density-Based Spatial Clustering of Applications with Noise</em> (HDBSCAN) models are fitted to facilitate document vector representations which is an essential component in the subsequent unsupervised ranking step.</p> <h3 id="33-preprocessing-the-data">3.3. Preprocessing the data</h3> <p>All sentences comprising the raw resume text are then fed through various preprocessing steps, the outputs of which are each stored in separate indices in Elasticsearch. During this phase, the finetuned industry and job title BERT models as well as the custom NER model are leveraged, together with two pre-trained BERT-based models. More specifically:</p> <ul> <li>Each resume sentence is classified according to the most likely industry and associated job title using the IndustryBERT and JobTitleBERT models.</li> <li>Resume-specific named entities are extracted from the resume using the custom spaCy NER model.</li> <li>Each resume sentence is converted to a vector representation using the well-known <a href="https://github.com/UKPLab/sentence-transformers">SentenceTransformers</a> package.</li> <li>Each resume sentence is classified according to what part of a resume the sentence likely describes (e.g. experience, education, skills, certifications, awards, hobbies, references, etc.) using <a href="https://huggingface.co/manishiitg/distilbert-resume-parts-classify">a pretrained model</a>.</li> </ul> <h3 id="34-data-storage">3.4. Data storage</h3> <p>Elasticsearch served as the central repository for storing preprocessed document data. Furthermore, the ability to execute database queries according to the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity measure</a> makes it an extremely efficient database for enabling efficient document retrieval. A dedicated AWS EC2 instance is used to host the Elasticsearch instance.</p> <h3 id="35-the-ai-logic">3.5. The “AI logic”</h3> <p>The AI logic is the heart of the system, orchestrating the matchmaking process. By considering predicted industry and job titles, user-specified keyword entities, and leveraging unsupervised ranking techniques, our system intelligently filters and ranks documents, providing personalized recommendations to users. This is accomplished through 5 sequential steps facilitated by a <a href="https://streamlit.io/">Streamlit</a> user interface displayed in Figure 2.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog10.3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog10.3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog10.3-1400.webp"/> <img src="/assets/img/blog/blog10.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 2: The graphic user interface for the resume-job matching solution.</em> </div> <ul> <li><strong>Step 1.</strong> The first step requires the user to select whether they are wanting to match a resume to a job specification.</li> <li><strong>Step 2.</strong> The user may then upload the resume to the system in their PDF or DOCX format. Uploading a resume triggers the initialisation and prepocessing procedures described in §3.1 and §3.3, the results of which are also stored in the Elasticsearch database. During this step, the industry and corresponding job title of the candidate are predicted using the IndustryBERT and JobTitleBERT models desribed in §3.2, respectively.</li> <li><strong>Step 3.</strong> The predicted industry and corresponding job title of the candidate are then surfaced to the user as a confirmation. If an errorous prediction was made by the model, the user has the opportunity to correct the classification at this stage. This step is important since the candidate’s suitability will only be assessed according to those job specifications matching the candidate’s industry and job title.</li> <li><strong>Step 4</strong> In some cases, the user may wish to further filter out the suitability of the candidate to a job specification unless specific entities are not present in both documents. More specifically, the user may specify in this step that specific skills, degrees, colleges and/or histroic company names must be present in both documents. This is accomplished by utilising our custom spaCy NER model described in §3.2.</li> <li><strong>Step 5</strong>: The combination of all information obtained in Steps 1-4 culminate in this final step. All job specifications which assume the same industry and job titles as the resume confirmed by the user in Step 3, as well as any entities specified by the user in Step 4, are queried from the Elasticsearch database as potential candidates. All candidate job specifications then undergo an unsupervised ranking procedure according to the similarity to the uploaded resume. Two unsupervised ranking algorithms where proposed and evaluated according to preference-based assessments by a job recruitment expert: <ul> <li>The first algorithm relies on ordering all candidate job specifications by utilising Elasticsearch’s <a href="https://www.elastic.co/search-labs/blog/articles/text-similarity-search-with-vectors-in-elasticsearch">text similarity functionality</a>. More specifically, job specifications are ordered according to the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosign similarity</a> between the text vector representations of the job specifications according to the resume’s vector representation.</li> <li>The second algorithm takes inspiration from the recently proposed <a href="https://arxiv.org/pdf/2008.09470.pdf">Top2Vec approach</a> and utilises the pre-fitted UMAP and HDBSCAN models described in §3.2. First, the spaCy document vector representation of the resume is reduced from 300 to 80 dimensions using the UMAP model. This is done since UMAP has been shown to be an effective preprocessing step to boost the performance of density based clustering methods. This reduced vector representation then acts as input to the HDBSCAN model which produces a cluster prediction. The most appropriate job specifications to recommend are those which share the same cluster as the resume.</li> </ul> </li> </ul> <p>The effectiveness of the resume matching system was extensively evaluated by the client using the Streamlit user interface.</p> <blockquote> <p>After thoughrough evaluation of the system, the client was impressed with the overall quality of the recommendations and I proceeded to productionising the solution.</p> </blockquote> <hr/> <h2 id="4-wrapping-up">4. Wrapping up</h2> <p>By leveraging advanced NLP techniques, I was able to create a system that significantly improved the client’s efficiency and effectiveness in finding the right candidates for their open positions, as confirmed by the positive client feedback in Figure 3. The client was so impressed with the solution that they decided to reposition their business offering around this new AI-based recruitment matching system, as shown in the accompanying video below.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog10.6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog10.6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog10.6-1400.webp"/> <img src="/assets/img/blog/blog10.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 3: Client feedback.</em> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/jobcrystal.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <p>As NLP continues to evolve, I can only expect even more innovative applications in the field of recruitment, further transforming the way companies find and hire top talent. It’s projects like this that futher cement my belief that Data Science and Machine Learning has the power to revolutionise almost every industry.</p>]]></content><author><name>Shane van Heerden</name></author><category term="CaseStudy"/><summary type="html"><![CDATA[How I used NLP to help efficiently place prospective candidates]]></summary></entry><entry><title type="html">🌌 Identifying Knowledge-sharing Opportunities with Knowledge Graphs</title><link href="https://shanevanheerden.github.io/blog/2021/identifying_knowledge_sharing_opportunities_with_knowledge_graphs/" rel="alternate" type="text/html" title="🌌 Identifying Knowledge-sharing Opportunities with Knowledge Graphs"/><published>2021-05-25T00:00:00+00:00</published><updated>2021-05-25T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2021/identifying_knowledge_sharing_opportunities_with_knowledge_graphs</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2021/identifying_knowledge_sharing_opportunities_with_knowledge_graphs/"><![CDATA[<h1 id="-work-in-progress-">🚧 <b>WORK IN PROGRESS</b> 🚧</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog11.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog11.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog11.1-1400.webp"/> <img src="/assets/img/blog/blog11.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="1-the-opportunity">1. The opportunity</h2> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/knowledgemarker.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> <hr/> <h2 id="2-the-solution-architecture">2. The solution architecture</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog11.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog11.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog11.2-1400.webp"/> <img src="/assets/img/blog/blog11.2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog11.3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog11.3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog11.3-1400.webp"/> <img src="/assets/img/blog/blog11.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 2: Solution architecture.</em> </div> <hr/> <h2 id="3-insights">3. Insights</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog11.4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog11.4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog11.4-1400.webp"/> <img src="/assets/img/blog/blog11.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 3: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog11.5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog11.5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog11.5-1400.webp"/> <img src="/assets/img/blog/blog11.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 4: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog11.6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog11.6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog11.6-1400.webp"/> <img src="/assets/img/blog/blog11.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 5: Solution architecture.</em> </div> <hr/> <h2 id="4-project-impact">4. Project impact</h2> <hr/> <h2 id="5-wrapping-up">5. Wrapping up</h2>]]></content><author><name>Shane van Heerden</name></author><category term="CaseStudy"/><summary type="html"><![CDATA[How I used NLP and knowledgegraphs to increase collaboration in the workplace]]></summary></entry><entry><title type="html">🎭 Customer Segmentation using RFM Analysis</title><link href="https://shanevanheerden.github.io/blog/2021/customer_segmentation_using_rfm_analysis/" rel="alternate" type="text/html" title="🎭 Customer Segmentation using RFM Analysis"/><published>2021-03-29T00:00:00+00:00</published><updated>2021-03-29T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2021/customer_segmentation_using_rfm_analysis</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2021/customer_segmentation_using_rfm_analysis/"><![CDATA[<h1 id="-work-in-progress-">🚧 <b>WORK IN PROGRESS</b> 🚧</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.2-1400.webp"/> <img src="/assets/img/blog/blog12.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="1-the-problem">1. The problem</h2> <hr/> <h2 id="2-understanding-the-clients-needs">2. Understanding the clients needs</h2> <hr/> <h2 id="3-deep-dive-anaysis">3. Deep dive anaysis</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.3-1400.webp"/> <img src="/assets/img/blog/blog12.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.4-1400.webp"/> <img src="/assets/img/blog/blog12.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 2: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.5-1400.webp"/> <img src="/assets/img/blog/blog12.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 3: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.6-1400.webp"/> <img src="/assets/img/blog/blog12.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 4: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.7-1400.webp"/> <img src="/assets/img/blog/blog12.7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 5: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.8-1400.webp"/> <img src="/assets/img/blog/blog12.8.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 6: Solution architecture.</em> </div> <hr/> <h2 id="4-productionising-the-solution">4. Productionising the solution</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.9-1400.webp"/> <img src="/assets/img/blog/blog12.9.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 7: Solution architecture.</em> </div> <hr/> <h2 id="5-wrapping-up">5. Wrapping up</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog12.10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog12.10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog12.10-1400.webp"/> <img src="/assets/img/blog/blog12.10.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 8: Client feedback.</em> </div>]]></content><author><name>Shane van Heerden</name></author><category term="CaseStudy"/><summary type="html"><![CDATA[How I helped a client identify their most valuable customers to retain and reactivate]]></summary></entry><entry><title type="html">🔍 Using Machine Learning to Classify Personally Identifiable Data Fields</title><link href="https://shanevanheerden.github.io/blog/2021/using_machine_learning_to_classify_personally_identifiable_data_fields_description/" rel="alternate" type="text/html" title="🔍 Using Machine Learning to Classify Personally Identifiable Data Fields"/><published>2021-02-16T00:00:00+00:00</published><updated>2021-02-16T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2021/using_machine_learning_to_classify_personally_identifiable_data_fields_description</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2021/using_machine_learning_to_classify_personally_identifiable_data_fields_description/"><![CDATA[<p><em>This blog post was originally featured on the <a href="https://medium.com/cape-ai-stories/case-study-using-machine-learning-to-classify-personally-identifiable-data-fields-6b9c5b0743e7">Cape AI Medium page</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog4.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog4.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog4.1-1400.webp"/> <img src="/assets/img/blog/blog4.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>I always jump at the opportunity to tackle new and interesting challenges using Machine Learning and Advanced Analytics. In the constantly changing field of AI, this is how I discover and push the bounds of what is truly possible.</p> <h2 id="1-the-problem">1. The problem</h2> <p>Recently, I was tasked with a new challenge by a client who provides services related to secure sharing of consumer data. In order to strengthen their <a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation"><em>General Data Protection Regulation</em></a> (GDPR) compliance and boost their clients’ trust, they wanted a solution that could help them automatically identify sensitive fields in their clients’ consumer data so that these could be encrypted appropriately. This was an important problem to tackle since all organisations and companies that handle data relating to EU citizens must comply with GDPR.</p> <p>This is a two-part blog post where I am going to walk you through how I helped them solve this problem with some clever engineering and the help of Machine Learning!</p> <hr/> <h2 id="2-understanding-the-clients-needs">2. Understanding the clients needs</h2> <p>The first step in my problem-solving process always involves engaging with the client to better understand their problem and agree on a set of solution requirements. So, what exactly was the problem? Let us unpack it a bit…</p> <p>Roughly once a week, the client would receive new structured data tables like that in Figure 1, where rows denote consumers and columns various attributes related to these consumers. In essence, the problem entailed identifying so-called Personal Information (PI) columns that could be linked to a specific individual (e.g. name, ID number, phone number). To add further complications, there was hardly ever a predictable column structure in these tables and, on occasion, they contained junk data, or were missing data, in some cells.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog4.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog4.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog4.2-1400.webp"/> <img src="/assets/img/blog/blog4.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1:</em> Typical consumer data. </div> <p>After the first formal engagement with the client, I helped establish a set of requirements for the solution. More specifically, the solution should be:</p> <ol> <li>Able to Determine if a column is PI or non-PI, and allocate a PI column to one of 7 categories: (1) first name, (2) last name, (3) ID number, (4) phone number, (5) email address, (6) date-of-birth (DoB) or (7) Vehicle Identification Number (VIN).</li> <li>Packeged as a production-ready system that is able to self-improve through implicit feedback gathered by clients engaging with the model.</li> </ol> <p>In this blog post, I’m just going going to describe how I addressed the first requirement and we’ll save the second requirement for Part 2!</p> <hr/> <h2 id="3-rapid-development">3. Rapid development</h2> <p>The next step in my problem-solving process typically involves rapid analysis and developing a proof-of-concept to demonstrate how my proposed solution would work and the value it offers.</p> <blockquote> <p>Time for some Machine Learning!</p> </blockquote> <p>Upon reviewing the requirements again, it was clear that this problem could be framed as a classification problem (something I previously discussed in my <a href="https://shanevanheerden.github.io/blog/2018/an_overview_of_supervised_machine_learning/">Supervise Learning blog post</a>). In this case, if I had a bunch of examples of first and last names, phone numbers, ID numbers, DoB, email addresses and VINs, each labelled as such, we could train a multi-class supervised learning model, such as <a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic Regression</a>. This model could then be used to classify each new “string” as one of the above labels (a catch-all term for sets of letters and/or numbers in this context). Luckily, the client had many examples of these different categories on hand!</p> <p>But there was just one problem: to train a supervised model, we needed a set of features that could appropriately characterise similar strings belonging to different categories — a phone number and an ID number, for example. The idea was therefore to construct a set of informative features using <a href="https://en.wikipedia.org/wiki/Regular_expression"><em>Regular Expressions</em></a> (RegEx). Table 1 shows the 17 features that were constructed together with a RegEx description of each feature and the category the feature was intended to target. Feature 10, for example, checks whether the 7th character in the string is either a “0” or a “5” since, at least when dealing with South African IDs, would indicate whether a person is male or female, respectively.</p> <table> <tr> <th>#</th> <th>Feature name</th> <th>Target category</th> <th>RegEx description</th> </tr> <tr> <td>1</td> <td>countLength</td> <td>All</td> <td>Count # of characters in string</td> </tr> <tr> <td>2</td> <td>countUpper</td> <td>Name &amp; VIN</td> <td>Count # of uppercase characters in string</td> </tr> <tr> <td>3</td> <td>countLower</td> <td>Name &amp; VIN</td> <td>Count # of lowercase characters in string</td> </tr> <tr> <td>4</td> <td>countNumerical</td> <td>Name</td> <td>Count # of numbers in string</td> </tr> <tr> <td>5</td> <td>countSpace</td> <td>Last name</td> <td>Count # of spaces (‘ ’) in string</td> </tr> <tr> <td>6</td> <td>countDashesSlashes</td> <td>DoB</td> <td>Count # of dashes (‘-’) and forward slashes (‘/’) in string</td> </tr> <tr> <td>7</td> <td>countVowels</td> <td>First name &amp; Last name</td> <td>Count # of vowels in string</td> </tr> <tr> <td>8</td> <td>countConsonants</td> <td>First name &amp; Last name</td> <td>Count # of consonants in string</td> </tr> <tr> <td>9</td> <td>checkCitizen</td> <td>ID number</td> <td>Check if 3rd to 2nd last characters are ‘08’</td> </tr> <tr> <td>10</td> <td>checkGender</td> <td>ID number</td> <td>Check if 7th character is either ‘0’ or ‘5’</td> </tr> <tr> <td>11</td> <td>checkCountry</td> <td>Phone number</td> <td>Check if string starts with ‘27’ or ‘+27’</td> </tr> <tr> <td>12</td> <td>checkArea</td> <td>Phone number</td> <td>Check if 2nd to 4th characters are area code numbers</td> </tr> <tr> <td>13</td> <td>checkPrefix</td> <td>Last name</td> <td>Check if string starts with common surname prefixes</td> </tr> <tr> <td>14</td> <td>checkSyllables</td> <td>Name</td> <td>Check if string contains syllables</td> </tr> <tr> <td>15</td> <td>checkEmail</td> <td>Email</td> <td>Check if string contains ‘@’ or ‘.’</td> </tr> <tr> <td>16</td> <td>checkFirst</td> <td>First name</td> <td>Check if string is in list of first names</td> </tr> <tr> <td>17</td> <td>checkLast</td> <td>Last name</td> <td>Check if string is in list of last names</td> </tr> </table> <div class="caption"> <em>Table 1:</em> The set of features that were constructed from RegEx. </div> <p>Before jumping straight to training the model with my newly constructed features, I wanted to get a bit of an intuitive feel of the feature space I had constructed. Figure 2 allows just that, showing the result of squashing the 17-dimensional feature space down to only two dimensions with the help of a dimensionality reduction technique called <a href="https://arxiv.org/abs/1802.03426"><em>Uniform Manifold Approximation and Projection</em></a> (UMAP). It seemed like the RegEx features did a pretty good job at separating out the 7 categories.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog4.3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog4.3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog4.3-1400.webp"/> <img src="/assets/img/blog/blog4.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 2:</em> A two-dimensional projection of the 17-dimensional feature space. </div> <p>Since the feature space appeared pretty <a href="https://en.wikipedia.org/wiki/Linear_separability">linearly separable</a>, at this point it seemed reasonable to assume that Logistic Regression wouldn’t have much of a problem distinguishing between the categories (with maybe the exception of first and last names), and that is exactly what I observed. Below, in Figure 3, you can see the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"><em>Area Under the Curve</em></a> (AUC) and <em>Classification Accuracy</em> (CA) results for each category (using 10-fold <a href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross-validation</a>), together with the corresponding <a href="https://en.wikipedia.org/wiki/Confusion_matrix">confusion matrix</a>, which shows the difference between the actual categories and the ones predicted by the Logistic Regression model.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog4.4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog4.4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog4.4-1400.webp"/> <img src="/assets/img/blog/blog4.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(a)</center> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog4.5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog4.5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog4.5-1400.webp"/> <img src="/assets/img/blog/blog4.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(b)</center> </div> </div> <div class="caption"> <em>Figure 3:</em> Results from training a Logistic Regression model using 10-fold cross validation. </div> <p>With this in mind, I had enough confidence to progress further with this idea and develop it into a consumable and concrete solution for the client.</p> <hr/> <h2 id="4-defining-the-process-flow">4. Defining the process flow</h2> <p>One natural idea that occurred to me at the beginning of the project was to determine the column categories based solely on clever RegEx formulations. Indeed, this would be quite easy for categories such as email addresses, phone- and ID numbers, but a lot more difficult for columns that contained, say, a person’s name. Nonetheless, I realised that it would be foolish to let my Logistic Regression model predict a column category if we could already say, for certain, that it belonged to a specific category based on the value of a certain RegEx feature. Take, for example, a string that contains an at-sign (“@”) — it wouldn’t seem too crazy to immediately assign this column to the “Email” category if we observe the first 10 records subscribing to this pattern. With this notion in mind, we decided to impose some features as having a so-called <em>assign condition</em> which would facilitate this automatic assignment of a column to a specific category.</p> <p>Figure 4 describes my proposed process flow for predicting whether a column is PI or non-PI, as well as the corresponding category (if the column is PI). Starting from the top, a number of string records are sampled from the column in question. For each string record, a feature value is computed according to the set of defined RegEx features. If the feature currently being computed has an assign condition, the total proportion of records satisfying this condition is calculated and compared to a <em>category assignment threshold</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog4.6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog4.6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog4.6-1400.webp"/> <img src="/assets/img/blog/blog4.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 4:</em> A process flow diagram describing how a category is either assigned or predicted for a column. </div> <p>As a way of explanation, consider an example where 100 records have been sampled from a column and a category assignment threshold of 0.1 (or 10%) has been specified. If the feature under consideration is checking whether the string record contains a “@”, the column will immediately be assigned as a PI column with the category of “Email”, if at least 10 of the 100 sampled string records (or 10%) contain an “@”. In the case where all features have been extracted and no assign conditions have been satisfied, the pre-trained Logistic Regression model would only then be used to predict the category of the column under consideration. For each of the defined categories, a probability is computed using the values of the extracted features and the Logistic Regression weights.</p> <p>If the largest predicted probability aggregated across all samples exceeds some specified <em>PI assignment threshold</em>, the column is assigned as a PI column with the category that achieved the highest predicted probability. If this threshold is not exceeded, the column is returned as a non-PI column.</p> <hr/> <h2 id="5-evaluating-the-systems-performance">5. Evaluating the systems performance</h2> <p>The final step in my formulation was to assess the solution’s performance in a real-life setting. For this, the client gave myself the challenge of correctly categorising 192 consumer data columns. Figure 5 shows a snippet of the evaluation procedure we followed, where green dots indicate a correct prediction and red an incorrect prediction. I also tracked whether the assigned category was the result of assignment by a feature or prediction by the Logistic Regression model, indicated by the blue and purple dots, respectively.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog4.7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog4.7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog4.7-1400.webp"/> <img src="/assets/img/blog/blog4.7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 5:</em> A snippet of the column evaluation procedure. </div> <p>So, how well did my solution fare overall? Figure 6 shows the results of my evaluation in the form of a confusion matrix. I found that, overall, my solution was able to correctly classify <strong>97.9%</strong> of the columns, where the Logistic Regression model contributed 26.6% of these predictions. Confusion typically arose when institution names were sometimes misclassified as first names by the Logistic Regression model.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog4.8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog4.8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog4.8-1400.webp"/> <img src="/assets/img/blog/blog4.8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 6:</em> Confusion matrix of column predictions. </div> <blockquote> <p>The client was happy with this performance and we proceeded to the next phase of the project which entailed packaging the solution as a production-ready system for them.</p> </blockquote> <hr/> <h2 id="6-wrapping-up">6. Wrapping up</h2> <p>And that’s all! In this post, I walked you through my approach for automatically identifying sensitive data fields as well as how this solution was productionised in a real-world system. It goes to show that Machine Learning can be used to solve almost any problem.</p>]]></content><author><name>Shane van Heerden</name></author><category term="CaseStudy"/><summary type="html"><![CDATA[How I developed a solution to increase a client's data labelling process efficiency by 100-fold]]></summary></entry><entry><title type="html">🧠 Neural Language Models</title><link href="https://shanevanheerden.github.io/blog/2020/neural_language_models/" rel="alternate" type="text/html" title="🧠 Neural Language Models"/><published>2020-11-12T00:00:00+00:00</published><updated>2020-11-12T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2020/neural_language_models</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2020/neural_language_models/"><![CDATA[<p><em>This blog post was originally featured on the <a href="https://medium.com/cape-ai-stories/natural-language-processing-series-3d2857bc67ab">Cape AI Medium page</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog3.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog3.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog3.1-1400.webp"/> <img src="/assets/img/blog/blog3.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This is the third blog post in our series of blog posts focusing on the exciting field of Natural Language Processing! In <a href="https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/">our first post</a>, we saw that the application of neural networks for building language models was a major turning point in the NLP timeline, and in <a href="https://shanevanheerden.github.io/blog/2020/word_embeddings/">our second post</a> we explored the significance of Word Embeddings in advancing the field. With this, we’re now ready to build our own language model!</p> <h2 id="1-what-exactly-is-a-language-model">1. What exactly is a language model?</h2> <p>In its most simple form:</p> <blockquote> <p>The task of a language model is simply to predict the probability of the next word appearing in a sequence of text given the previous words that have occurred.</p> </blockquote> <p>Traditionally, this problem was tackled with Statistical Language Models which primarily consisted of using so-called n-gram models in combination with some sort of smoothing technique<d-cite key="Kneser1995"></d-cite>. The big pivot in the way researchers thought about this problem occurred when Bengio <em>et al.</em><d-cite key="Bengio2000"></d-cite> proposed using a feed-forward neural network together with a word “lookup-table” for representing the n previous words (often referred to as a <em>token</em>) in a sequence, as shown in Figure 1. Today, this “lookup-table” is known as a <em>word embedding</em> which you may already be familiar with if you read our second blog post. And thus the <em>Neural Language Model</em> was born!</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog3.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog3.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog3.2-1400.webp"/> <img src="/assets/img/blog/blog3.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1:</em> The first feed-forward neural network used for language modelling<d-cite key="Kneser1995"></d-cite>. </div> <hr/> <h2 id="2-building-our-own-neural-language-model">2. Building our own Neural Language Model</h2> <p>We’re going to keep things very practical in this post by jumping straight into a coding example! In this example, we are going to walk you through:</p> <ol> <li>How you can prepare a document of input text for developing a word-based language model,</li> <li>How you can design and implement your very own neural language model, and</li> <li>How you can use this model to generate some new text with a similar tone to the input text.</li> </ol> <p>Let’s get started!</p> <h3 id="21-package-installations">2.1. Package installations</h3> <p>We only need to install two packages for this tutorial: good-ol’ numpy and keras (which will do most of the deep learning heavy-lifting for us). Go ahead and run the following in your terminal:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>pip <span class="nb">install </span>numpy
<span class="nv">$ </span>pip <span class="nb">install </span>keras</code></pre></figure> <h3 id="22-creating-a-training-document">2.2. Creating a training document</h3> <p>Now we need some high-quality text. And what better place to look than the much-loved <em>Cat in the Hat</em> story by Dr Seuss that we all probably all read as kids. Thankfully, <a href="https://github.com/robertsdionne">Robert Dionne</a> has already compiled a <a href="https://raw.githubusercontent.com/robertsdionne/rwet/master/hw2/catinthehat.txt">text file</a> containing the full story which we can read in using the following code:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">requests</span>
 
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/robertsdionne/rwet/master/hw2/catinthehat.txt</span><span class="sh">'</span><span class="p">)</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">text</span>
<span class="nf">print</span><span class="p">(</span><span class="n">doc</span><span class="p">[:</span><span class="mi">300</span><span class="p">])</span></code></pre></figure> <p>In the script above, we use the very useful <a href="https://pypi.org/project/requests/"><code class="language-plaintext highlighter-rouge">requests</code></a> module and get the text file directly from GitHub. Let’s print out the first 300 characters of our document just to be sure we’ve grabbed the right file:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="o">&gt;&gt;</span> <span class="n">The</span> <span class="n">Cat</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">Hat</span>
<span class="n">By</span> <span class="n">Dr</span><span class="p">.</span> <span class="n">Seuss</span>
<span class="n">The</span> <span class="n">sun</span> <span class="n">did</span> <span class="ow">not</span> <span class="n">shine</span><span class="p">.</span>
<span class="n">It</span> <span class="n">was</span> <span class="n">too</span> <span class="n">wet</span> <span class="n">to</span> <span class="n">play</span><span class="p">.</span>
<span class="n">So</span> <span class="n">we</span> <span class="n">sat</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">house</span>
<span class="n">All</span> <span class="n">that</span> <span class="n">cold</span><span class="p">,</span> <span class="n">cold</span><span class="p">,</span> <span class="n">wet</span> <span class="n">day</span><span class="p">.</span>
<span class="n">I</span> <span class="n">sat</span> <span class="n">there</span> <span class="k">with</span> <span class="n">Sally</span><span class="p">.</span>
<span class="n">We</span> <span class="n">sat</span> <span class="n">there</span><span class="p">,</span> <span class="n">we</span> <span class="n">two</span><span class="p">.</span>
<span class="n">And</span> <span class="n">I</span> <span class="n">said</span><span class="p">,</span> <span class="sh">"</span><span class="s">How I wish
We had something to do!</span><span class="sh">"</span>
<span class="n">Too</span> <span class="n">wet</span> <span class="n">to</span> <span class="n">go</span> <span class="n">out</span>
<span class="n">And</span> <span class="n">too</span> <span class="n">cold</span> <span class="n">to</span> <span class="n">play</span> <span class="n">ball</span><span class="p">.</span>
<span class="n">So</span> <span class="n">we</span> <span class="n">sat</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">house</span><span class="p">.</span></code></pre></figure> <p>Seems right!</p> <h3 id="23-text-pre-processing">2.3. Text pre-processing</h3> <p>Next, we need to do some text pre-processing in which we will transform our document into a sequence of tokens which we can use to construct a training data set for our model. Based on the short snippet of the story we saw, we clean the text in the following way:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">string</span>
 
<span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="nf">replace</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">).</span><span class="nf">lower</span><span class="p">()</span>
<span class="n">doc</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="nf">translate</span><span class="p">(</span><span class="nb">str</span><span class="p">.</span><span class="nf">maketrans</span><span class="p">(</span><span class="sh">''</span><span class="p">,</span> <span class="sh">''</span><span class="p">,</span> <span class="n">string</span><span class="p">.</span><span class="n">punctuation</span><span class="p">))</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">doc</span><span class="p">.</span><span class="nf">split</span><span class="p">()</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="nf">set</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Tokens: </span><span class="si">{</span><span class="n">tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Total tokens: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Vocabulary size: </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <p>We begin by replacing all new line characters with spaces and converting all characters to their lower case form. Next, we remove any punctuation from our document. We can then split our document into individual tokens (or words) based on the space characters by making use of the sring’s <code class="language-plaintext highlighter-rouge">.split()</code> method. Let’s count the number of tokens to see what we have to work with:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Tokens</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">cat</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">in</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">the</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">hat</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">by</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">dr</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">seuss</span><span class="sh">'</span><span class="p">,</span> <span class="bp">...</span>
<span class="n">Total</span> <span class="n">tokens</span><span class="p">:</span> <span class="mi">6290</span>
<span class="n">Vocabulary</span> <span class="n">size</span><span class="p">:</span> <span class="mi">855</span></code></pre></figure> <p>Seems like our story contains 6290 tokens in total but has a vocabulary size of only 855 unique words.</p> <p>Now, the next step is to transform our tokens into a set of sequences that can act as our training dataset. For this, let’s organise our list of tokens into sequences of 64 input words and one target word (giving us a total sequence length of 65). We can do this by “sliding” a window (of size 65) sliding across our list of tokens and joining them together to create a sequence sample.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">length</span> <span class="o">=</span> <span class="mi">64</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)):</span>
  <span class="n">line</span> <span class="o">=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">tokens</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="n">length</span><span class="p">:</span><span class="n">i</span><span class="p">])</span>
  <span class="n">sequences</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">line</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Number of sequences: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span></code></pre></figure> <p>Let’s also print out how many training sequences we obtained:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Number</span> <span class="n">of</span> <span class="n">sequences</span><span class="p">:</span> <span class="mi">6225</span></code></pre></figure> <p>This should be more than enough training sequences for demonstration purposes.</p> <h3 id="24-prepare-the-dataset">2.4. Prepare the dataset</h3> <p>Although the string representation of our words is nice for humans to look at, it won’t mean much for a neural network which only deals with numbers. We therefore need to map each of the words in our vocabulary to a corresponding integer value.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
 
<span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">()</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">sequences</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span>
<span class="n">vocab_size</span> <span class="o">+=</span> <span class="mi">1</span></code></pre></figure> <p>For this, we can use keras’s <code class="language-plaintext highlighter-rouge">Tokenizer</code> class. We can first define a tokenizer object and fit it on all our set of sequences, which, in effect, finds all unique words in our data and maps each of them to a unique integer ID. More specifically, words are assigned values from 1 to the total number of words. We then use the tokenizer to re-define our sequences to be a set of integers and store the result as a numpy array. At this stage, since the word at the end of the vocabulary will be 855 but Python indexing of arrays starts at zero, we increment the vocabulary size to correct for this.</p> <p>Now that our sequences are properly encoded, we can split them into the set of features X and target variables y.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">sequences</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="nf">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">)</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span></code></pre></figure> <p>For indexing reasons, we utilise numpy’s handy splicing operation to perform this splitting. After this, we one-hot encode the target word using keras’s <code class="language-plaintext highlighter-rouge">to_categorical()</code> method which, in effect, transforms our output to a vector of length <code class="language-plaintext highlighter-rouge">vocab_size</code> with a value of 1 in the place of the word’s position and a value of 0 everywhere else. It will then be the job of the model to learn a probability distribution over all words in our vocabulary.</p> <h3 id="25-define-and-train-the-model">2.5. Define and train the model</h3> <p>Hooray! We’ve arrived at the fun part of choosing the structure of our neural language model and training it.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="n">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">Embedding</span>
 
<span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">seq_length</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">128</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">())</span></code></pre></figure> <p>We’ll keep things fairly standard by defining a keras sequential model and adding some layers to it. More specifically, we will use an Embedding Layer to learn the representation of words, as well as a <em>Long Short-Term Memory</em> (LSTM) recurrent neural network to learn to predict words based on their context. The inputs to the Embedding layer is the vocabulary size, the length of the embedding vector (which we choose as 100) and the length of the sequence. We also specify a layer size of 128 for both LSTM layers (since powers of two are computationally more efficient). Finally, we will define two fully connected layers which will be used to interpret the features extracted from the sequence, the first having again 128 layers and a <em>Rectified Linear Unit</em> (ReLU) activation function and the last having 856 layers and a softmax activation function (to ensure the outputs are scaled between zero and one). We can print out a summary of our model as a sort of sanity check:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">Model</span><span class="p">:</span> <span class="sh">"</span><span class="s">sequential</span><span class="sh">"</span>
<span class="n">_________________________________________________________________</span>
<span class="nc">Layer </span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>                 <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   
</span><span class="o">=================================================================</span>
<span class="nf">embedding_1 </span><span class="p">(</span><span class="n">Embedding</span><span class="p">)</span>      <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>            <span class="mi">54784</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">lstm_1 </span><span class="p">(</span><span class="n">LSTM</span><span class="p">)</span>                <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>           <span class="mi">98816</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">lstm_2 </span><span class="p">(</span><span class="n">LSTM</span><span class="p">)</span>                <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>               <span class="mi">131584</span>    
<span class="n">_________________________________________________________________</span>
<span class="nf">dense_1 </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>               <span class="mi">16512</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">dense_2 </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">856</span><span class="p">)</span>               <span class="mi">110424</span>    
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">412</span><span class="p">,</span><span class="mi">120</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">412</span><span class="p">,</span><span class="mi">120</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">_________________________________________________________________</span></code></pre></figure> <p>Everything seems in order. Now we can compile and train our model as follows:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span></code></pre></figure> <p>We define a cross entropy loss function which is sensible since we are technically dealing with a multi-class classification problem. We will also specify that keras must use the efficient Adam optimizer for updating the model weights evaluated on accuracy. Finally, we fit the model on our data for 100 training epochs with a fairly modest batch size of 128. Now all we have to do is go grab a coffee and let our model train.</p> <h3 id="26-training-the-model">2.6. Training the model</h3> <p>Congratulations! You have just trained your very own neural language model! Let’s explore your new model’s capabilities by generating some random text with it. For this, we are going to create a function that takes as input the model and associated tokenizer we have just created, together with the sequence length, number of words to generate, and some input text which will act as a starting point in the generation process.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
 
<span class="k">def</span> <span class="nf">generate_sequence</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">num_words</span><span class="p">,</span> <span class="n">input_text</span><span class="p">):</span>
  <span class="n">word_lookup</span> <span class="o">=</span> <span class="nf">dict</span><span class="p">((</span><span class="n">v</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span><span class="p">.</span><span class="nf">items</span><span class="p">())</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_words</span><span class="p">):</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">([</span><span class="n">input_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">([</span><span class="n">encoded</span><span class="p">],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict_classes</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_word</span> <span class="o">=</span> <span class="n">word_lookup</span><span class="p">[</span><span class="n">yhat</span><span class="p">]</span>
    <span class="n">input_text</span> <span class="o">+=</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span> <span class="o">+</span> <span class="n">new_word</span>
 <span class="k">return</span> <span class="n">input_text</span></code></pre></figure> <p>We begin by defining a dictionary that can act as a sort of lookup that provides us with the string representation of a word given a word’s ID. Next, we encode the input text according to the mapping defined by our tokenizer. To ensure that the input text doesn’t grow too long, we truncate the text to the sequence length required by our model using Keras’s <code class="language-plaintext highlighter-rouge">pad_sequences()</code> method. We can now pass this encoded sequence to our model as input and the output we receive is the ID of the most likely word in the sequence. We can then use our lookup dictionary to get the string representation of our word and append it to our input text with a space.</p> <p>All that’s left to do now is try our function out.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">input_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">then the cat turned around and laughed</span><span class="sh">"</span>
<span class="nf">generate_sequence</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">input_text</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span></code></pre></figure> <p>We pass the model, tokenizer and sequence length, as well as some input text and specify that 10 additional tokens must be generated. And the moment of truth…</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="sh">'</span><span class="s">then the cat turned around and laughed you should a yink i have a lot of one my teeth are gold</span><span class="sh">'</span></code></pre></figure> <p>Interesting choice of words, but I don’t quite think our model is going to be the next Dr Seuss. Nonetheless, this should give you a good enough idea about how neural language models are working under the hood. Advanced neural language models produce significantly better results since they utilise much more sophisticated architectures, are trained on much more text and have significantly more parameters compared to our toy example. But I encourage you to take this example and make it your own. Try playing around with the model architecture and training hyperparameters or even change the input text to something completely different and see what kind of results you can generate!</p> <hr/> <h2 id="3-wrapping-up">3. Wrapping up</h2> <p>And that’s all! In this post, I walked you through how to create your very own neural language model in Python with some help from Keras and how you can use this model to generate your own text sequences.</p>]]></content><author><name>Shane van Heerden</name></author><category term="NLP"/><summary type="html"><![CDATA[Part 3 - Natural Language Processing Series]]></summary></entry><entry><title type="html">📊 Word Embeddings</title><link href="https://shanevanheerden.github.io/blog/2020/word_embeddings/" rel="alternate" type="text/html" title="📊 Word Embeddings"/><published>2020-10-12T00:00:00+00:00</published><updated>2020-10-12T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2020/word_embeddings</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2020/word_embeddings/"><![CDATA[<p><em>This blog post was originally featured on the <a href="https://medium.com/cape-ai-stories/natural-language-processing-series-3f412d3ab933">Cape AI Medium page</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.1-1400.webp"/> <img src="/assets/img/blog/blog2.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This is the second post in a series of blog posts focusing on the exciting field of <em>Natural Language Processing</em> (NLP)! In <a href="https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/">our previous blog post</a>, we saw that word embeddings were an important milestone in the history of NLP. Although these were used as far back as 2001, in 2013 Mikolov <em>et al.</em><d-cite key="Mikolov2013"></d-cite> proposed a simple but novel method for efficiently training word embeddings (or <em>word2vec</em> models) on very large unlabelled corpora which ultimately led to their wide-scale adoption. So, what actually are word embeddings and how do they fit into an NLP practitioner’s toolkit? Let’s find out!</p> <h2 id="1-what-are-word-embeddings">1. What are Word Embeddings?</h2> <p>A word embedding is simply an alternative representation for text where words are represented as real-valued vectors as opposed to a sequence of string characters. This representation is <em>learnt</em> in such a way that words that share the same or similar meaning also have similar vector representations.</p> <p>To get a feel for this concept, let’s look at Figure 1 which is a visual representation of a word’s vector, where red indicates a value close to 2 and blue a value close to -2. As we can see, the vectors for “man” and “woman” appear to be more similar in colour to each other than when compared to the word “king”. A mystical property of word embeddings is that they often capture semantic meaning between words. This means that we can add and subtract word vectors and arrive at intuitive results. Probably the most famous of these is the vector resulting from taking the word “king”, subtracting the word “man” and adding the word “woman”. The resultant vector is visually shown in Figure 1 and, if we compare this against all 400 000 words that make up the embedding vocabulary, we find that the vector for the word “queen” is in fact the closest to this resultant vector!</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.2-1400.webp"/> <img src="/assets/img/blog/blog2.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1:</em> A visual representation of the word vectors corresponding to gender-specific words<d-cite key="Alammar2019"></d-cite>. </div> <hr/> <h2 id="2-word2vec-methods">2. Word2Vec Methods</h2> <p>Now that we have a bit of an intuitive feel of word embeddings, let’s get a better grasp on the theory surrounding word embeddings. In their paper, Mikolov et al. proposed two methods for creating word embeddings, namely the <em>Continuous Bag-of-Words</em> (CBOW) and <em>Skip-gram</em> method, as shown in Figure 2. In the CBOW method, the current word <em>w(t)</em> is predicted based on the context of the surrounding words. The Skip-gram method does the exact opposite by attempting to predict the surrounding words given the current word <em>w(t)</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.3-1400.webp"/> <img src="/assets/img/blog/blog2.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 2:</em> The CBOW and Skip-gram method<d-cite key="Mikolov2013"></d-cite>. </div> <p>Let’s expand on this a bit more by diving into how one would go about constructing a dataset from an unlabelled corpus of text to leverage each of these two methods. For a more in-depth explanation of these concepts, we highly recommend reading <a href="http://jalammar.github.io/illustrated-word2vec/">Jay Alammar’s blog post</a> on this subject which inspired this section.</p> <h3 id="21-continuous-bag-of-words">2.1. Continuous Bag-of-Words</h3> <p>As a way of example, suppose we had a large text corpus which contained the sentence: “<em>Thou shalt not make a machine in the likeness of a human mind</em>”. In order to create a dataset from this, the CBOW method, in essence, slides a context window of a fixed word size (let’s say three in this case) over the sentence, as illustrated in Figure 3. In this case, we take the first two words in the window as input features and the last word to be the output label. We repeat this process of constructing the dataset by moving the window one word on until the end of the corpus has been reached.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.4-1400.webp"/> <img src="/assets/img/blog/blog2.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 3:</em> The CBOW method for constructing a dataset<d-cite key="Alammar2019"></d-cite>. </div> <h3 id="22-skip-gram">2.2. Skip-gram</h3> <p>Continuing with the previous example, the Skip-gram model constructs a dataset in a slightly different way. Instead of just considering the n previous words when trying to predict a target word, we now consider predicting all the words within a specific context window surrounding a specific word. A context window could be, say, two words before and after the word being considered. This is illustrated in Figure 4. In this case, the centre word in the window is the input word and a target word is created for each surrounding word. Again, we repeat this window-sliding process of constructing the dataset until the end of the corpus has been reached.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.5-1400.webp"/> <img src="/assets/img/blog/blog2.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 4:</em> The skip-gram method for constructing a dataset<d-cite key="Alammar2019"></d-cite>. </div> <p>In reality, training a model on this form of the dataset is computationally expensive. This is due to the fact that we have to compute the error of the neural language model against the whole output vocabulary vector for every training sample! In practice, a technique called <em>negative sampling</em> is employed. This involves transforming the dataset shown in Figure 4 to a new form shown in Figure 5. In this procedure, the input and output words are now both features and a new target column is added, where a value of <code class="language-plaintext highlighter-rouge">1</code> indicates that the two words are neighbours.</p> <p>In other words, the task has now changed from predicting the neighbour of a word to predicting if two words are neighbours.</p> <p>In order to ensure that not all samples have a target variable of one, we introduce a few negative samples (hence the method’s name) to the dataset by sampling a set of random words from our vocabulary and setting the corresponding target label to zero.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.6-1400.webp"/> <img src="/assets/img/blog/blog2.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 5:</em> The notion of negative sampling in the skip-gram method<d-cite key="Alammar2019"></d-cite>. </div> <hr/> <h2 id="3-training-a-word2vec-model">3. Training a Word2Vec Model</h2> <p>Alright, quick checkpoint: CBOW and Skip-gram with negative sampling are methods used to create/train word embeddings and not for training a word2vec model. Let us now dive into the process of training a word2vec model.</p> <p>At the beginning, we initialise each word in our dataset as a vector (with some specified length) of random values. In each training step, we look at just a single positive sample with its associated negative samples, as shown in Figure 6. If we compute the dot product between the input and output word vectors for each sample and squash this value through a sigmoid function (so that all values are between zero and one), we obtain a specific score. Now, we know that in the case of the positive sample the resultant sigmoid score should be close to one (because the two words are indeed neighbours) and the others should be close to zero. We can therefore compute an error by subtracting this sigmoid score from the target labels.</p> <p>Here comes the <em>learning</em> part: we can now use this error to adjust the values of each word vector in such a way so that we compute a lower error score the next time. We continue this process for all word vectors in our dataset, incrementally adjusting their values to achieve slightly better embeddings until the learning algorithm (which is usually a neural network) converges.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog2.7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog2.7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog2.7-1400.webp"/> <img src="/assets/img/blog/blog2.7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 6:</em> Training a Skip-gram language model with negative sampling<d-cite key="Alammar2019"></d-cite>. </div> <hr/> <h2 id="4-word2vec-tutorial">4. Word2Vec Tutorial</h2> <blockquote> <p>Time to code!</p> </blockquote> <p>Okay, enough with the theory. Let’s get a bit more practical by showing you how to implement this in the real world using Python. In this tutorial, we are going to walk you through how to construct your own text corpus and how to train your very own word2vec model.</p> <h3 id="41-package-installations-and-imports">4.1. Package Installations and Imports</h3> <p>First things first, we need to install and import a few packages. We will begin by installing the <a href="https://pypi.org/project/beautifulsoup4/"><code class="language-plaintext highlighter-rouge">beautifulsoup4</code></a> package which will be used for cleaning up scraped data from Wikipedia, together with the <a href="https://lxml.de/"><code class="language-plaintext highlighter-rouge">lxml</code></a> package to parse HTML content from Wikipedia pages. Additionally, we will also need the <a href="https://www.nltk.org/"><code class="language-plaintext highlighter-rouge">nltk</code></a> and <a href="https://pypi.org/project/gensim/"><code class="language-plaintext highlighter-rouge">gensim</code></a> packages to do most of the NLP heavy-lifting for us.</p> <p>Go ahead and run:</p> <figure class="highlight"><pre><code class="language-bash" data-lang="bash"><span class="nv">$ </span>pip <span class="nb">install </span>beautifulsoup4
<span class="nv">$ </span>pip <span class="nb">install </span>lxml
<span class="nv">$ </span>pip <span class="nb">install </span>nltk
<span class="nv">$ </span>pip <span class="nb">install </span>gensim</code></pre></figure> <h3 id="42-creating-a-corpus">4.2. Creating a Corpus</h3> <p>In practice, word2vec models are commonly trained on millions of words. For illustration purposes, however, in this tutorial, we are just going to scrape a small corpus of text from the Wikipedia article on <a href="https://en.wikipedia.org/wiki/Machine_learning">Machine Learning</a> to train our model.</p> <p>The following Python script does this for us:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">bs4</span> <span class="k">as</span> <span class="n">bs</span>
<span class="kn">import</span> <span class="n">urllib.request</span>
<span class="kn">import</span> <span class="n">re</span>

<span class="c1"># Scrape text from Wikipedia
</span><span class="n">scrapped_html</span> <span class="o">=</span> <span class="n">urllib</span><span class="p">.</span><span class="n">request</span><span class="p">.</span><span class="nf">urlopen</span><span class="p">(</span><span class="sh">'</span><span class="s">https://en.wikipedia.org/wiki/Machine_learning</span><span class="sh">'</span><span class="p">)</span>
<span class="n">raw_html</span> <span class="o">=</span> <span class="n">scrapped_html</span><span class="p">.</span><span class="nf">read</span><span class="p">()</span>  <span class="c1"># Parse all paragraphs
</span><span class="n">parsed_html</span> <span class="o">=</span> <span class="n">bs</span><span class="p">.</span><span class="nc">BeautifulSoup</span><span class="p">(</span><span class="n">raw_html</span><span class="p">,</span> <span class="sh">'</span><span class="s">lxml</span><span class="sh">'</span><span class="p">)</span>
<span class="n">paragraphs</span> <span class="o">=</span> <span class="n">parsed_html</span><span class="p">.</span><span class="nf">find_all</span><span class="p">(</span><span class="sh">'</span><span class="s">p</span><span class="sh">'</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">p</span><span class="p">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">paragraphs</span><span class="p">])</span></code></pre></figure> <p>In the script above, we first scrape and read all html data from the Wikipedia page using the <code class="language-plaintext highlighter-rouge">urlopen</code> method from the <a href="https://pypi.org/project/requests/"><code class="language-plaintext highlighter-rouge">requests</code></a> package. We then parse the raw html into a <code class="language-plaintext highlighter-rouge">BeautifulSoup</code> object and extract all text contained within <code class="language-plaintext highlighter-rouge">p</code> (paragraph) tags using the <code class="language-plaintext highlighter-rouge">find_all</code> method. Finally, we combine all of the paragraphs into a single text string.</p> <h3 id="43-text-pre-processing">4.3. Text Pre-Processing</h3> <p>The next step involves pre-processing the text so that it is in the correct format for training the Gensim Word2Vec model:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="n">nltk</span>
<span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>

<span class="c1"># Clean the text
</span><span class="n">clean_text</span> <span class="o">=</span> <span class="n">text</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span>
<span class="n">clean_text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sh">'</span><span class="s">[^a-zA-Z]</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">clean_text</span><span class="p">)</span>
<span class="n">clean_text</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="nf">sub</span><span class="p">(</span><span class="sa">r</span><span class="sh">'</span><span class="s">\s+</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s"> </span><span class="sh">'</span><span class="p">,</span> <span class="n">clean_text</span><span class="p">)</span>

<span class="c1"># Prepare the dataset
</span><span class="n">sentences</span> <span class="o">=</span> <span class="n">nltk</span><span class="p">.</span><span class="nf">sent_tokenize</span><span class="p">(</span><span class="n">clean_text</span><span class="p">)</span>
<span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">nltk</span><span class="p">.</span><span class="nf">word_tokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>

<span class="c1"># Remove any stopwords
</span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">words</span><span class="p">)):</span>
    <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">'</span><span class="s">english</span><span class="sh">'</span><span class="p">)]</span></code></pre></figure> <p>In the script above, we clean the text by first converting all characters to their lowercase form and then removing any digits, special characters and trailing spaces that may exist in the text. Next, we prepare the dataset into a collection of words so that it can be used to train Gensim’s Word2Vec model. We first split our <code class="language-plaintext highlighter-rouge">clean_text</code> into individual sentences using nltk’s <code class="language-plaintext highlighter-rouge">sent_tokenize</code> method and then further into individual words using the <code class="language-plaintext highlighter-rouge">word_tokenize</code> method. Finally, we cycle through our collection of words and remove any which are in nltk’s predefined list of English stopwords (or common words like “the”, “and”, “to”, etc).</p> <h3 id="44-creating-a-word2vec-model">4.4. Creating a Word2Vec Model</h3> <p>With this, we can now easily train our own Word2Vec model as follows:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="n">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="c1"># Create the model
</span><span class="n">word2vec</span> <span class="o">=</span> <span class="nc">Word2Vec</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">sg</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">min_count</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Print vocabulary of model
</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">vocabprint</span><span class="p">(</span><span class="n">vocab</span><span class="p">.</span><span class="nf">keys</span><span class="p">())</span></code></pre></figure> <p>Here, we simply instantiate a Gensim Word2Vec object using our collection of words. In this instantiation, we specify a value of <code class="language-plaintext highlighter-rouge">1</code> for the <code class="language-plaintext highlighter-rouge">sg</code> parameter and a value of <code class="language-plaintext highlighter-rouge">5</code> for the <code class="language-plaintext highlighter-rouge">negative</code> parameter — telling Gensim to utilise the skip-gram model and employ 5 negative samples respectively. We also specify a value of <code class="language-plaintext highlighter-rouge">2</code> for the <code class="language-plaintext highlighter-rouge">min_count</code> parameter so that the model only includes words that have appeared at least twice in our corpus. We can view a list of all of the unique words of the model by printing out the keys of the <code class="language-plaintext highlighter-rouge">wv.vocab</code> attribute.</p> <h3 id="45-exploring-the-model">4.5. Exploring the Model</h3> <p>Congratulations! You now have your very own word2vec model trained on your own corpus of text. Let’s explore your creation a bit further.</p> <p>We know that the word2vec model is used to map all words to a vector representation. We can view the vector corresponding to any word in our model’s vocabulary as follows:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Print vector of word
</span><span class="nf">print</span><span class="p">(</span><span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">[</span><span class="sh">'</span><span class="s">learning</span><span class="sh">'</span><span class="p">])</span>

<span class="o">&gt;&gt;</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.0321747</span>  <span class="o">-</span><span class="mf">0.09254234</span>  <span class="mf">0.15719296</span> <span class="o">-</span><span class="mf">0.18106991</span> <span class="o">-</span><span class="mf">0.05903807</span>  <span class="mf">0.06826855</span> <span class="o">-</span><span class="mf">0.0546078</span>  <span class="o">-</span><span class="mf">0.14852802</span>  <span class="mf">0.00465734</span>  <span class="mf">0.00357654</span> <span class="o">-</span><span class="mf">0.11191269</span>  <span class="mf">0.00123405</span> <span class="o">-</span><span class="mf">0.00260608</span> <span class="o">-</span><span class="mf">0.01252941</span>  <span class="mf">0.03632444</span>  <span class="mf">0.05868229</span> <span class="o">-</span><span class="mf">0.01147301</span>  <span class="mf">0.01271282</span> <span class="mf">0.20245183</span> <span class="o">-</span><span class="mf">0.08651368</span> <span class="o">-</span><span class="mf">0.02063143</span>  <span class="mf">0.10166611</span>  <span class="mf">0.05482733</span> <span class="o">-</span><span class="mf">0.06414133</span> <span class="o">-</span><span class="mf">0.09078009</span>  <span class="mf">0.03067572</span> <span class="o">-</span><span class="mf">0.06758043</span>  <span class="mf">0.00037985</span>  <span class="mf">0.10958873</span>  <span class="mf">0.00133575</span> <span class="o">-</span><span class="mf">0.17905715</span> <span class="o">-</span><span class="mf">0.20932317</span> <span class="o">-</span><span class="mf">0.08205332</span> <span class="o">-</span><span class="mf">0.02681177</span> <span class="o">-</span><span class="mf">0.04361923</span>  <span class="mf">0.06187554</span> <span class="o">-</span><span class="mf">0.03218165</span> <span class="o">-</span><span class="mf">0.12924905</span>  <span class="mf">0.02013807</span>  <span class="mf">0.06686062</span>  <span class="mf">0.0272868</span>  <span class="o">-</span><span class="mf">0.02437109</span> <span class="mf">0.00260349</span> <span class="o">-</span><span class="mf">0.02714068</span>  <span class="mf">0.02746866</span>  <span class="mf">0.05804974</span>  <span class="mf">0.08129382</span> <span class="o">-</span><span class="mf">0.03174321</span> <span class="mf">0.08987447</span> <span class="o">-</span><span class="mf">0.05901277</span>  <span class="mf">0.07368492</span>  <span class="mf">0.04791974</span>  <span class="mf">0.08164269</span> <span class="o">-</span><span class="mf">0.05956567</span> <span class="o">-</span><span class="mf">0.12904255</span> <span class="o">-</span><span class="mf">0.07182135</span> <span class="o">-</span><span class="mf">0.0183635</span>   <span class="mf">0.0681318</span>   <span class="mf">0.09037064</span>  <span class="mf">0.03774516</span> <span class="mf">0.12213644</span>  <span class="mf">0.1648079</span>  <span class="o">-</span><span class="mf">0.18914327</span>  <span class="mf">0.02890724</span> <span class="o">-</span><span class="mf">0.02372251</span> <span class="o">-</span><span class="mf">0.11012954</span> <span class="o">-</span><span class="mf">0.1020454</span>  <span class="o">-</span><span class="mf">0.01853278</span> <span class="o">-</span><span class="mf">0.05396225</span> <span class="o">-</span><span class="mf">0.00242959</span> <span class="o">-</span><span class="mf">0.10753106</span> <span class="o">-</span><span class="mf">0.07141761</span> <span class="mf">0.10347419</span> <span class="o">-</span><span class="mf">0.10766012</span> <span class="o">-</span><span class="mf">0.16925317</span> <span class="o">-</span><span class="mf">0.10747337</span> <span class="o">-</span><span class="mf">0.08550954</span>  <span class="mf">0.01928806</span> <span class="mf">0.2854533</span>  <span class="o">-</span><span class="mf">0.12928957</span>  <span class="mf">0.05894407</span>  <span class="mf">0.05380363</span>  <span class="mf">0.06413457</span> <span class="o">-</span><span class="mf">0.18124926</span> <span class="mf">0.06807419</span> <span class="o">-</span><span class="mf">0.12306273</span> <span class="o">-</span><span class="mf">0.16328035</span>  <span class="mf">0.14293176</span> <span class="o">-</span><span class="mf">0.23447195</span> <span class="o">-</span><span class="mf">0.07432053</span> <span class="o">-</span><span class="mf">0.1858378</span>   <span class="mf">0.07563769</span>  <span class="mf">0.06148988</span>  <span class="mf">0.10675731</span> <span class="o">-</span><span class="mf">0.03910929</span> <span class="o">-</span><span class="mf">0.00869188</span> <span class="o">-</span><span class="mf">0.05734098</span>  <span class="mf">0.07828433</span>  <span class="mf">0.04085582</span>  <span class="mf">0.0544676</span><span class="p">]</span></code></pre></figure> <p>From these outputs, we can see that Gensim, by default, maps all words to a one hundred dimensional numpy vector. As we have seen, one of the benefits of using word2vec is that the model is able to capture the semantic meaning of words in relation to one another. Let’s try and confirm this by printing out the words most similar (in vector space) to the word “learning”:</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="c1"># Print most similar words
</span><span class="nf">print</span><span class="p">(</span><span class="n">word2vec</span><span class="p">.</span><span class="n">wv</span><span class="p">.</span><span class="nf">most_similar</span><span class="p">(</span><span class="sh">'</span><span class="s">learning</span><span class="sh">'</span><span class="p">))</span>

<span class="o">&gt;&gt;</span>  <span class="p">[(</span><span class="sh">'</span><span class="s">machine</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9990278482437134</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9989383220672607</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9988012313842773</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">training</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9987066984176636</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">biases</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9984654784202576</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">set</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9983999729156494</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">classification</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9983198046684265</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">used</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9982497692108154</span><span class="p">),</span> <span class="p">(</span><span class="sh">'</span><span class="s">use</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9981957674026489</span><span class="p">),(</span><span class="sh">'</span><span class="s">systems</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.9980695247650146</span><span class="p">)]</span></code></pre></figure> <p>The list above shows us the words most similar to the word “learning” and displays their corresponding cosine similarities. No surprise, the word “machine” appears at the top of our list. We can also see words such as “data”, “model” and “training” also appearing near the top which means they often occur with the word “learning” — this makes sense. We should therefore be quite confident that our model has indeed successfully managed to capture some sort of semantic relationship between words using just a single Wikipedia page!</p> <hr/> <h2 id="5-wrapping-up">5. Wrapping up</h2> <p>And there you have it! In this blog post, we described the two flavours of word2vec models, namely CBOW and Skip-gram as well as the model training procedure. Armed with this information, we got a bit more practical by walking you through how to create your very own word2vec model in Python using the Gensim library.</p> <p>Stay tuned to this series to learn more about the awesome world of NLP as we share more on the latest developments, code implementations and thought-provoking perspectives on NLP’s impact on the way we interact with the world. It’s an extremely exciting time for anyone to get into the world of NLP!</p>]]></content><author><name>Shane van Heerden</name></author><category term="NLP"/><summary type="html"><![CDATA[Part 2 - Natural Language Processing Series]]></summary></entry><entry><title type="html">🕰️ A Brief History of Natural Language Processing</title><link href="https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/" rel="alternate" type="text/html" title="🕰️ A Brief History of Natural Language Processing"/><published>2020-10-07T00:00:00+00:00</published><updated>2020-10-07T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2020/a_brief_history_of_natural_language_processing/"><![CDATA[<p><em>This blog post was originally featured on the <a href="https://medium.com/cape-ai-stories/natural-language-processing-series-8d51b87b6004">Cape AI Medium page</a>.</em></p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.1-1400.webp"/> <img src="/assets/img/blog/blog1.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This is the first blog post in a series focusing on the wonderful world of Natural Language Processing (NLP)! In this post I’ll present you with the lay of the land — describing seven important milestones which have been achieved in the NLP field over the last 20 years. This is largely inspired by <a href="https://www.youtube.com/watch?v=sGVi4gb90zk">Sebastian Ruder’s talk</a> at the 2018 Deep Learning Indaba which <a href="https://shanevanheerden.github.io/news/news11/">I attended</a> in Stellenbosch.</p> <p>Short disclaimer before I begin: This post is heavily skewed towards neural network-based advancements. Many of these milestones, however, were built on many influential ideas presented by non-neural network-based work during the same era, which, for brevity purposes, have been omitted from this post.</p> <h2 id="1-neural-language-models-2001">1. Neural Language Models (2001)</h2> <p>It’s 2001 and the field of NLP is quite nascent. Academics all around the world are beginning to think more about how language could be modelled. After a lot of research, Neural Language models are born. Language modelling is simply the task of determining the probability of the next word (often referred to as a <em>token</em>) occurring in a piece of text given all the previous words. Traditional approaches for tackling this problem were based on n-gram models in combination with some sort of smoothing technique<d-cite key="Kneser1995"></d-cite>. Bengio <em>et al.</em><d-cite key="Bengio2000"></d-cite> were the first to propose using a feed-forward neural network, a so-called word “lookup-table”, for representing the <em>n</em> previous words in a sequence as illustrated in Figure 1. Today, this is known as <em>word embeddings</em>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.2-1400.webp"/> <img src="/assets/img/blog/blog1.2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1:</em> The first feed-forward neural network used for language modelling<d-cite key="Kneser1995"></d-cite>. </div> <hr/> <h2 id="2-multi-task-learning-2008">2. Multi-task Learning (2008)</h2> <p>Excitement and interest grows steadily in the years following Neural Language models. Advances in computer hardware allow researchers to push the boundaries on language modelling, giving rise to new NLP methods. One such method is multi-task learning. The notion of multi-task learning involves training models to solve more than one learning task, while also using a set of shared parameters. As a result, models are forced to learn a representation that exploits the commonalities and differences across all tasks.</p> <p>Collobert and Weston<d-cite key="Collobert2008"></d-cite> were the first to apply a form of multi-task learning in the NLP domain back in 2008. They trained two convolutional models with max pooling to perform both part-of-speech and named entity recognition tagging, while also sharing a common word lookup table (or word embedding), as shown in Figure 2. Years later, their paper was highlighted by many experts as a fundamental milestone in deep learning for NLP and received the Test-of-time Award at the 2018 <em>International Conference on Machine Learning</em> (ICML).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.3-1400.webp"/> <img src="/assets/img/blog/blog1.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 2:</em> The first multi-task model sharing a common word lookup table<d-cite key="Collobert2008"></d-cite>. </div> <hr/> <h2 id="3-word-embeddings-2013">3. Word Embeddings (2013)</h2> <p>If you’ve had any exposure to NLP, the first thing you have probably come across is the idea of word embeddings (or more commonly known as <em>word2vec</em>). Although we have seen that word embeddings have been used as far back as 2001, in 2013 Mikolov <em>et al.</em><d-cite key="Mikolov2013"></d-cite> proposed a simple but novel method for efficiently training these word embeddings on very large unlabeled corpora which ultimately led to their wide-scale adoption.</p> <p>Word embeddings attempt to create a dense vector representation of text, and addresses many challenges faced with using traditional sparse bag-of-words representation. Word embeddings were shown to capture every intuitive relationship between words such as gender, verb tense and country capital, as illustrated in Figure 3.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.4-1400.webp"/> <img src="/assets/img/blog/blog1.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 3:</em> The intuitive relationships captured by word embeddings<d-cite key="Mikolov2013"></d-cite>. </div> <hr/> <h2 id="4-neural-networks-for-nlp-2013">4. Neural Networks for NLP (2013)</h2> <p>Looking back, 2013 appeared to be an inflection point in the NLP field, as research and development grew exponentially thereon. The advancements in word embeddings ultimately sparked the wider application of neural networks in NLP. The key challenge that needed to be addressed was architecturally allowing sequences of variable lengths to be inputted into the neural net which ultimately lead to three architectures emerging, namely: <em>recurrent neural networks</em> (RNNs) (which were soon replaced by <em>long-short term memory</em> (LSTM) networks), <em>convolutional neural networks</em> (CNNs), and recursive neural networks. Today, these neural network architectures have produced exceptional results and are widely used for many NLP applications.</p> <hr/> <h2 id="5-sequence-to-sequence-models-2014">5. Sequence-to-Sequence Models (2014)</h2> <p>Soon after the emergence of RNNs and CNNs for language modelling, Sutskever <em>et al.</em><d-cite key="Sutskever2014"></d-cite> were the first to propose a general framework for mapping one sequence to another, which is now known as <em>sequence-to-sequence</em> models. In this framework, an encoder network processes an input sequence token by token and compresses it into a vector representation, represented by the blue layers in Figure 4. A decoder network (represented by the red layers) is then used to predict a new sequence of output tokens based on the encoder state, which takes every previously predicted token as input.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.5-1400.webp"/> <img src="/assets/img/blog/blog1.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 4:</em> A visual representation of a sequence-to-sequence model used for translation<d-cite key="See2016"></d-cite>. </div> <p>This architecture is particularly useful in tasks such as <em>machine translation</em> (MT) and <em>natural language generation</em> (NLG). It’s no surprise that, in 2016, Google announced that it is in the process of replacing all of its statistical-based MT systems with neural MT models<d-cite key="Wu2016"></d-cite>. Additionally, since the decoder model can be conditioned on any arbitrary representation, it can also be used for tasks like generating captions for images<d-cite key="Vinyals2015"></d-cite>.</p> <hr/> <h2 id="6-attention-mechanisms-2015">6. Attention Mechanisms (2015)</h2> <p>Although useful in a wide range of tasks, sequence-to-sequence models were struggling with being able to capture long-range dependencies between words in text. In 2015, the concept of attention was introduced by Bahdanau <em>et al.</em><d-cite key="Bahdanau2015"></d-cite> as a way of addressing this bottleneck. In essence, attention in a neural network is a mechanism for deciding which parts of the input sequence to attend to when routing information. Various attention mechanisms have also been applied in the computer vision space for image captioning<d-cite key="Xu2015"></d-cite>, which also provides a glimpse into the inner workings of the model, as is seen in Figure 5.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.6-1400.webp"/> <img src="/assets/img/blog/blog1.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 5:</em> A visual representation of the attention mechanism in an image captioning model<d-cite key="Xu2015"></d-cite>. </div> <p>Attention is not only restricted to the input sequence and can also be used to focus on surrounding words in a body of text — commonly referred to as <em>self attention</em> — to obtain more contextual meaning. This is at the heart of the current state-of-the-art <em>transformer</em> architecture, proposed by Vaswani <em>et al.</em><d-cite key="Vaswani2017"></d-cite> in 2017, which is composed of multiple self-attention layers. The transformer sparked an explosion of new language model architectures (and an inside joke among AI practitioners regarding Sesame Street Muppets), the most notable being <em>Bidirectional Encoder Representations from Transformers</em> (BERT) and <em>Generative Pre-trained Transformers</em> (GPT).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.7-1400.webp"/> <img src="/assets/img/blog/blog1.7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 6:</em> The various language model architectures based on the transformer model<d-cite key="Wang2019"></d-cite>. </div> <h2 id="7-pre-trained-language-models-2018">7. Pre-trained Language Models (2018)</h2> <p>Dai &amp; Le<d-cite key="Dai2015"></d-cite>. were the first to propose using pre-trained language models in 2015 but this notion was only recently shown to be beneficial across a broad range of NLP-related tasks. More specifically, it was shown that pre-trained language models could be <em>fine-tuned</em> on other data related to a specific target task<d-cite key="Ramachandran2017, Howard2018"></d-cite>. Additionally, language model embeddings could also be used as features in a target model leading to significant improvements over the then state-of-the-art models<d-cite key="Peters2018"></d-cite>, as shown in Figure 7.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog1.8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog1.8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog1.8-1400.webp"/> <img src="/assets/img/blog/blog1.8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 7:</em> Improvements over state-of-the-art models when employing language model embeddings<d-cite key="Peters2018"></d-cite>. </div> <hr/> <h2 id="8-where-we-are-today-and-looking-forward">8. Where we are today and looking forward…</h2> <p>Nowadays, there exists an array of initiatives aimed at open-sourcing many large state-of-the-art pre-trained models. These models can be fine-tuned to perform various NLP-related tasks like <em>sequence classification</em>, <em>extractive question answering</em>, <em>named entity recognition</em> and <em>text summarization</em> (to name a few). NLP is advancing at an incredible pace and is giving rise to global communities dedicated to solving the world’s most important problems through language understanding.</p> <p>Stay tuned to this series to learn more about the awesome world of NLP as I share more on the latest developments, code implementations and thought-provoking perspectives on NLP’s impact on the way we interact with the world. It’s an extremely exciting time for anyone to get into the world of NLP!</p>]]></content><author><name>Shane van Heerden</name></author><category term="NLP"/><summary type="html"><![CDATA[Part 1 - Natural Language Processing Series]]></summary></entry><entry><title type="html">📧 Identifying High Contact Drivers for a Personal Insurance Provider</title><link href="https://shanevanheerden.github.io/blog/2020/identifying_high_contact_drivers_for_a_personal_insurance_provider/" rel="alternate" type="text/html" title="📧 Identifying High Contact Drivers for a Personal Insurance Provider"/><published>2020-10-01T00:00:00+00:00</published><updated>2020-10-01T00:00:00+00:00</updated><id>https://shanevanheerden.github.io/blog/2020/identifying_high_contact_drivers_for_a_personal_insurance_provider</id><content type="html" xml:base="https://shanevanheerden.github.io/blog/2020/identifying_high_contact_drivers_for_a_personal_insurance_provider/"><![CDATA[<h1 id="-work-in-progress-">🚧 <b>WORK IN PROGRESS</b> 🚧</h1> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.1-1400.webp"/> <img src="/assets/img/blog/blog13.1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <h2 id="1-the-problem">1. The problem</h2> <hr/> <h2 id="2-understanding-the-clients-needs">2. Understanding the clients needs</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.2-1400.webp"/> <img src="/assets/img/blog/blog13.2.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 1: Solution architecture.</em> </div> <hr/> <h2 id="3-deep-dive-anaysis">3. Deep dive anaysis</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.3-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.3-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.3-1400.webp"/> <img src="/assets/img/blog/blog13.3.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 2: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.4-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.4-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.4-1400.webp"/> <img src="/assets/img/blog/blog13.4.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 3: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.5-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.5-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.5-1400.webp"/> <img src="/assets/img/blog/blog13.5.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 4: Solution architecture.</em> </div> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.6-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.6-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.6-1400.webp"/> <img src="/assets/img/blog/blog13.6.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 5: Solution architecture.</em> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.7-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.7-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.7-1400.webp"/> <img src="/assets/img/blog/blog13.7.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(a)</center> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.8-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.8-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.8-1400.webp"/> <img src="/assets/img/blog/blog13.8.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(b)</center> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.9-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.9-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.9-1400.webp"/> <img src="/assets/img/blog/blog13.9.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(c)</center> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.10-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.10-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.10-1400.webp"/> <img src="/assets/img/blog/blog13.10.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(d)</center> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.11-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.11-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.11-1400.webp"/> <img src="/assets/img/blog/blog13.11.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(e)</center> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.12-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.12-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.12-1400.webp"/> <img src="/assets/img/blog/blog13.12.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(f)</center> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.13-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.13-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.13-1400.webp"/> <img src="/assets/img/blog/blog13.13.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(g)</center> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.14-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.14-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.14-1400.webp"/> <img src="/assets/img/blog/blog13.14.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(h)</center> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.15-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.15-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.15-1400.webp"/> <img src="/assets/img/blog/blog13.15.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(i)</center> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.16-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.16-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.16-1400.webp"/> <img src="/assets/img/blog/blog13.16.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(j)</center> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.17-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.17-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.17-1400.webp"/> <img src="/assets/img/blog/blog13.17.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(k)</center> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.18-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.18-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.18-1400.webp"/> <img src="/assets/img/blog/blog13.18.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <center>(l)</center> </div> </div> <div class="caption"> <em>Figure 6:</em> . </div> <hr/> <h2 id="4-productionising-the-solution">4. Productionising the solution</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.19-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.19-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.19-1400.webp"/> <img src="/assets/img/blog/blog13.19.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 7: Solution architecture.</em> </div> <hr/> <h2 id="5-wrapping-up">5. Wrapping up</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/blog/blog13.20-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/blog/blog13.20-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blog/blog13.20-1400.webp"/> <img src="/assets/img/blog/blog13.20.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> <em>Figure 8: Client feedback.</em> </div>]]></content><author><name>Shane van Heerden</name></author><category term="CaseStudy"/><summary type="html"><![CDATA[How I advised a client in reducing the human workload involved with answering customer queries]]></summary></entry></feed>